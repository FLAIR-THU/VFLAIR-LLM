/bin/bash: /home/DAIR/guzx/.conda/envs/py38/lib/libtinfo.so.6: no version information available (required by /bin/bash)
2024-09-30 23:03:36.564 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-09-30 23:03:51.788 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
2024-09-30 23:11:19.668 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
================= iter seed  60  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 3
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 0.5, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/3_0/MID_Passive_MIDModel_Linear_head_0.5,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 60 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 43928832 || trainable%: 0.902113673315967
model slices: dict_keys([0])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 87050501 || trainable%: 26.719723301764798
model slices: dict_keys([1])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.69it/s]Epoch 0/49: 1it [00:00,  1.23it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.39 GiB total capacity; 37.05 GiB already allocated; 308.81 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-09-30 23:13:21.894 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-09-30 23:13:39.911 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
2024-09-30 23:20:55.499 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
================= iter seed  60  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 3
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 0.1, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/3_0/MID_Passive_MIDModel_Linear_head_0.1,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 60 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 43928832 || trainable%: 0.902113673315967
model slices: dict_keys([0])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 87050501 || trainable%: 26.719723301764798
model slices: dict_keys([1])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.33it/s]Epoch 0/49: 1it [00:01,  1.20s/it]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.39 GiB total capacity; 37.05 GiB already allocated; 308.81 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-09-30 23:22:38.568 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-09-30 23:22:58.108 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
2024-09-30 23:30:07.523 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
================= iter seed  60  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 3
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 0.01, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/3_0/MID_Passive_MIDModel_Linear_head_0.01,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 60 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 43928832 || trainable%: 0.902113673315967
model slices: dict_keys([0])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 87050501 || trainable%: 26.719723301764798
model slices: dict_keys([1])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.64it/s]Epoch 0/49: 1it [00:00,  1.10it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.39 GiB total capacity; 37.05 GiB already allocated; 308.81 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-09-30 23:32:05.796 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-09-30 23:32:21.456 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
2024-09-30 23:39:39.539 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
================= iter seed  60  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 3
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 0.001, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/3_0/MID_Passive_MIDModel_Linear_head_0.001,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 60 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 43928832 || trainable%: 0.902113673315967
model slices: dict_keys([0])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 87050501 || trainable%: 26.719723301764798
model slices: dict_keys([1])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.19it/s]Epoch 0/49: 1it [00:01,  1.17s/it]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.39 GiB total capacity; 37.05 GiB already allocated; 308.81 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-09-30 23:41:15.729 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-09-30 23:41:27.122 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
2024-09-30 23:48:41.866 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
================= iter seed  60  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 3
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 0.0001, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/3_0/MID_Passive_MIDModel_Linear_head_0.0001,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 60 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 43928832 || trainable%: 0.902113673315967
model slices: dict_keys([0])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 87050501 || trainable%: 26.719723301764798
model slices: dict_keys([1])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.62it/s]Epoch 0/49: 1it [00:00,  1.19it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.39 GiB total capacity; 37.05 GiB already allocated; 308.81 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-09-30 23:50:36.334 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-09-30 23:50:50.658 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
2024-09-30 23:57:57.157 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
================= iter seed  60  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 3
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 1e-05, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/3_0/MID_Passive_MIDModel_Linear_head_1e-05,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 60 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 43928832 || trainable%: 0.902113673315967
model slices: dict_keys([0])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 87050501 || trainable%: 26.719723301764798
model slices: dict_keys([1])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.52it/s]Epoch 0/49: 1it [00:00,  1.09it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.39 GiB total capacity; 37.05 GiB already allocated; 308.81 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-09-30 23:59:47.182 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-09-30 23:59:58.511 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
2024-10-01 00:07:18.746 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
================= iter seed  60  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 3
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 0.05, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/3_0/MID_Passive_MIDModel_Linear_head_0.05,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 60 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 43928832 || trainable%: 0.902113673315967
model slices: dict_keys([0])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 87050501 || trainable%: 26.719723301764798
model slices: dict_keys([1])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.74it/s]Epoch 0/49: 1it [00:00,  1.27it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.39 GiB total capacity; 37.05 GiB already allocated; 308.81 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 00:09:06.313 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 00:09:14.997 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
2024-10-01 00:16:28.616 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
================= iter seed  60  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 3
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 0.005, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/3_0/MID_Passive_MIDModel_Linear_head_0.005,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 60 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 43928832 || trainable%: 0.902113673315967
model slices: dict_keys([0])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 87050501 || trainable%: 26.719723301764798
model slices: dict_keys([1])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.78it/s]Epoch 0/49: 1it [00:00,  1.25it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.39 GiB total capacity; 37.05 GiB already allocated; 308.81 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 00:18:12.967 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 00:18:21.462 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
2024-10-01 00:25:45.149 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
================= iter seed  61  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 3
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 0.5, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/3_0/MID_Passive_MIDModel_Linear_head_0.5,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 61 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 43928832 || trainable%: 0.902113673315967
model slices: dict_keys([0])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 87050501 || trainable%: 26.719723301764798
model slices: dict_keys([1])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.75it/s]Epoch 0/49: 1it [00:00,  1.26it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.39 GiB total capacity; 37.05 GiB already allocated; 308.81 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 00:27:39.132 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 00:27:47.019 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
2024-10-01 00:34:58.011 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
================= iter seed  61  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 3
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 0.1, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/3_0/MID_Passive_MIDModel_Linear_head_0.1,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 61 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 43928832 || trainable%: 0.902113673315967
model slices: dict_keys([0])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 87050501 || trainable%: 26.719723301764798
model slices: dict_keys([1])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.72it/s]Epoch 0/49: 1it [00:00,  1.23it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.39 GiB total capacity; 37.05 GiB already allocated; 308.81 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 00:36:42.827 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 00:36:50.600 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
2024-10-01 00:44:10.946 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
================= iter seed  61  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 3
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 0.01, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/3_0/MID_Passive_MIDModel_Linear_head_0.01,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 61 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 43928832 || trainable%: 0.902113673315967
model slices: dict_keys([0])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 87050501 || trainable%: 26.719723301764798
model slices: dict_keys([1])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.85it/s]Epoch 0/49: 1it [00:00,  1.29it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.39 GiB total capacity; 37.05 GiB already allocated; 308.81 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 00:45:59.020 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 00:46:06.523 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
2024-10-01 00:54:12.694 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
================= iter seed  61  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 3
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 0.001, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/3_0/MID_Passive_MIDModel_Linear_head_0.001,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 61 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 43928832 || trainable%: 0.902113673315967
model slices: dict_keys([0])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 87050501 || trainable%: 26.719723301764798
model slices: dict_keys([1])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.13it/s]Epoch 0/49: 1it [00:01,  1.20s/it]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.39 GiB total capacity; 37.05 GiB already allocated; 308.81 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 00:55:51.946 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 00:56:04.562 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
2024-10-01 01:04:03.921 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
================= iter seed  61  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 3
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 0.0001, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/3_0/MID_Passive_MIDModel_Linear_head_0.0001,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 61 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 43928832 || trainable%: 0.902113673315967
model slices: dict_keys([0])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 87050501 || trainable%: 26.719723301764798
model slices: dict_keys([1])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.55it/s]Epoch 0/49: 1it [00:00,  1.09it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.39 GiB total capacity; 37.05 GiB already allocated; 308.81 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 01:05:53.812 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 01:06:02.801 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
2024-10-01 01:14:13.661 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
================= iter seed  61  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 3
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 1e-05, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/3_0/MID_Passive_MIDModel_Linear_head_1e-05,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 61 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 43928832 || trainable%: 0.902113673315967
model slices: dict_keys([0])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 87050501 || trainable%: 26.719723301764798
model slices: dict_keys([1])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.80it/s]Epoch 0/49: 1it [00:00,  1.29it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.39 GiB total capacity; 37.05 GiB already allocated; 308.81 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 01:16:02.660 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 01:16:10.858 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
2024-10-01 01:24:07.024 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
================= iter seed  61  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 3
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 0.05, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/3_0/MID_Passive_MIDModel_Linear_head_0.05,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 61 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 43928832 || trainable%: 0.902113673315967
model slices: dict_keys([0])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 87050501 || trainable%: 26.719723301764798
model slices: dict_keys([1])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.89it/s]Epoch 0/49: 1it [00:00,  1.34it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.39 GiB total capacity; 37.05 GiB already allocated; 308.81 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 01:26:04.350 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 01:26:13.620 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
2024-10-01 01:34:17.866 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
================= iter seed  61  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 3
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 0.005, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/3_0/MID_Passive_MIDModel_Linear_head_0.005,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 61 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 43928832 || trainable%: 0.902113673315967
model slices: dict_keys([0])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 87050501 || trainable%: 26.719723301764798
model slices: dict_keys([1])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.84it/s]Epoch 0/49: 1it [00:00,  1.34it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.39 GiB total capacity; 37.05 GiB already allocated; 308.81 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 01:36:09.921 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 01:36:17.873 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
2024-10-01 01:44:15.004 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
================= iter seed  62  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 3
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 0.5, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/3_0/MID_Passive_MIDModel_Linear_head_0.5,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 62 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 43928832 || trainable%: 0.902113673315967
model slices: dict_keys([0])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 87050501 || trainable%: 26.719723301764798
model slices: dict_keys([1])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.34it/s]Epoch 0/49: 1it [00:00,  1.03it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.39 GiB total capacity; 37.05 GiB already allocated; 308.81 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 01:46:00.511 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 01:46:09.619 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
2024-10-01 01:54:09.822 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
================= iter seed  62  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 3
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 0.1, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/3_0/MID_Passive_MIDModel_Linear_head_0.1,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 62 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 43928832 || trainable%: 0.902113673315967
model slices: dict_keys([0])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 87050501 || trainable%: 26.719723301764798
model slices: dict_keys([1])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.57it/s]Epoch 0/49: 1it [00:00,  1.05it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.39 GiB total capacity; 37.05 GiB already allocated; 308.81 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 01:55:59.610 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 01:56:11.685 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
2024-10-01 02:04:22.521 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
================= iter seed  62  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 3
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 0.01, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/3_0/MID_Passive_MIDModel_Linear_head_0.01,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 62 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 43928832 || trainable%: 0.902113673315967
model slices: dict_keys([0])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 87050501 || trainable%: 26.719723301764798
model slices: dict_keys([1])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.76it/s]Epoch 0/49: 1it [00:00,  1.21it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.39 GiB total capacity; 37.05 GiB already allocated; 308.81 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 02:06:13.874 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 02:06:24.331 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
2024-10-01 02:14:33.742 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
================= iter seed  62  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 3
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 0.001, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/3_0/MID_Passive_MIDModel_Linear_head_0.001,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 62 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 43928832 || trainable%: 0.902113673315967
model slices: dict_keys([0])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 87050501 || trainable%: 26.719723301764798
model slices: dict_keys([1])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.50it/s]Epoch 0/49: 1it [00:00,  1.11it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.39 GiB total capacity; 37.05 GiB already allocated; 308.81 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 02:16:25.064 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 02:16:33.073 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
2024-10-01 02:24:28.559 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
================= iter seed  62  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 3
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 0.0001, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/3_0/MID_Passive_MIDModel_Linear_head_0.0001,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 62 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 43928832 || trainable%: 0.902113673315967
model slices: dict_keys([0])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 87050501 || trainable%: 26.719723301764798
model slices: dict_keys([1])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.83it/s]Epoch 0/49: 1it [00:00,  1.34it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.39 GiB total capacity; 37.05 GiB already allocated; 308.81 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 02:26:27.124 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 02:26:37.269 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
2024-10-01 02:34:41.770 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
================= iter seed  62  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 3
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 1e-05, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/3_0/MID_Passive_MIDModel_Linear_head_1e-05,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 62 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 43928832 || trainable%: 0.902113673315967
model slices: dict_keys([0])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 87050501 || trainable%: 26.719723301764798
model slices: dict_keys([1])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.12it/s]Epoch 0/49: 1it [00:01,  1.15s/it]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.39 GiB total capacity; 37.05 GiB already allocated; 308.81 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 02:36:46.827 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 02:36:57.395 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
2024-10-01 02:45:04.834 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
================= iter seed  62  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 3
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 0.05, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/3_0/MID_Passive_MIDModel_Linear_head_0.05,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 62 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 43928832 || trainable%: 0.902113673315967
model slices: dict_keys([0])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 87050501 || trainable%: 26.719723301764798
model slices: dict_keys([1])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.40it/s]Epoch 0/49: 1it [00:01,  1.12s/it]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.39 GiB total capacity; 37.05 GiB already allocated; 308.81 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 02:47:00.426 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 02:47:10.527 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
2024-10-01 02:55:03.497 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
================= iter seed  62  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 3
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 0.005, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/3_0/MID_Passive_MIDModel_Linear_head_0.005,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 62 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 43928832 || trainable%: 0.902113673315967
model slices: dict_keys([0])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 87050501 || trainable%: 26.719723301764798
model slices: dict_keys([1])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.85it/s]Epoch 0/49: 1it [00:00,  1.36it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.39 GiB total capacity; 37.05 GiB already allocated; 308.81 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 02:56:52.457 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 02:57:03.055 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
2024-10-01 03:05:00.286 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
================= iter seed  63  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 3
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 0.5, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/3_0/MID_Passive_MIDModel_Linear_head_0.5,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 63 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 43928832 || trainable%: 0.902113673315967
model slices: dict_keys([0])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 87050501 || trainable%: 26.719723301764798
model slices: dict_keys([1])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.75it/s]Epoch 0/49: 1it [00:00,  1.21it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.39 GiB total capacity; 37.05 GiB already allocated; 308.81 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 03:06:43.105 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 03:06:51.837 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
2024-10-01 03:14:55.980 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
================= iter seed  63  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 3
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 0.1, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/3_0/MID_Passive_MIDModel_Linear_head_0.1,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 63 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 43928832 || trainable%: 0.902113673315967
model slices: dict_keys([0])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 87050501 || trainable%: 26.719723301764798
model slices: dict_keys([1])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.36it/s]Epoch 0/49: 1it [00:00,  1.03it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.39 GiB total capacity; 37.05 GiB already allocated; 308.81 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 03:16:46.137 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 03:16:53.460 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
2024-10-01 03:24:52.267 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
================= iter seed  63  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 3
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 0.01, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/3_0/MID_Passive_MIDModel_Linear_head_0.01,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 63 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 43928832 || trainable%: 0.902113673315967
model slices: dict_keys([0])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 87050501 || trainable%: 26.719723301764798
model slices: dict_keys([1])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.66it/s]Epoch 0/49: 1it [00:00,  1.16it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.39 GiB total capacity; 37.05 GiB already allocated; 308.81 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 03:26:51.292 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 03:26:58.968 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
2024-10-01 03:35:04.751 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
================= iter seed  63  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 3
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 0.001, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/3_0/MID_Passive_MIDModel_Linear_head_0.001,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 63 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 43928832 || trainable%: 0.902113673315967
model slices: dict_keys([0])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 87050501 || trainable%: 26.719723301764798
model slices: dict_keys([1])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.12it/s]Epoch 0/49: 1it [00:01,  1.15s/it]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.39 GiB total capacity; 37.05 GiB already allocated; 308.81 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 03:37:06.224 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 03:37:10.453 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
2024-10-01 03:45:00.213 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
================= iter seed  63  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 3
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 0.0001, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/3_0/MID_Passive_MIDModel_Linear_head_0.0001,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 63 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 43928832 || trainable%: 0.902113673315967
model slices: dict_keys([0])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 87050501 || trainable%: 26.719723301764798
model slices: dict_keys([1])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.66it/s]Epoch 0/49: 1it [00:00,  1.19it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.39 GiB total capacity; 37.05 GiB already allocated; 308.81 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 03:46:47.364 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 03:46:54.624 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
2024-10-01 03:54:57.722 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
================= iter seed  63  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 3
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 1e-05, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/3_0/MID_Passive_MIDModel_Linear_head_1e-05,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 63 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 43928832 || trainable%: 0.902113673315967
model slices: dict_keys([0])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 87050501 || trainable%: 26.719723301764798
model slices: dict_keys([1])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.69it/s]Epoch 0/49: 1it [00:00,  1.21it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.39 GiB total capacity; 37.05 GiB already allocated; 308.81 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 03:56:43.856 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 03:56:52.094 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
2024-10-01 04:04:57.634 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
================= iter seed  63  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 3
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 0.05, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/3_0/MID_Passive_MIDModel_Linear_head_0.05,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 63 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 43928832 || trainable%: 0.902113673315967
model slices: dict_keys([0])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 87050501 || trainable%: 26.719723301764798
model slices: dict_keys([1])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.74it/s]Epoch 0/49: 1it [00:00,  1.28it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.39 GiB total capacity; 37.05 GiB already allocated; 308.81 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 04:06:47.176 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 04:06:54.934 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
2024-10-01 04:14:53.921 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
================= iter seed  63  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 3
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 0.005, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/3_0/MID_Passive_MIDModel_Linear_head_0.005,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 63 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 43928832 || trainable%: 0.902113673315967
model slices: dict_keys([0])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 87050501 || trainable%: 26.719723301764798
model slices: dict_keys([1])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.37it/s]Epoch 0/49: 1it [00:01,  1.11s/it]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.39 GiB total capacity; 37.05 GiB already allocated; 308.81 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 04:16:45.803 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 04:16:52.640 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
2024-10-01 04:24:55.963 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
================= iter seed  64  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 3
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 0.5, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/3_0/MID_Passive_MIDModel_Linear_head_0.5,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 64 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 43928832 || trainable%: 0.902113673315967
model slices: dict_keys([0])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 87050501 || trainable%: 26.719723301764798
model slices: dict_keys([1])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.42it/s]Epoch 0/49: 1it [00:00,  1.10it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.39 GiB total capacity; 37.05 GiB already allocated; 308.81 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 04:26:49.158 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 04:26:56.644 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
2024-10-01 04:34:50.373 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
================= iter seed  64  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 3
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 0.1, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/3_0/MID_Passive_MIDModel_Linear_head_0.1,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 64 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 43928832 || trainable%: 0.902113673315967
model slices: dict_keys([0])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 87050501 || trainable%: 26.719723301764798
model slices: dict_keys([1])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.77it/s]Epoch 0/49: 1it [00:00,  1.20it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.39 GiB total capacity; 37.05 GiB already allocated; 308.81 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 04:36:34.102 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 04:36:41.361 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
2024-10-01 04:44:37.865 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
================= iter seed  64  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 3
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 0.01, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/3_0/MID_Passive_MIDModel_Linear_head_0.01,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 64 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 43928832 || trainable%: 0.902113673315967
model slices: dict_keys([0])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 87050501 || trainable%: 26.719723301764798
model slices: dict_keys([1])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.54it/s]Epoch 0/49: 1it [00:00,  1.09it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.39 GiB total capacity; 37.05 GiB already allocated; 308.81 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 04:46:32.312 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 04:46:39.713 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
2024-10-01 04:54:50.487 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
================= iter seed  64  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 3
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 0.001, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/3_0/MID_Passive_MIDModel_Linear_head_0.001,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 64 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 43928832 || trainable%: 0.902113673315967
model slices: dict_keys([0])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 87050501 || trainable%: 26.719723301764798
model slices: dict_keys([1])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.76it/s]Epoch 0/49: 1it [00:00,  1.31it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.39 GiB total capacity; 37.05 GiB already allocated; 308.81 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 04:56:39.917 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 04:56:47.238 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
2024-10-01 05:04:43.174 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
================= iter seed  64  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 3
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 0.0001, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/3_0/MID_Passive_MIDModel_Linear_head_0.0001,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 64 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 43928832 || trainable%: 0.902113673315967
model slices: dict_keys([0])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 87050501 || trainable%: 26.719723301764798
model slices: dict_keys([1])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.81it/s]Epoch 0/49: 1it [00:00,  1.14it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.39 GiB total capacity; 37.05 GiB already allocated; 308.81 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 05:06:42.736 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 05:06:49.817 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
2024-10-01 05:14:55.251 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
================= iter seed  64  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 3
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 1e-05, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/3_0/MID_Passive_MIDModel_Linear_head_1e-05,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 64 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 43928832 || trainable%: 0.902113673315967
model slices: dict_keys([0])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 87050501 || trainable%: 26.719723301764798
model slices: dict_keys([1])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.74it/s]Epoch 0/49: 1it [00:00,  1.22it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.39 GiB total capacity; 37.05 GiB already allocated; 308.81 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 05:16:49.490 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 05:16:56.164 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
2024-10-01 05:24:48.301 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
================= iter seed  64  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 3
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 0.05, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/3_0/MID_Passive_MIDModel_Linear_head_0.05,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 64 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 43928832 || trainable%: 0.902113673315967
model slices: dict_keys([0])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 87050501 || trainable%: 26.719723301764798
model slices: dict_keys([1])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.82it/s]Epoch 0/49: 1it [00:00,  1.07it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.39 GiB total capacity; 37.05 GiB already allocated; 308.81 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 05:26:26.101 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 05:26:33.970 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
2024-10-01 05:34:35.428 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
================= iter seed  64  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 3
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 0.005, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/3_0/MID_Passive_MIDModel_Linear_head_0.005,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 64 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 43928832 || trainable%: 0.902113673315967
model slices: dict_keys([0])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 87050501 || trainable%: 26.719723301764798
model slices: dict_keys([1])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.66it/s]Epoch 0/49: 1it [00:00,  1.05it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.39 GiB total capacity; 37.05 GiB already allocated; 308.81 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 05:36:23.151 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 05:36:30.116 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
2024-10-01 05:44:42.985 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
================= iter seed  65  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 3
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 0.5, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/3_0/MID_Passive_MIDModel_Linear_head_0.5,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 65 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 43928832 || trainable%: 0.902113673315967
model slices: dict_keys([0])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 87050501 || trainable%: 26.719723301764798
model slices: dict_keys([1])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.74it/s]Epoch 0/49: 1it [00:00,  1.20it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.39 GiB total capacity; 37.05 GiB already allocated; 308.81 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 05:46:26.397 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 05:46:33.354 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
2024-10-01 05:54:29.135 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
================= iter seed  65  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 3
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 0.1, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/3_0/MID_Passive_MIDModel_Linear_head_0.1,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 65 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 43928832 || trainable%: 0.902113673315967
model slices: dict_keys([0])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 87050501 || trainable%: 26.719723301764798
model slices: dict_keys([1])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.81it/s]Epoch 0/49: 1it [00:00,  1.17it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.39 GiB total capacity; 37.05 GiB already allocated; 308.81 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 05:56:20.853 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 05:56:28.227 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
2024-10-01 06:04:43.762 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
================= iter seed  65  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 3
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 0.01, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/3_0/MID_Passive_MIDModel_Linear_head_0.01,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 65 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 43928832 || trainable%: 0.902113673315967
model slices: dict_keys([0])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 87050501 || trainable%: 26.719723301764798
model slices: dict_keys([1])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.72it/s]Epoch 0/49: 1it [00:00,  1.22it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.39 GiB total capacity; 37.05 GiB already allocated; 308.81 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 06:06:32.691 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 06:06:39.475 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
2024-10-01 06:15:02.356 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
================= iter seed  65  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 3
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 0.001, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/3_0/MID_Passive_MIDModel_Linear_head_0.001,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 65 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 43928832 || trainable%: 0.902113673315967
model slices: dict_keys([0])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 87050501 || trainable%: 26.719723301764798
model slices: dict_keys([1])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.23it/s]Epoch 0/49: 1it [00:01,  1.29s/it]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.39 GiB total capacity; 37.05 GiB already allocated; 308.81 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 06:16:54.210 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 06:17:01.732 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
2024-10-01 06:25:31.151 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
================= iter seed  65  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 3
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 0.0001, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/3_0/MID_Passive_MIDModel_Linear_head_0.0001,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 65 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 43928832 || trainable%: 0.902113673315967
model slices: dict_keys([0])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 87050501 || trainable%: 26.719723301764798
model slices: dict_keys([1])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.26it/s]Epoch 0/49: 1it [00:01,  1.18s/it]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.39 GiB total capacity; 37.05 GiB already allocated; 308.81 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 06:27:25.350 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 06:27:32.321 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
2024-10-01 06:35:59.514 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
================= iter seed  65  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 3
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 1e-05, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/3_0/MID_Passive_MIDModel_Linear_head_1e-05,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 65 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 43928832 || trainable%: 0.902113673315967
model slices: dict_keys([0])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 87050501 || trainable%: 26.719723301764798
model slices: dict_keys([1])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.54it/s]Epoch 0/49: 1it [00:01,  1.02s/it]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.39 GiB total capacity; 37.05 GiB already allocated; 308.81 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 06:37:48.413 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 06:37:54.996 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
2024-10-01 06:46:29.416 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
================= iter seed  65  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 3
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 0.05, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/3_0/MID_Passive_MIDModel_Linear_head_0.05,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 65 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 43928832 || trainable%: 0.902113673315967
model slices: dict_keys([0])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 87050501 || trainable%: 26.719723301764798
model slices: dict_keys([1])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.18it/s]Epoch 0/49: 1it [00:01,  1.22s/it]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.39 GiB total capacity; 37.05 GiB already allocated; 308.81 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 06:48:14.061 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 06:48:18.234 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
2024-10-01 06:56:56.441 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
================= iter seed  65  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 3
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 0.005, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/3_0/MID_Passive_MIDModel_Linear_head_0.005,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 65 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 43928832 || trainable%: 0.902113673315967
model slices: dict_keys([0])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 87050501 || trainable%: 26.719723301764798
model slices: dict_keys([1])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.40it/s]Epoch 0/49: 1it [00:01,  1.14s/it]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.39 GiB total capacity; 37.05 GiB already allocated; 308.81 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

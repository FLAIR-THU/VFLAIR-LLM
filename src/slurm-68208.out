/bin/bash: /home/DAIR/guzx/.conda/envs/py38/lib/libtinfo.so.6: no version information available (required by /bin/bash)
2024-10-06 00:31:57.118 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-06 00:32:12.720 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/meta-mathMetaMath-Mistral-7B_vfl_(2,)
2024-10-06 00:33:09.400 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/meta-mathMetaMath-Mistral-7B_vfl_(2,)
================= iter seed  60  =================
communication_protocol: FedSGD
load model_architect: CLM
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 2
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 0
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: AdversarialTraining
Defense_Config: {'party': [0], 'position': 'head', 'adversarial_model': 'Mapping_MLP3_noflatten', 'adversarial_model_lr': 0.001, 'imagined_adversary': 'ImaginedAdversary_MLP3_noflatten', 'imagined_adversary_lr': 0.001, 'lambda': 0.001, 'seq_length': 512, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/GMS8K/2-slice/2_0/AdversarialTraining_0.001,finetuned_model=meta-mathMetaMath-Mistral-7B.txt
=================================

===== iter 60 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
default tokenizer.pad_token: [PAD]
pad_token: [PAD]   pad_id: 32000
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
trainable params: 0 || all params: 567300096 || trainable%: 0.0
model slices: dict_keys([0])
model partition: 0head-2/1tail-30
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
7473 train examples
1319 test examples
X: <class 'numpy.ndarray'> 7473 1319
y <class 'numpy.ndarray'> 7473 1319
Passive Party 0: init Adversarial Trainining Defense
Adversarial_MLP init: 512 4096
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 32000
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:53<04:28, 53.66s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [02:59<06:25, 96.25s/it]Loading checkpoint shards:  50%|█████     | 3/6 [05:05<05:28, 109.61s/it]Loading checkpoint shards:  50%|█████     | 3/6 [07:14<07:14, 144.72s/it]
2024-10-06 00:40:24.812 | WARNING  | models.llm_models.base:from_pretrained:99 - RuntimeError('CUDA out of memory. Tried to allocate 224.00 MiB (GPU 1; 39.39 GiB total capacity; 7.81 GiB already allocated; 17.31 MiB free; 7.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')
Try to load from raw model
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:47<?, ?it/s]
Traceback (most recent call last):
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/base.py", line 97, in from_pretrained
    return self.from_vfl(model_name_or_path, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/base.py", line 122, in from_vfl
    return self.from_vfl(self._vfl_model_folder(model_name_or_path), **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/base.py", line 128, in from_vfl
    _model = self._load_model_tail(model_path, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/mistral.py", line 550, in _load_model_tail
    model_tail = MistralTailForCausalLM.from_pretrained(model_name_or_path, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/modeling_utils.py", line 3677, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/modeling_utils.py", line 4104, in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/modeling_utils.py", line 886, in _load_state_dict_into_meta_model
    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/accelerate/utils/modeling.py", line 317, in set_module_tensor_to_device
    new_value = value.to(device)
RuntimeError: CUDA out of memory. Tried to allocate 224.00 MiB (GPU 1; 39.39 GiB total capacity; 7.81 GiB already allocated; 17.31 MiB free; 7.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 296, in <module>
    args = load_parties_llm(args)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/load/LoadParty.py", line 43, in load_parties_llm
    args.parties[args.k - 1] = get_class_constructor(args.active_party_class)(args, args.k - 1, need_data=need_data)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 24, in __init__
    super().__init__(args, index, need_data=need_data, need_model=need_model)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 107, in __init__
    self.prepare_model(args, index)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 276, in prepare_model
    result = load_models_per_party_llm(args, index)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/load/LoadModels.py", line 373, in load_models_per_party_llm
    result = load_basic_models_llm(args,index)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/load/LoadModels.py", line 358, in load_basic_models_llm
    result = loader.load(args=args, model_path=model_path, is_active_party = is_active_party)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/load/llm_model_loaders/MistralModelLoader.py", line 29, in load
    self._models=p.from_pretrained(model_path, **args.kwargs_model_loading)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/base.py", line 100, in from_pretrained
    return self._from_raw(model_name_or_path, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/base.py", line 147, in _from_raw
    _model = self._load_model_tail(model_name_or_path, do_split=True, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/mistral.py", line 550, in _load_model_tail
    model_tail = MistralTailForCausalLM.from_pretrained(model_name_or_path, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/modeling_utils.py", line 3677, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/modeling_utils.py", line 4104, in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/modeling_utils.py", line 886, in _load_state_dict_into_meta_model
    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/accelerate/utils/modeling.py", line 317, in set_module_tensor_to_device
    new_value = value.to(device)
RuntimeError: CUDA out of memory. Tried to allocate 502.00 MiB (GPU 1; 39.39 GiB total capacity; 7.81 GiB already allocated; 17.31 MiB free; 7.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-06 00:41:27.103 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-06 00:41:34.640 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/meta-mathMetaMath-Mistral-7B_vfl_(2,)
2024-10-06 00:42:02.250 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/meta-mathMetaMath-Mistral-7B_vfl_(2,)
================= iter seed  60  =================
communication_protocol: FedSGD
load model_architect: CLM
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 2
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 0
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: AdversarialTraining
Defense_Config: {'party': [0], 'position': 'head', 'adversarial_model': 'Mapping_MLP3_noflatten', 'adversarial_model_lr': 0.001, 'imagined_adversary': 'ImaginedAdversary_MLP3_noflatten', 'imagined_adversary_lr': 0.001, 'lambda': 0.01, 'seq_length': 512, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/GMS8K/2-slice/2_0/AdversarialTraining_0.01,finetuned_model=meta-mathMetaMath-Mistral-7B.txt
=================================

===== iter 60 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
default tokenizer.pad_token: [PAD]
pad_token: [PAD]   pad_id: 32000
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
trainable params: 0 || all params: 567300096 || trainable%: 0.0
model slices: dict_keys([0])
model partition: 0head-2/1tail-30
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
7473 train examples
1319 test examples
X: <class 'numpy.ndarray'> 7473 1319
y <class 'numpy.ndarray'> 7473 1319
Passive Party 0: init Adversarial Trainining Defense
Adversarial_MLP init: 512 4096
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 32000
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:21<01:45, 21.07s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:32<01:00, 15.13s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:40<00:35, 11.90s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:47<00:47, 15.91s/it]
2024-10-06 00:42:51.149 | WARNING  | models.llm_models.base:from_pretrained:99 - RuntimeError('CUDA out of memory. Tried to allocate 224.00 MiB (GPU 1; 39.39 GiB total capacity; 7.81 GiB already allocated; 17.31 MiB free; 7.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')
Try to load from raw model
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:25<?, ?it/s]
Traceback (most recent call last):
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/base.py", line 97, in from_pretrained
    return self.from_vfl(model_name_or_path, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/base.py", line 122, in from_vfl
    return self.from_vfl(self._vfl_model_folder(model_name_or_path), **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/base.py", line 128, in from_vfl
    _model = self._load_model_tail(model_path, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/mistral.py", line 550, in _load_model_tail
    model_tail = MistralTailForCausalLM.from_pretrained(model_name_or_path, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/modeling_utils.py", line 3677, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/modeling_utils.py", line 4104, in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/modeling_utils.py", line 886, in _load_state_dict_into_meta_model
    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/accelerate/utils/modeling.py", line 317, in set_module_tensor_to_device
    new_value = value.to(device)
RuntimeError: CUDA out of memory. Tried to allocate 224.00 MiB (GPU 1; 39.39 GiB total capacity; 7.81 GiB already allocated; 17.31 MiB free; 7.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 296, in <module>
    args = load_parties_llm(args)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/load/LoadParty.py", line 43, in load_parties_llm
    args.parties[args.k - 1] = get_class_constructor(args.active_party_class)(args, args.k - 1, need_data=need_data)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 24, in __init__
    super().__init__(args, index, need_data=need_data, need_model=need_model)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 107, in __init__
    self.prepare_model(args, index)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 276, in prepare_model
    result = load_models_per_party_llm(args, index)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/load/LoadModels.py", line 373, in load_models_per_party_llm
    result = load_basic_models_llm(args,index)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/load/LoadModels.py", line 358, in load_basic_models_llm
    result = loader.load(args=args, model_path=model_path, is_active_party = is_active_party)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/load/llm_model_loaders/MistralModelLoader.py", line 29, in load
    self._models=p.from_pretrained(model_path, **args.kwargs_model_loading)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/base.py", line 100, in from_pretrained
    return self._from_raw(model_name_or_path, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/base.py", line 147, in _from_raw
    _model = self._load_model_tail(model_name_or_path, do_split=True, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/mistral.py", line 550, in _load_model_tail
    model_tail = MistralTailForCausalLM.from_pretrained(model_name_or_path, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/modeling_utils.py", line 3677, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/modeling_utils.py", line 4104, in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/modeling_utils.py", line 886, in _load_state_dict_into_meta_model
    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/accelerate/utils/modeling.py", line 317, in set_module_tensor_to_device
    new_value = value.to(device)
RuntimeError: CUDA out of memory. Tried to allocate 502.00 MiB (GPU 1; 39.39 GiB total capacity; 7.81 GiB already allocated; 17.31 MiB free; 7.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-06 00:43:30.623 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-06 00:43:37.595 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/meta-mathMetaMath-Mistral-7B_vfl_(2,)
2024-10-06 00:44:04.973 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/meta-mathMetaMath-Mistral-7B_vfl_(2,)
================= iter seed  60  =================
communication_protocol: FedSGD
load model_architect: CLM
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 2
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 0
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: AdversarialTraining
Defense_Config: {'party': [0], 'position': 'head', 'adversarial_model': 'Mapping_MLP3_noflatten', 'adversarial_model_lr': 0.001, 'imagined_adversary': 'ImaginedAdversary_MLP3_noflatten', 'imagined_adversary_lr': 0.001, 'lambda': 0.1, 'seq_length': 512, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/GMS8K/2-slice/2_0/AdversarialTraining_0.1,finetuned_model=meta-mathMetaMath-Mistral-7B.txt
=================================

===== iter 60 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
default tokenizer.pad_token: [PAD]
pad_token: [PAD]   pad_id: 32000
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
trainable params: 0 || all params: 567300096 || trainable%: 0.0
model slices: dict_keys([0])
model partition: 0head-2/1tail-30
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
7473 train examples
1319 test examples
X: <class 'numpy.ndarray'> 7473 1319
y <class 'numpy.ndarray'> 7473 1319
Passive Party 0: init Adversarial Trainining Defense
Adversarial_MLP init: 512 4096
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 32000
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:08<00:44,  8.94s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:24<00:51, 12.97s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:34<00:33, 11.29s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:42<00:42, 14.14s/it]
2024-10-06 00:44:48.372 | WARNING  | models.llm_models.base:from_pretrained:99 - RuntimeError('CUDA out of memory. Tried to allocate 224.00 MiB (GPU 1; 39.39 GiB total capacity; 7.81 GiB already allocated; 17.31 MiB free; 7.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')
Try to load from raw model
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:28<?, ?it/s]
Traceback (most recent call last):
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/base.py", line 97, in from_pretrained
    return self.from_vfl(model_name_or_path, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/base.py", line 122, in from_vfl
    return self.from_vfl(self._vfl_model_folder(model_name_or_path), **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/base.py", line 128, in from_vfl
    _model = self._load_model_tail(model_path, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/mistral.py", line 550, in _load_model_tail
    model_tail = MistralTailForCausalLM.from_pretrained(model_name_or_path, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/modeling_utils.py", line 3677, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/modeling_utils.py", line 4104, in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/modeling_utils.py", line 886, in _load_state_dict_into_meta_model
    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/accelerate/utils/modeling.py", line 317, in set_module_tensor_to_device
    new_value = value.to(device)
RuntimeError: CUDA out of memory. Tried to allocate 224.00 MiB (GPU 1; 39.39 GiB total capacity; 7.81 GiB already allocated; 17.31 MiB free; 7.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 296, in <module>
    args = load_parties_llm(args)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/load/LoadParty.py", line 43, in load_parties_llm
    args.parties[args.k - 1] = get_class_constructor(args.active_party_class)(args, args.k - 1, need_data=need_data)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 24, in __init__
    super().__init__(args, index, need_data=need_data, need_model=need_model)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 107, in __init__
    self.prepare_model(args, index)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 276, in prepare_model
    result = load_models_per_party_llm(args, index)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/load/LoadModels.py", line 373, in load_models_per_party_llm
    result = load_basic_models_llm(args,index)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/load/LoadModels.py", line 358, in load_basic_models_llm
    result = loader.load(args=args, model_path=model_path, is_active_party = is_active_party)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/load/llm_model_loaders/MistralModelLoader.py", line 29, in load
    self._models=p.from_pretrained(model_path, **args.kwargs_model_loading)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/base.py", line 100, in from_pretrained
    return self._from_raw(model_name_or_path, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/base.py", line 147, in _from_raw
    _model = self._load_model_tail(model_name_or_path, do_split=True, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/mistral.py", line 550, in _load_model_tail
    model_tail = MistralTailForCausalLM.from_pretrained(model_name_or_path, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/modeling_utils.py", line 3677, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/modeling_utils.py", line 4104, in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/modeling_utils.py", line 886, in _load_state_dict_into_meta_model
    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/accelerate/utils/modeling.py", line 317, in set_module_tensor_to_device
    new_value = value.to(device)
RuntimeError: CUDA out of memory. Tried to allocate 502.00 MiB (GPU 1; 39.39 GiB total capacity; 7.81 GiB already allocated; 17.31 MiB free; 7.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-06 00:45:31.565 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-06 00:45:38.633 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/meta-mathMetaMath-Mistral-7B_vfl_(2,)
2024-10-06 00:46:03.515 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/meta-mathMetaMath-Mistral-7B_vfl_(2,)
================= iter seed  61  =================
communication_protocol: FedSGD
load model_architect: CLM
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 2
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 0
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: AdversarialTraining
Defense_Config: {'party': [0], 'position': 'head', 'adversarial_model': 'Mapping_MLP3_noflatten', 'adversarial_model_lr': 0.001, 'imagined_adversary': 'ImaginedAdversary_MLP3_noflatten', 'imagined_adversary_lr': 0.001, 'lambda': 0.001, 'seq_length': 512, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/GMS8K/2-slice/2_0/AdversarialTraining_0.001,finetuned_model=meta-mathMetaMath-Mistral-7B.txt
=================================

===== iter 61 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
default tokenizer.pad_token: [PAD]
pad_token: [PAD]   pad_id: 32000
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
trainable params: 0 || all params: 567300096 || trainable%: 0.0
model slices: dict_keys([0])
model partition: 0head-2/1tail-30
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
7473 train examples
1319 test examples
X: <class 'numpy.ndarray'> 7473 1319
y <class 'numpy.ndarray'> 7473 1319
Passive Party 0: init Adversarial Trainining Defense
Adversarial_MLP init: 512 4096
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 32000
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:13<01:06, 13.24s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:29<01:00, 15.16s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:40<00:39, 13.16s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:50<00:50, 16.83s/it]
2024-10-06 00:46:55.059 | WARNING  | models.llm_models.base:from_pretrained:99 - RuntimeError('CUDA out of memory. Tried to allocate 224.00 MiB (GPU 1; 39.39 GiB total capacity; 7.81 GiB already allocated; 17.31 MiB free; 7.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')
Try to load from raw model
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:27<?, ?it/s]
Traceback (most recent call last):
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/base.py", line 97, in from_pretrained
    return self.from_vfl(model_name_or_path, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/base.py", line 122, in from_vfl
    return self.from_vfl(self._vfl_model_folder(model_name_or_path), **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/base.py", line 128, in from_vfl
    _model = self._load_model_tail(model_path, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/mistral.py", line 550, in _load_model_tail
    model_tail = MistralTailForCausalLM.from_pretrained(model_name_or_path, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/modeling_utils.py", line 3677, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/modeling_utils.py", line 4104, in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/modeling_utils.py", line 886, in _load_state_dict_into_meta_model
    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/accelerate/utils/modeling.py", line 317, in set_module_tensor_to_device
    new_value = value.to(device)
RuntimeError: CUDA out of memory. Tried to allocate 224.00 MiB (GPU 1; 39.39 GiB total capacity; 7.81 GiB already allocated; 17.31 MiB free; 7.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 296, in <module>
    args = load_parties_llm(args)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/load/LoadParty.py", line 43, in load_parties_llm
    args.parties[args.k - 1] = get_class_constructor(args.active_party_class)(args, args.k - 1, need_data=need_data)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 24, in __init__
    super().__init__(args, index, need_data=need_data, need_model=need_model)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 107, in __init__
    self.prepare_model(args, index)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 276, in prepare_model
    result = load_models_per_party_llm(args, index)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/load/LoadModels.py", line 373, in load_models_per_party_llm
    result = load_basic_models_llm(args,index)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/load/LoadModels.py", line 358, in load_basic_models_llm
    result = loader.load(args=args, model_path=model_path, is_active_party = is_active_party)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/load/llm_model_loaders/MistralModelLoader.py", line 29, in load
    self._models=p.from_pretrained(model_path, **args.kwargs_model_loading)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/base.py", line 100, in from_pretrained
    return self._from_raw(model_name_or_path, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/base.py", line 147, in _from_raw
    _model = self._load_model_tail(model_name_or_path, do_split=True, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/mistral.py", line 550, in _load_model_tail
    model_tail = MistralTailForCausalLM.from_pretrained(model_name_or_path, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/modeling_utils.py", line 3677, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/modeling_utils.py", line 4104, in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/modeling_utils.py", line 886, in _load_state_dict_into_meta_model
    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/accelerate/utils/modeling.py", line 317, in set_module_tensor_to_device
    new_value = value.to(device)
RuntimeError: CUDA out of memory. Tried to allocate 502.00 MiB (GPU 1; 39.39 GiB total capacity; 7.81 GiB already allocated; 17.31 MiB free; 7.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-06 00:47:36.884 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-06 00:47:42.711 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/meta-mathMetaMath-Mistral-7B_vfl_(2,)
2024-10-06 00:48:07.306 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/meta-mathMetaMath-Mistral-7B_vfl_(2,)
================= iter seed  61  =================
communication_protocol: FedSGD
load model_architect: CLM
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 2
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 0
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: AdversarialTraining
Defense_Config: {'party': [0], 'position': 'head', 'adversarial_model': 'Mapping_MLP3_noflatten', 'adversarial_model_lr': 0.001, 'imagined_adversary': 'ImaginedAdversary_MLP3_noflatten', 'imagined_adversary_lr': 0.001, 'lambda': 0.01, 'seq_length': 512, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/GMS8K/2-slice/2_0/AdversarialTraining_0.01,finetuned_model=meta-mathMetaMath-Mistral-7B.txt
=================================

===== iter 61 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
default tokenizer.pad_token: [PAD]
pad_token: [PAD]   pad_id: 32000
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
trainable params: 0 || all params: 567300096 || trainable%: 0.0
model slices: dict_keys([0])
model partition: 0head-2/1tail-30
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
7473 train examples
1319 test examples
X: <class 'numpy.ndarray'> 7473 1319
y <class 'numpy.ndarray'> 7473 1319
Passive Party 0: init Adversarial Trainining Defense
Adversarial_MLP init: 512 4096
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 32000
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:13<01:06, 13.28s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:27<00:55, 13.94s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:43<00:44, 14.93s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:53<00:53, 17.88s/it]
2024-10-06 00:49:01.754 | WARNING  | models.llm_models.base:from_pretrained:99 - RuntimeError('CUDA out of memory. Tried to allocate 224.00 MiB (GPU 1; 39.39 GiB total capacity; 7.81 GiB already allocated; 17.31 MiB free; 7.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')
Try to load from raw model
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:24<?, ?it/s]
Traceback (most recent call last):
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/base.py", line 97, in from_pretrained
    return self.from_vfl(model_name_or_path, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/base.py", line 122, in from_vfl
    return self.from_vfl(self._vfl_model_folder(model_name_or_path), **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/base.py", line 128, in from_vfl
    _model = self._load_model_tail(model_path, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/mistral.py", line 550, in _load_model_tail
    model_tail = MistralTailForCausalLM.from_pretrained(model_name_or_path, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/modeling_utils.py", line 3677, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/modeling_utils.py", line 4104, in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/modeling_utils.py", line 886, in _load_state_dict_into_meta_model
    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/accelerate/utils/modeling.py", line 317, in set_module_tensor_to_device
    new_value = value.to(device)
RuntimeError: CUDA out of memory. Tried to allocate 224.00 MiB (GPU 1; 39.39 GiB total capacity; 7.81 GiB already allocated; 17.31 MiB free; 7.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 296, in <module>
    args = load_parties_llm(args)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/load/LoadParty.py", line 43, in load_parties_llm
    args.parties[args.k - 1] = get_class_constructor(args.active_party_class)(args, args.k - 1, need_data=need_data)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 24, in __init__
    super().__init__(args, index, need_data=need_data, need_model=need_model)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 107, in __init__
    self.prepare_model(args, index)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 276, in prepare_model
    result = load_models_per_party_llm(args, index)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/load/LoadModels.py", line 373, in load_models_per_party_llm
    result = load_basic_models_llm(args,index)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/load/LoadModels.py", line 358, in load_basic_models_llm
    result = loader.load(args=args, model_path=model_path, is_active_party = is_active_party)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/load/llm_model_loaders/MistralModelLoader.py", line 29, in load
    self._models=p.from_pretrained(model_path, **args.kwargs_model_loading)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/base.py", line 100, in from_pretrained
    return self._from_raw(model_name_or_path, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/base.py", line 147, in _from_raw
    _model = self._load_model_tail(model_name_or_path, do_split=True, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/mistral.py", line 550, in _load_model_tail
    model_tail = MistralTailForCausalLM.from_pretrained(model_name_or_path, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/modeling_utils.py", line 3677, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/modeling_utils.py", line 4104, in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/modeling_utils.py", line 886, in _load_state_dict_into_meta_model
    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/accelerate/utils/modeling.py", line 317, in set_module_tensor_to_device
    new_value = value.to(device)
RuntimeError: CUDA out of memory. Tried to allocate 502.00 MiB (GPU 1; 39.39 GiB total capacity; 7.81 GiB already allocated; 17.31 MiB free; 7.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-06 00:49:41.678 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-06 00:49:49.025 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/meta-mathMetaMath-Mistral-7B_vfl_(2,)
2024-10-06 00:50:13.680 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/meta-mathMetaMath-Mistral-7B_vfl_(2,)
================= iter seed  61  =================
communication_protocol: FedSGD
load model_architect: CLM
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 2
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 0
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: AdversarialTraining
Defense_Config: {'party': [0], 'position': 'head', 'adversarial_model': 'Mapping_MLP3_noflatten', 'adversarial_model_lr': 0.001, 'imagined_adversary': 'ImaginedAdversary_MLP3_noflatten', 'imagined_adversary_lr': 0.001, 'lambda': 0.1, 'seq_length': 512, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/GMS8K/2-slice/2_0/AdversarialTraining_0.1,finetuned_model=meta-mathMetaMath-Mistral-7B.txt
=================================

===== iter 61 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
default tokenizer.pad_token: [PAD]
pad_token: [PAD]   pad_id: 32000
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
trainable params: 0 || all params: 567300096 || trainable%: 0.0
model slices: dict_keys([0])
model partition: 0head-2/1tail-30
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
7473 train examples
1319 test examples
X: <class 'numpy.ndarray'> 7473 1319
y <class 'numpy.ndarray'> 7473 1319
Passive Party 0: init Adversarial Trainining Defense
Adversarial_MLP init: 512 4096
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 32000
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:10<00:51, 10.36s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:22<00:45, 11.28s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:36<00:38, 12.79s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:47<00:47, 15.89s/it]
2024-10-06 00:51:02.227 | WARNING  | models.llm_models.base:from_pretrained:99 - RuntimeError('CUDA out of memory. Tried to allocate 224.00 MiB (GPU 1; 39.39 GiB total capacity; 7.81 GiB already allocated; 17.31 MiB free; 7.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')
Try to load from raw model
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:25<?, ?it/s]
Traceback (most recent call last):
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/base.py", line 97, in from_pretrained
    return self.from_vfl(model_name_or_path, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/base.py", line 122, in from_vfl
    return self.from_vfl(self._vfl_model_folder(model_name_or_path), **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/base.py", line 128, in from_vfl
    _model = self._load_model_tail(model_path, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/mistral.py", line 550, in _load_model_tail
    model_tail = MistralTailForCausalLM.from_pretrained(model_name_or_path, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/modeling_utils.py", line 3677, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/modeling_utils.py", line 4104, in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/modeling_utils.py", line 886, in _load_state_dict_into_meta_model
    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/accelerate/utils/modeling.py", line 317, in set_module_tensor_to_device
    new_value = value.to(device)
RuntimeError: CUDA out of memory. Tried to allocate 224.00 MiB (GPU 1; 39.39 GiB total capacity; 7.81 GiB already allocated; 17.31 MiB free; 7.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 296, in <module>
    args = load_parties_llm(args)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/load/LoadParty.py", line 43, in load_parties_llm
    args.parties[args.k - 1] = get_class_constructor(args.active_party_class)(args, args.k - 1, need_data=need_data)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 24, in __init__
    super().__init__(args, index, need_data=need_data, need_model=need_model)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 107, in __init__
    self.prepare_model(args, index)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 276, in prepare_model
    result = load_models_per_party_llm(args, index)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/load/LoadModels.py", line 373, in load_models_per_party_llm
    result = load_basic_models_llm(args,index)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/load/LoadModels.py", line 358, in load_basic_models_llm
    result = loader.load(args=args, model_path=model_path, is_active_party = is_active_party)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/load/llm_model_loaders/MistralModelLoader.py", line 29, in load
    self._models=p.from_pretrained(model_path, **args.kwargs_model_loading)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/base.py", line 100, in from_pretrained
    return self._from_raw(model_name_or_path, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/base.py", line 147, in _from_raw
    _model = self._load_model_tail(model_name_or_path, do_split=True, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/mistral.py", line 550, in _load_model_tail
    model_tail = MistralTailForCausalLM.from_pretrained(model_name_or_path, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/modeling_utils.py", line 3677, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/modeling_utils.py", line 4104, in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/modeling_utils.py", line 886, in _load_state_dict_into_meta_model
    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/accelerate/utils/modeling.py", line 317, in set_module_tensor_to_device
    new_value = value.to(device)
RuntimeError: CUDA out of memory. Tried to allocate 502.00 MiB (GPU 1; 39.39 GiB total capacity; 7.81 GiB already allocated; 17.31 MiB free; 7.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-06 00:51:43.034 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-06 00:51:50.087 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/meta-mathMetaMath-Mistral-7B_vfl_(2,)
2024-10-06 00:52:14.432 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/meta-mathMetaMath-Mistral-7B_vfl_(2,)
================= iter seed  62  =================
communication_protocol: FedSGD
load model_architect: CLM
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 2
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 0
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: AdversarialTraining
Defense_Config: {'party': [0], 'position': 'head', 'adversarial_model': 'Mapping_MLP3_noflatten', 'adversarial_model_lr': 0.001, 'imagined_adversary': 'ImaginedAdversary_MLP3_noflatten', 'imagined_adversary_lr': 0.001, 'lambda': 0.001, 'seq_length': 512, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/GMS8K/2-slice/2_0/AdversarialTraining_0.001,finetuned_model=meta-mathMetaMath-Mistral-7B.txt
=================================

===== iter 62 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
default tokenizer.pad_token: [PAD]
pad_token: [PAD]   pad_id: 32000
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
trainable params: 0 || all params: 567300096 || trainable%: 0.0
model slices: dict_keys([0])
model partition: 0head-2/1tail-30
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
7473 train examples
1319 test examples
X: <class 'numpy.ndarray'> 7473 1319
y <class 'numpy.ndarray'> 7473 1319
Passive Party 0: init Adversarial Trainining Defense
Adversarial_MLP init: 512 4096
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 32000
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:08<00:40,  8.12s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:19<00:39,  9.84s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:30<00:31, 10.53s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:41<00:41, 13.71s/it]
2024-10-06 00:52:56.410 | WARNING  | models.llm_models.base:from_pretrained:99 - RuntimeError('CUDA out of memory. Tried to allocate 224.00 MiB (GPU 1; 39.39 GiB total capacity; 7.81 GiB already allocated; 17.31 MiB free; 7.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')
Try to load from raw model
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:29<?, ?it/s]
Traceback (most recent call last):
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/base.py", line 97, in from_pretrained
    return self.from_vfl(model_name_or_path, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/base.py", line 122, in from_vfl
    return self.from_vfl(self._vfl_model_folder(model_name_or_path), **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/base.py", line 128, in from_vfl
    _model = self._load_model_tail(model_path, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/mistral.py", line 550, in _load_model_tail
    model_tail = MistralTailForCausalLM.from_pretrained(model_name_or_path, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/modeling_utils.py", line 3677, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/modeling_utils.py", line 4104, in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/modeling_utils.py", line 886, in _load_state_dict_into_meta_model
    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/accelerate/utils/modeling.py", line 317, in set_module_tensor_to_device
    new_value = value.to(device)
RuntimeError: CUDA out of memory. Tried to allocate 224.00 MiB (GPU 1; 39.39 GiB total capacity; 7.81 GiB already allocated; 17.31 MiB free; 7.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 296, in <module>
    args = load_parties_llm(args)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/load/LoadParty.py", line 43, in load_parties_llm
    args.parties[args.k - 1] = get_class_constructor(args.active_party_class)(args, args.k - 1, need_data=need_data)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 24, in __init__
    super().__init__(args, index, need_data=need_data, need_model=need_model)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 107, in __init__
    self.prepare_model(args, index)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 276, in prepare_model
    result = load_models_per_party_llm(args, index)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/load/LoadModels.py", line 373, in load_models_per_party_llm
    result = load_basic_models_llm(args,index)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/load/LoadModels.py", line 358, in load_basic_models_llm
    result = loader.load(args=args, model_path=model_path, is_active_party = is_active_party)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/load/llm_model_loaders/MistralModelLoader.py", line 29, in load
    self._models=p.from_pretrained(model_path, **args.kwargs_model_loading)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/base.py", line 100, in from_pretrained
    return self._from_raw(model_name_or_path, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/base.py", line 147, in _from_raw
    _model = self._load_model_tail(model_name_or_path, do_split=True, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/mistral.py", line 550, in _load_model_tail
    model_tail = MistralTailForCausalLM.from_pretrained(model_name_or_path, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/modeling_utils.py", line 3677, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/modeling_utils.py", line 4104, in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/modeling_utils.py", line 886, in _load_state_dict_into_meta_model
    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/accelerate/utils/modeling.py", line 317, in set_module_tensor_to_device
    new_value = value.to(device)
RuntimeError: CUDA out of memory. Tried to allocate 502.00 MiB (GPU 1; 39.39 GiB total capacity; 7.81 GiB already allocated; 17.31 MiB free; 7.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-06 00:53:41.599 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-06 00:53:48.861 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/meta-mathMetaMath-Mistral-7B_vfl_(2,)
2024-10-06 00:54:13.576 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/meta-mathMetaMath-Mistral-7B_vfl_(2,)
================= iter seed  62  =================
communication_protocol: FedSGD
load model_architect: CLM
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 2
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 0
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: AdversarialTraining
Defense_Config: {'party': [0], 'position': 'head', 'adversarial_model': 'Mapping_MLP3_noflatten', 'adversarial_model_lr': 0.001, 'imagined_adversary': 'ImaginedAdversary_MLP3_noflatten', 'imagined_adversary_lr': 0.001, 'lambda': 0.01, 'seq_length': 512, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/GMS8K/2-slice/2_0/AdversarialTraining_0.01,finetuned_model=meta-mathMetaMath-Mistral-7B.txt
=================================

===== iter 62 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
default tokenizer.pad_token: [PAD]
pad_token: [PAD]   pad_id: 32000
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
trainable params: 0 || all params: 567300096 || trainable%: 0.0
model slices: dict_keys([0])
model partition: 0head-2/1tail-30
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
7473 train examples
1319 test examples
X: <class 'numpy.ndarray'> 7473 1319
y <class 'numpy.ndarray'> 7473 1319
Passive Party 0: init Adversarial Trainining Defense
Adversarial_MLP init: 512 4096
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 32000
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:08<00:44,  8.84s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:17<00:34,  8.52s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:26<00:26,  8.92s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:37<00:37, 12.36s/it]
2024-10-06 00:54:51.359 | WARNING  | models.llm_models.base:from_pretrained:99 - RuntimeError('CUDA out of memory. Tried to allocate 224.00 MiB (GPU 1; 39.39 GiB total capacity; 7.81 GiB already allocated; 17.31 MiB free; 7.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')
Try to load from raw model
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:11<?, ?it/s]
Traceback (most recent call last):
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/base.py", line 97, in from_pretrained
    return self.from_vfl(model_name_or_path, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/base.py", line 122, in from_vfl
    return self.from_vfl(self._vfl_model_folder(model_name_or_path), **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/base.py", line 128, in from_vfl
    _model = self._load_model_tail(model_path, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/mistral.py", line 550, in _load_model_tail
    model_tail = MistralTailForCausalLM.from_pretrained(model_name_or_path, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/modeling_utils.py", line 3677, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/modeling_utils.py", line 4104, in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/modeling_utils.py", line 886, in _load_state_dict_into_meta_model
    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/accelerate/utils/modeling.py", line 317, in set_module_tensor_to_device
    new_value = value.to(device)
RuntimeError: CUDA out of memory. Tried to allocate 224.00 MiB (GPU 1; 39.39 GiB total capacity; 7.81 GiB already allocated; 17.31 MiB free; 7.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 296, in <module>
    args = load_parties_llm(args)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/load/LoadParty.py", line 43, in load_parties_llm
    args.parties[args.k - 1] = get_class_constructor(args.active_party_class)(args, args.k - 1, need_data=need_data)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 24, in __init__
    super().__init__(args, index, need_data=need_data, need_model=need_model)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 107, in __init__
    self.prepare_model(args, index)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 276, in prepare_model
    result = load_models_per_party_llm(args, index)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/load/LoadModels.py", line 373, in load_models_per_party_llm
    result = load_basic_models_llm(args,index)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/load/LoadModels.py", line 358, in load_basic_models_llm
    result = loader.load(args=args, model_path=model_path, is_active_party = is_active_party)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/load/llm_model_loaders/MistralModelLoader.py", line 29, in load
    self._models=p.from_pretrained(model_path, **args.kwargs_model_loading)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/base.py", line 100, in from_pretrained
    return self._from_raw(model_name_or_path, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/base.py", line 147, in _from_raw
    _model = self._load_model_tail(model_name_or_path, do_split=True, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/mistral.py", line 550, in _load_model_tail
    model_tail = MistralTailForCausalLM.from_pretrained(model_name_or_path, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/modeling_utils.py", line 3677, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/modeling_utils.py", line 4104, in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/modeling_utils.py", line 886, in _load_state_dict_into_meta_model
    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/accelerate/utils/modeling.py", line 317, in set_module_tensor_to_device
    new_value = value.to(device)
RuntimeError: CUDA out of memory. Tried to allocate 502.00 MiB (GPU 1; 39.39 GiB total capacity; 7.81 GiB already allocated; 17.31 MiB free; 7.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-06 00:55:20.749 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-06 00:55:28.541 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/meta-mathMetaMath-Mistral-7B_vfl_(2,)
2024-10-06 00:55:54.400 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/meta-mathMetaMath-Mistral-7B_vfl_(2,)
================= iter seed  62  =================
communication_protocol: FedSGD
load model_architect: CLM
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 2
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 0
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: AdversarialTraining
Defense_Config: {'party': [0], 'position': 'head', 'adversarial_model': 'Mapping_MLP3_noflatten', 'adversarial_model_lr': 0.001, 'imagined_adversary': 'ImaginedAdversary_MLP3_noflatten', 'imagined_adversary_lr': 0.001, 'lambda': 0.1, 'seq_length': 512, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/GMS8K/2-slice/2_0/AdversarialTraining_0.1,finetuned_model=meta-mathMetaMath-Mistral-7B.txt
=================================

===== iter 62 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
default tokenizer.pad_token: [PAD]
pad_token: [PAD]   pad_id: 32000
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
trainable params: 0 || all params: 567300096 || trainable%: 0.0
model slices: dict_keys([0])
model partition: 0head-2/1tail-30
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
7473 train examples
1319 test examples
X: <class 'numpy.ndarray'> 7473 1319
y <class 'numpy.ndarray'> 7473 1319
Passive Party 0: init Adversarial Trainining Defense
Adversarial_MLP init: 512 4096
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 32000
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:13<01:07, 13.59s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:21<00:40, 10.01s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:28<00:27,  9.04s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:36<00:36, 12.06s/it]
2024-10-06 00:56:31.383 | WARNING  | models.llm_models.base:from_pretrained:99 - RuntimeError('CUDA out of memory. Tried to allocate 224.00 MiB (GPU 1; 39.39 GiB total capacity; 7.81 GiB already allocated; 17.31 MiB free; 7.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')
Try to load from raw model
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:30<?, ?it/s]
Traceback (most recent call last):
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/base.py", line 97, in from_pretrained
    return self.from_vfl(model_name_or_path, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/base.py", line 122, in from_vfl
    return self.from_vfl(self._vfl_model_folder(model_name_or_path), **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/base.py", line 128, in from_vfl
    _model = self._load_model_tail(model_path, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/mistral.py", line 550, in _load_model_tail
    model_tail = MistralTailForCausalLM.from_pretrained(model_name_or_path, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/modeling_utils.py", line 3677, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/modeling_utils.py", line 4104, in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/modeling_utils.py", line 886, in _load_state_dict_into_meta_model
    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/accelerate/utils/modeling.py", line 317, in set_module_tensor_to_device
    new_value = value.to(device)
RuntimeError: CUDA out of memory. Tried to allocate 224.00 MiB (GPU 1; 39.39 GiB total capacity; 7.81 GiB already allocated; 17.31 MiB free; 7.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 296, in <module>
    args = load_parties_llm(args)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/load/LoadParty.py", line 43, in load_parties_llm
    args.parties[args.k - 1] = get_class_constructor(args.active_party_class)(args, args.k - 1, need_data=need_data)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 24, in __init__
    super().__init__(args, index, need_data=need_data, need_model=need_model)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 107, in __init__
    self.prepare_model(args, index)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 276, in prepare_model
    result = load_models_per_party_llm(args, index)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/load/LoadModels.py", line 373, in load_models_per_party_llm
    result = load_basic_models_llm(args,index)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/load/LoadModels.py", line 358, in load_basic_models_llm
    result = loader.load(args=args, model_path=model_path, is_active_party = is_active_party)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/load/llm_model_loaders/MistralModelLoader.py", line 29, in load
    self._models=p.from_pretrained(model_path, **args.kwargs_model_loading)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/base.py", line 100, in from_pretrained
    return self._from_raw(model_name_or_path, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/base.py", line 147, in _from_raw
    _model = self._load_model_tail(model_name_or_path, do_split=True, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/mistral.py", line 550, in _load_model_tail
    model_tail = MistralTailForCausalLM.from_pretrained(model_name_or_path, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/modeling_utils.py", line 3677, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/modeling_utils.py", line 4104, in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/modeling_utils.py", line 886, in _load_state_dict_into_meta_model
    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/accelerate/utils/modeling.py", line 317, in set_module_tensor_to_device
    new_value = value.to(device)
RuntimeError: CUDA out of memory. Tried to allocate 502.00 MiB (GPU 1; 39.39 GiB total capacity; 7.81 GiB already allocated; 17.31 MiB free; 7.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-06 00:57:16.538 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-06 00:57:24.570 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/meta-mathMetaMath-Mistral-7B_vfl_(2,)
2024-10-06 00:57:51.944 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/meta-mathMetaMath-Mistral-7B_vfl_(2,)
================= iter seed  63  =================
communication_protocol: FedSGD
load model_architect: CLM
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 2
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 0
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: AdversarialTraining
Defense_Config: {'party': [0], 'position': 'head', 'adversarial_model': 'Mapping_MLP3_noflatten', 'adversarial_model_lr': 0.001, 'imagined_adversary': 'ImaginedAdversary_MLP3_noflatten', 'imagined_adversary_lr': 0.001, 'lambda': 0.001, 'seq_length': 512, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/GMS8K/2-slice/2_0/AdversarialTraining_0.001,finetuned_model=meta-mathMetaMath-Mistral-7B.txt
=================================

===== iter 63 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
default tokenizer.pad_token: [PAD]
pad_token: [PAD]   pad_id: 32000
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
trainable params: 0 || all params: 567300096 || trainable%: 0.0
model slices: dict_keys([0])
model partition: 0head-2/1tail-30
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
7473 train examples
1319 test examples
X: <class 'numpy.ndarray'> 7473 1319
y <class 'numpy.ndarray'> 7473 1319
Passive Party 0: init Adversarial Trainining Defense
Adversarial_MLP init: 512 4096
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 32000
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:14<01:12, 14.51s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:24<00:46, 11.69s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:31<00:29,  9.75s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:38<00:38, 12.81s/it]
2024-10-06 00:58:31.181 | WARNING  | models.llm_models.base:from_pretrained:99 - RuntimeError('CUDA out of memory. Tried to allocate 224.00 MiB (GPU 1; 39.39 GiB total capacity; 7.81 GiB already allocated; 17.31 MiB free; 7.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')
Try to load from raw model
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:20<?, ?it/s]
Traceback (most recent call last):
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/base.py", line 97, in from_pretrained
    return self.from_vfl(model_name_or_path, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/base.py", line 122, in from_vfl
    return self.from_vfl(self._vfl_model_folder(model_name_or_path), **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/base.py", line 128, in from_vfl
    _model = self._load_model_tail(model_path, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/mistral.py", line 550, in _load_model_tail
    model_tail = MistralTailForCausalLM.from_pretrained(model_name_or_path, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/modeling_utils.py", line 3677, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/modeling_utils.py", line 4104, in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/modeling_utils.py", line 886, in _load_state_dict_into_meta_model
    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/accelerate/utils/modeling.py", line 317, in set_module_tensor_to_device
    new_value = value.to(device)
RuntimeError: CUDA out of memory. Tried to allocate 224.00 MiB (GPU 1; 39.39 GiB total capacity; 7.81 GiB already allocated; 17.31 MiB free; 7.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 296, in <module>
    args = load_parties_llm(args)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/load/LoadParty.py", line 43, in load_parties_llm
    args.parties[args.k - 1] = get_class_constructor(args.active_party_class)(args, args.k - 1, need_data=need_data)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 24, in __init__
    super().__init__(args, index, need_data=need_data, need_model=need_model)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 107, in __init__
    self.prepare_model(args, index)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 276, in prepare_model
    result = load_models_per_party_llm(args, index)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/load/LoadModels.py", line 373, in load_models_per_party_llm
    result = load_basic_models_llm(args,index)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/load/LoadModels.py", line 358, in load_basic_models_llm
    result = loader.load(args=args, model_path=model_path, is_active_party = is_active_party)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/load/llm_model_loaders/MistralModelLoader.py", line 29, in load
    self._models=p.from_pretrained(model_path, **args.kwargs_model_loading)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/base.py", line 100, in from_pretrained
    return self._from_raw(model_name_or_path, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/base.py", line 147, in _from_raw
    _model = self._load_model_tail(model_name_or_path, do_split=True, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/mistral.py", line 550, in _load_model_tail
    model_tail = MistralTailForCausalLM.from_pretrained(model_name_or_path, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/modeling_utils.py", line 3677, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/modeling_utils.py", line 4104, in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/modeling_utils.py", line 886, in _load_state_dict_into_meta_model
    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/accelerate/utils/modeling.py", line 317, in set_module_tensor_to_device
    new_value = value.to(device)
RuntimeError: CUDA out of memory. Tried to allocate 502.00 MiB (GPU 1; 39.39 GiB total capacity; 7.81 GiB already allocated; 17.31 MiB free; 7.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-06 00:59:04.976 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-06 00:59:11.693 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/meta-mathMetaMath-Mistral-7B_vfl_(2,)
2024-10-06 00:59:43.316 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/meta-mathMetaMath-Mistral-7B_vfl_(2,)
================= iter seed  63  =================
communication_protocol: FedSGD
load model_architect: CLM
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 2
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 0
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: AdversarialTraining
Defense_Config: {'party': [0], 'position': 'head', 'adversarial_model': 'Mapping_MLP3_noflatten', 'adversarial_model_lr': 0.001, 'imagined_adversary': 'ImaginedAdversary_MLP3_noflatten', 'imagined_adversary_lr': 0.001, 'lambda': 0.01, 'seq_length': 512, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/GMS8K/2-slice/2_0/AdversarialTraining_0.01,finetuned_model=meta-mathMetaMath-Mistral-7B.txt
=================================

===== iter 63 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
default tokenizer.pad_token: [PAD]
pad_token: [PAD]   pad_id: 32000
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
trainable params: 0 || all params: 567300096 || trainable%: 0.0
model slices: dict_keys([0])
model partition: 0head-2/1tail-30
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
7473 train examples
1319 test examples
X: <class 'numpy.ndarray'> 7473 1319
y <class 'numpy.ndarray'> 7473 1319
Passive Party 0: init Adversarial Trainining Defense
Adversarial_MLP init: 512 4096
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 32000
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:13<01:07, 13.49s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:22<00:42, 10.66s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:32<00:32, 10.68s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:41<00:41, 13.92s/it]
2024-10-06 01:00:25.849 | WARNING  | models.llm_models.base:from_pretrained:99 - RuntimeError('CUDA out of memory. Tried to allocate 224.00 MiB (GPU 1; 39.39 GiB total capacity; 7.81 GiB already allocated; 17.31 MiB free; 7.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')
Try to load from raw model
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:19<?, ?it/s]
Traceback (most recent call last):
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/base.py", line 97, in from_pretrained
    return self.from_vfl(model_name_or_path, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/base.py", line 122, in from_vfl
    return self.from_vfl(self._vfl_model_folder(model_name_or_path), **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/base.py", line 128, in from_vfl
    _model = self._load_model_tail(model_path, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/mistral.py", line 550, in _load_model_tail
    model_tail = MistralTailForCausalLM.from_pretrained(model_name_or_path, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/modeling_utils.py", line 3677, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/modeling_utils.py", line 4104, in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/modeling_utils.py", line 886, in _load_state_dict_into_meta_model
    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/accelerate/utils/modeling.py", line 317, in set_module_tensor_to_device
    new_value = value.to(device)
RuntimeError: CUDA out of memory. Tried to allocate 224.00 MiB (GPU 1; 39.39 GiB total capacity; 7.81 GiB already allocated; 17.31 MiB free; 7.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 296, in <module>
    args = load_parties_llm(args)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/load/LoadParty.py", line 43, in load_parties_llm
    args.parties[args.k - 1] = get_class_constructor(args.active_party_class)(args, args.k - 1, need_data=need_data)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 24, in __init__
    super().__init__(args, index, need_data=need_data, need_model=need_model)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 107, in __init__
    self.prepare_model(args, index)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 276, in prepare_model
    result = load_models_per_party_llm(args, index)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/load/LoadModels.py", line 373, in load_models_per_party_llm
    result = load_basic_models_llm(args,index)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/load/LoadModels.py", line 358, in load_basic_models_llm
    result = loader.load(args=args, model_path=model_path, is_active_party = is_active_party)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/load/llm_model_loaders/MistralModelLoader.py", line 29, in load
    self._models=p.from_pretrained(model_path, **args.kwargs_model_loading)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/base.py", line 100, in from_pretrained
    return self._from_raw(model_name_or_path, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/base.py", line 147, in _from_raw
    _model = self._load_model_tail(model_name_or_path, do_split=True, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/mistral.py", line 550, in _load_model_tail
    model_tail = MistralTailForCausalLM.from_pretrained(model_name_or_path, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/modeling_utils.py", line 3677, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/modeling_utils.py", line 4104, in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/modeling_utils.py", line 886, in _load_state_dict_into_meta_model
    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/accelerate/utils/modeling.py", line 317, in set_module_tensor_to_device
    new_value = value.to(device)
RuntimeError: CUDA out of memory. Tried to allocate 502.00 MiB (GPU 1; 39.39 GiB total capacity; 7.81 GiB already allocated; 17.31 MiB free; 7.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-06 01:00:58.867 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-06 01:01:05.567 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/meta-mathMetaMath-Mistral-7B_vfl_(2,)
2024-10-06 01:01:28.785 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/meta-mathMetaMath-Mistral-7B_vfl_(2,)
================= iter seed  63  =================
communication_protocol: FedSGD
load model_architect: CLM
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 2
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 0
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: AdversarialTraining
Defense_Config: {'party': [0], 'position': 'head', 'adversarial_model': 'Mapping_MLP3_noflatten', 'adversarial_model_lr': 0.001, 'imagined_adversary': 'ImaginedAdversary_MLP3_noflatten', 'imagined_adversary_lr': 0.001, 'lambda': 0.1, 'seq_length': 512, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/GMS8K/2-slice/2_0/AdversarialTraining_0.1,finetuned_model=meta-mathMetaMath-Mistral-7B.txt
=================================

===== iter 63 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
default tokenizer.pad_token: [PAD]
pad_token: [PAD]   pad_id: 32000
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
trainable params: 0 || all params: 567300096 || trainable%: 0.0
model slices: dict_keys([0])
model partition: 0head-2/1tail-30
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
7473 train examples
1319 test examples
X: <class 'numpy.ndarray'> 7473 1319
y <class 'numpy.ndarray'> 7473 1319
Passive Party 0: init Adversarial Trainining Defense
Adversarial_MLP init: 512 4096
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 32000
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:18<01:32, 18.47s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:29<00:56, 14.04s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:37<00:33, 11.33s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:45<00:45, 15.09s/it]
2024-10-06 01:02:15.436 | WARNING  | models.llm_models.base:from_pretrained:99 - RuntimeError('CUDA out of memory. Tried to allocate 224.00 MiB (GPU 1; 39.39 GiB total capacity; 7.81 GiB already allocated; 17.31 MiB free; 7.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')
Try to load from raw model
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:25<?, ?it/s]
Traceback (most recent call last):
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/base.py", line 97, in from_pretrained
    return self.from_vfl(model_name_or_path, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/base.py", line 122, in from_vfl
    return self.from_vfl(self._vfl_model_folder(model_name_or_path), **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/base.py", line 128, in from_vfl
    _model = self._load_model_tail(model_path, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/mistral.py", line 550, in _load_model_tail
    model_tail = MistralTailForCausalLM.from_pretrained(model_name_or_path, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/modeling_utils.py", line 3677, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/modeling_utils.py", line 4104, in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/modeling_utils.py", line 886, in _load_state_dict_into_meta_model
    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/accelerate/utils/modeling.py", line 317, in set_module_tensor_to_device
    new_value = value.to(device)
RuntimeError: CUDA out of memory. Tried to allocate 224.00 MiB (GPU 1; 39.39 GiB total capacity; 7.81 GiB already allocated; 17.31 MiB free; 7.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 296, in <module>
    args = load_parties_llm(args)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/load/LoadParty.py", line 43, in load_parties_llm
    args.parties[args.k - 1] = get_class_constructor(args.active_party_class)(args, args.k - 1, need_data=need_data)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 24, in __init__
    super().__init__(args, index, need_data=need_data, need_model=need_model)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 107, in __init__
    self.prepare_model(args, index)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 276, in prepare_model
    result = load_models_per_party_llm(args, index)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/load/LoadModels.py", line 373, in load_models_per_party_llm
    result = load_basic_models_llm(args,index)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/load/LoadModels.py", line 358, in load_basic_models_llm
    result = loader.load(args=args, model_path=model_path, is_active_party = is_active_party)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/load/llm_model_loaders/MistralModelLoader.py", line 29, in load
    self._models=p.from_pretrained(model_path, **args.kwargs_model_loading)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/base.py", line 100, in from_pretrained
    return self._from_raw(model_name_or_path, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/base.py", line 147, in _from_raw
    _model = self._load_model_tail(model_name_or_path, do_split=True, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/mistral.py", line 550, in _load_model_tail
    model_tail = MistralTailForCausalLM.from_pretrained(model_name_or_path, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/modeling_utils.py", line 3677, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/modeling_utils.py", line 4104, in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/modeling_utils.py", line 886, in _load_state_dict_into_meta_model
    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/accelerate/utils/modeling.py", line 317, in set_module_tensor_to_device
    new_value = value.to(device)
RuntimeError: CUDA out of memory. Tried to allocate 502.00 MiB (GPU 1; 39.39 GiB total capacity; 7.81 GiB already allocated; 17.31 MiB free; 7.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-06 01:02:55.874 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-06 01:03:02.994 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/meta-mathMetaMath-Mistral-7B_vfl_(2,)
2024-10-06 01:03:30.443 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/meta-mathMetaMath-Mistral-7B_vfl_(2,)
================= iter seed  64  =================
communication_protocol: FedSGD
load model_architect: CLM
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 2
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 0
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: AdversarialTraining
Defense_Config: {'party': [0], 'position': 'head', 'adversarial_model': 'Mapping_MLP3_noflatten', 'adversarial_model_lr': 0.001, 'imagined_adversary': 'ImaginedAdversary_MLP3_noflatten', 'imagined_adversary_lr': 0.001, 'lambda': 0.001, 'seq_length': 512, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/GMS8K/2-slice/2_0/AdversarialTraining_0.001,finetuned_model=meta-mathMetaMath-Mistral-7B.txt
=================================

===== iter 64 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
default tokenizer.pad_token: [PAD]
pad_token: [PAD]   pad_id: 32000
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
trainable params: 0 || all params: 567300096 || trainable%: 0.0
model slices: dict_keys([0])
model partition: 0head-2/1tail-30
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
7473 train examples
1319 test examples
X: <class 'numpy.ndarray'> 7473 1319
y <class 'numpy.ndarray'> 7473 1319
Passive Party 0: init Adversarial Trainining Defense
Adversarial_MLP init: 512 4096
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 32000
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:22<01:51, 22.23s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:36<01:09, 17.35s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:45<00:41, 13.87s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:56<00:56, 18.67s/it]
2024-10-06 01:04:27.293 | WARNING  | models.llm_models.base:from_pretrained:99 - RuntimeError('CUDA out of memory. Tried to allocate 224.00 MiB (GPU 1; 39.39 GiB total capacity; 7.81 GiB already allocated; 17.31 MiB free; 7.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')
Try to load from raw model
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:21<?, ?it/s]
Traceback (most recent call last):
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/base.py", line 97, in from_pretrained
    return self.from_vfl(model_name_or_path, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/base.py", line 122, in from_vfl
    return self.from_vfl(self._vfl_model_folder(model_name_or_path), **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/base.py", line 128, in from_vfl
    _model = self._load_model_tail(model_path, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/mistral.py", line 550, in _load_model_tail
    model_tail = MistralTailForCausalLM.from_pretrained(model_name_or_path, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/modeling_utils.py", line 3677, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/modeling_utils.py", line 4104, in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/modeling_utils.py", line 886, in _load_state_dict_into_meta_model
    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/accelerate/utils/modeling.py", line 317, in set_module_tensor_to_device
    new_value = value.to(device)
RuntimeError: CUDA out of memory. Tried to allocate 224.00 MiB (GPU 1; 39.39 GiB total capacity; 7.81 GiB already allocated; 17.31 MiB free; 7.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 296, in <module>
    args = load_parties_llm(args)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/load/LoadParty.py", line 43, in load_parties_llm
    args.parties[args.k - 1] = get_class_constructor(args.active_party_class)(args, args.k - 1, need_data=need_data)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 24, in __init__
    super().__init__(args, index, need_data=need_data, need_model=need_model)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 107, in __init__
    self.prepare_model(args, index)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 276, in prepare_model
    result = load_models_per_party_llm(args, index)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/load/LoadModels.py", line 373, in load_models_per_party_llm
    result = load_basic_models_llm(args,index)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/load/LoadModels.py", line 358, in load_basic_models_llm
    result = loader.load(args=args, model_path=model_path, is_active_party = is_active_party)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/load/llm_model_loaders/MistralModelLoader.py", line 29, in load
    self._models=p.from_pretrained(model_path, **args.kwargs_model_loading)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/base.py", line 100, in from_pretrained
    return self._from_raw(model_name_or_path, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/base.py", line 147, in _from_raw
    _model = self._load_model_tail(model_name_or_path, do_split=True, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/mistral.py", line 550, in _load_model_tail
    model_tail = MistralTailForCausalLM.from_pretrained(model_name_or_path, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/modeling_utils.py", line 3677, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/modeling_utils.py", line 4104, in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/modeling_utils.py", line 886, in _load_state_dict_into_meta_model
    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/accelerate/utils/modeling.py", line 317, in set_module_tensor_to_device
    new_value = value.to(device)
RuntimeError: CUDA out of memory. Tried to allocate 502.00 MiB (GPU 1; 39.39 GiB total capacity; 7.81 GiB already allocated; 17.31 MiB free; 7.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-06 01:05:03.412 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-06 01:05:10.253 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/meta-mathMetaMath-Mistral-7B_vfl_(2,)
2024-10-06 01:05:36.147 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/meta-mathMetaMath-Mistral-7B_vfl_(2,)
================= iter seed  64  =================
communication_protocol: FedSGD
load model_architect: CLM
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 2
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 0
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: AdversarialTraining
Defense_Config: {'party': [0], 'position': 'head', 'adversarial_model': 'Mapping_MLP3_noflatten', 'adversarial_model_lr': 0.001, 'imagined_adversary': 'ImaginedAdversary_MLP3_noflatten', 'imagined_adversary_lr': 0.001, 'lambda': 0.01, 'seq_length': 512, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/GMS8K/2-slice/2_0/AdversarialTraining_0.01,finetuned_model=meta-mathMetaMath-Mistral-7B.txt
=================================

===== iter 64 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
default tokenizer.pad_token: [PAD]
pad_token: [PAD]   pad_id: 32000
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
trainable params: 0 || all params: 567300096 || trainable%: 0.0
model slices: dict_keys([0])
model partition: 0head-2/1tail-30
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
7473 train examples
1319 test examples
X: <class 'numpy.ndarray'> 7473 1319
y <class 'numpy.ndarray'> 7473 1319
Passive Party 0: init Adversarial Trainining Defense
Adversarial_MLP init: 512 4096
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 32000
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:23<01:56, 23.37s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:32<01:00, 15.24s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:43<00:39, 13.10s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:51<00:51, 17.24s/it]
2024-10-06 01:06:28.801 | WARNING  | models.llm_models.base:from_pretrained:99 - RuntimeError('CUDA out of memory. Tried to allocate 224.00 MiB (GPU 1; 39.39 GiB total capacity; 7.81 GiB already allocated; 17.31 MiB free; 7.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')
Try to load from raw model
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:26<?, ?it/s]
Traceback (most recent call last):
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/base.py", line 97, in from_pretrained
    return self.from_vfl(model_name_or_path, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/base.py", line 122, in from_vfl
    return self.from_vfl(self._vfl_model_folder(model_name_or_path), **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/base.py", line 128, in from_vfl
    _model = self._load_model_tail(model_path, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/mistral.py", line 550, in _load_model_tail
    model_tail = MistralTailForCausalLM.from_pretrained(model_name_or_path, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/modeling_utils.py", line 3677, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/modeling_utils.py", line 4104, in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/modeling_utils.py", line 886, in _load_state_dict_into_meta_model
    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/accelerate/utils/modeling.py", line 317, in set_module_tensor_to_device
    new_value = value.to(device)
RuntimeError: CUDA out of memory. Tried to allocate 224.00 MiB (GPU 1; 39.39 GiB total capacity; 7.81 GiB already allocated; 17.31 MiB free; 7.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 296, in <module>
    args = load_parties_llm(args)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/load/LoadParty.py", line 43, in load_parties_llm
    args.parties[args.k - 1] = get_class_constructor(args.active_party_class)(args, args.k - 1, need_data=need_data)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 24, in __init__
    super().__init__(args, index, need_data=need_data, need_model=need_model)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 107, in __init__
    self.prepare_model(args, index)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 276, in prepare_model
    result = load_models_per_party_llm(args, index)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/load/LoadModels.py", line 373, in load_models_per_party_llm
    result = load_basic_models_llm(args,index)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/load/LoadModels.py", line 358, in load_basic_models_llm
    result = loader.load(args=args, model_path=model_path, is_active_party = is_active_party)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/load/llm_model_loaders/MistralModelLoader.py", line 29, in load
    self._models=p.from_pretrained(model_path, **args.kwargs_model_loading)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/base.py", line 100, in from_pretrained
    return self._from_raw(model_name_or_path, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/base.py", line 147, in _from_raw
    _model = self._load_model_tail(model_name_or_path, do_split=True, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/mistral.py", line 550, in _load_model_tail
    model_tail = MistralTailForCausalLM.from_pretrained(model_name_or_path, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/modeling_utils.py", line 3677, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/modeling_utils.py", line 4104, in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/modeling_utils.py", line 886, in _load_state_dict_into_meta_model
    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/accelerate/utils/modeling.py", line 317, in set_module_tensor_to_device
    new_value = value.to(device)
RuntimeError: CUDA out of memory. Tried to allocate 502.00 MiB (GPU 1; 39.39 GiB total capacity; 7.81 GiB already allocated; 17.31 MiB free; 7.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-06 01:07:09.928 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-06 01:07:16.836 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/meta-mathMetaMath-Mistral-7B_vfl_(2,)
2024-10-06 01:07:43.959 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/meta-mathMetaMath-Mistral-7B_vfl_(2,)
================= iter seed  64  =================
communication_protocol: FedSGD
load model_architect: CLM
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 2
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 0
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: AdversarialTraining
Defense_Config: {'party': [0], 'position': 'head', 'adversarial_model': 'Mapping_MLP3_noflatten', 'adversarial_model_lr': 0.001, 'imagined_adversary': 'ImaginedAdversary_MLP3_noflatten', 'imagined_adversary_lr': 0.001, 'lambda': 0.1, 'seq_length': 512, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/GMS8K/2-slice/2_0/AdversarialTraining_0.1,finetuned_model=meta-mathMetaMath-Mistral-7B.txt
=================================

===== iter 64 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
default tokenizer.pad_token: [PAD]
pad_token: [PAD]   pad_id: 32000
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
trainable params: 0 || all params: 567300096 || trainable%: 0.0
model slices: dict_keys([0])
model partition: 0head-2/1tail-30
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
7473 train examples
1319 test examples
X: <class 'numpy.ndarray'> 7473 1319
y <class 'numpy.ndarray'> 7473 1319
Passive Party 0: init Adversarial Trainining Defense
Adversarial_MLP init: 512 4096
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 32000
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:17<01:25, 17.14s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:31<01:02, 15.64s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:42<00:40, 13.48s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:50<00:50, 16.70s/it]
2024-10-06 01:08:34.904 | WARNING  | models.llm_models.base:from_pretrained:99 - RuntimeError('CUDA out of memory. Tried to allocate 224.00 MiB (GPU 1; 39.39 GiB total capacity; 7.81 GiB already allocated; 17.31 MiB free; 7.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')
Try to load from raw model
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:28<?, ?it/s]
Traceback (most recent call last):
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/base.py", line 97, in from_pretrained
    return self.from_vfl(model_name_or_path, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/base.py", line 122, in from_vfl
    return self.from_vfl(self._vfl_model_folder(model_name_or_path), **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/base.py", line 128, in from_vfl
    _model = self._load_model_tail(model_path, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/mistral.py", line 550, in _load_model_tail
    model_tail = MistralTailForCausalLM.from_pretrained(model_name_or_path, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/modeling_utils.py", line 3677, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/modeling_utils.py", line 4104, in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/modeling_utils.py", line 886, in _load_state_dict_into_meta_model
    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/accelerate/utils/modeling.py", line 317, in set_module_tensor_to_device
    new_value = value.to(device)
RuntimeError: CUDA out of memory. Tried to allocate 224.00 MiB (GPU 1; 39.39 GiB total capacity; 7.81 GiB already allocated; 17.31 MiB free; 7.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 296, in <module>
    args = load_parties_llm(args)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/load/LoadParty.py", line 43, in load_parties_llm
    args.parties[args.k - 1] = get_class_constructor(args.active_party_class)(args, args.k - 1, need_data=need_data)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 24, in __init__
    super().__init__(args, index, need_data=need_data, need_model=need_model)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 107, in __init__
    self.prepare_model(args, index)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 276, in prepare_model
    result = load_models_per_party_llm(args, index)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/load/LoadModels.py", line 373, in load_models_per_party_llm
    result = load_basic_models_llm(args,index)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/load/LoadModels.py", line 358, in load_basic_models_llm
    result = loader.load(args=args, model_path=model_path, is_active_party = is_active_party)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/load/llm_model_loaders/MistralModelLoader.py", line 29, in load
    self._models=p.from_pretrained(model_path, **args.kwargs_model_loading)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/base.py", line 100, in from_pretrained
    return self._from_raw(model_name_or_path, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/base.py", line 147, in _from_raw
    _model = self._load_model_tail(model_name_or_path, do_split=True, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/mistral.py", line 550, in _load_model_tail
    model_tail = MistralTailForCausalLM.from_pretrained(model_name_or_path, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/modeling_utils.py", line 3677, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/modeling_utils.py", line 4104, in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/modeling_utils.py", line 886, in _load_state_dict_into_meta_model
    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/accelerate/utils/modeling.py", line 317, in set_module_tensor_to_device
    new_value = value.to(device)
RuntimeError: CUDA out of memory. Tried to allocate 502.00 MiB (GPU 1; 39.39 GiB total capacity; 7.81 GiB already allocated; 17.31 MiB free; 7.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-06 01:09:17.290 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-06 01:09:23.944 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/meta-mathMetaMath-Mistral-7B_vfl_(2,)
2024-10-06 01:09:49.987 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/meta-mathMetaMath-Mistral-7B_vfl_(2,)
================= iter seed  65  =================
communication_protocol: FedSGD
load model_architect: CLM
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 2
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 0
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: AdversarialTraining
Defense_Config: {'party': [0], 'position': 'head', 'adversarial_model': 'Mapping_MLP3_noflatten', 'adversarial_model_lr': 0.001, 'imagined_adversary': 'ImaginedAdversary_MLP3_noflatten', 'imagined_adversary_lr': 0.001, 'lambda': 0.001, 'seq_length': 512, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/GMS8K/2-slice/2_0/AdversarialTraining_0.001,finetuned_model=meta-mathMetaMath-Mistral-7B.txt
=================================

===== iter 65 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
default tokenizer.pad_token: [PAD]
pad_token: [PAD]   pad_id: 32000
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
trainable params: 0 || all params: 567300096 || trainable%: 0.0
model slices: dict_keys([0])
model partition: 0head-2/1tail-30
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
7473 train examples
1319 test examples
X: <class 'numpy.ndarray'> 7473 1319
y <class 'numpy.ndarray'> 7473 1319
Passive Party 0: init Adversarial Trainining Defense
Adversarial_MLP init: 512 4096
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 32000
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:16<01:23, 16.67s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:33<01:07, 16.84s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:44<00:42, 14.01s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:52<00:52, 17.56s/it]
2024-10-06 01:10:43.477 | WARNING  | models.llm_models.base:from_pretrained:99 - RuntimeError('CUDA out of memory. Tried to allocate 224.00 MiB (GPU 1; 39.39 GiB total capacity; 7.81 GiB already allocated; 17.31 MiB free; 7.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')
Try to load from raw model
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:28<?, ?it/s]
Traceback (most recent call last):
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/base.py", line 97, in from_pretrained
    return self.from_vfl(model_name_or_path, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/base.py", line 122, in from_vfl
    return self.from_vfl(self._vfl_model_folder(model_name_or_path), **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/base.py", line 128, in from_vfl
    _model = self._load_model_tail(model_path, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/mistral.py", line 550, in _load_model_tail
    model_tail = MistralTailForCausalLM.from_pretrained(model_name_or_path, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/modeling_utils.py", line 3677, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/modeling_utils.py", line 4104, in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/modeling_utils.py", line 886, in _load_state_dict_into_meta_model
    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/accelerate/utils/modeling.py", line 317, in set_module_tensor_to_device
    new_value = value.to(device)
RuntimeError: CUDA out of memory. Tried to allocate 224.00 MiB (GPU 1; 39.39 GiB total capacity; 7.81 GiB already allocated; 17.31 MiB free; 7.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 296, in <module>
    args = load_parties_llm(args)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/load/LoadParty.py", line 43, in load_parties_llm
    args.parties[args.k - 1] = get_class_constructor(args.active_party_class)(args, args.k - 1, need_data=need_data)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 24, in __init__
    super().__init__(args, index, need_data=need_data, need_model=need_model)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 107, in __init__
    self.prepare_model(args, index)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 276, in prepare_model
    result = load_models_per_party_llm(args, index)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/load/LoadModels.py", line 373, in load_models_per_party_llm
    result = load_basic_models_llm(args,index)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/load/LoadModels.py", line 358, in load_basic_models_llm
    result = loader.load(args=args, model_path=model_path, is_active_party = is_active_party)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/load/llm_model_loaders/MistralModelLoader.py", line 29, in load
    self._models=p.from_pretrained(model_path, **args.kwargs_model_loading)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/base.py", line 100, in from_pretrained
    return self._from_raw(model_name_or_path, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/base.py", line 147, in _from_raw
    _model = self._load_model_tail(model_name_or_path, do_split=True, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/mistral.py", line 550, in _load_model_tail
    model_tail = MistralTailForCausalLM.from_pretrained(model_name_or_path, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/modeling_utils.py", line 3677, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/modeling_utils.py", line 4104, in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/modeling_utils.py", line 886, in _load_state_dict_into_meta_model
    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/accelerate/utils/modeling.py", line 317, in set_module_tensor_to_device
    new_value = value.to(device)
RuntimeError: CUDA out of memory. Tried to allocate 502.00 MiB (GPU 1; 39.39 GiB total capacity; 7.81 GiB already allocated; 17.31 MiB free; 7.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-06 01:11:25.943 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-06 01:11:33.035 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/meta-mathMetaMath-Mistral-7B_vfl_(2,)
2024-10-06 01:11:57.519 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/meta-mathMetaMath-Mistral-7B_vfl_(2,)
================= iter seed  65  =================
communication_protocol: FedSGD
load model_architect: CLM
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 2
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 0
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: AdversarialTraining
Defense_Config: {'party': [0], 'position': 'head', 'adversarial_model': 'Mapping_MLP3_noflatten', 'adversarial_model_lr': 0.001, 'imagined_adversary': 'ImaginedAdversary_MLP3_noflatten', 'imagined_adversary_lr': 0.001, 'lambda': 0.01, 'seq_length': 512, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/GMS8K/2-slice/2_0/AdversarialTraining_0.01,finetuned_model=meta-mathMetaMath-Mistral-7B.txt
=================================

===== iter 65 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
default tokenizer.pad_token: [PAD]
pad_token: [PAD]   pad_id: 32000
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
trainable params: 0 || all params: 567300096 || trainable%: 0.0
model slices: dict_keys([0])
model partition: 0head-2/1tail-30
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
7473 train examples
1319 test examples
X: <class 'numpy.ndarray'> 7473 1319
y <class 'numpy.ndarray'> 7473 1319
Passive Party 0: init Adversarial Trainining Defense
Adversarial_MLP init: 512 4096
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 32000
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:14<01:11, 14.27s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:33<01:07, 16.95s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:42<00:40, 13.42s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:51<00:51, 17.13s/it]
2024-10-06 01:12:49.700 | WARNING  | models.llm_models.base:from_pretrained:99 - RuntimeError('CUDA out of memory. Tried to allocate 224.00 MiB (GPU 1; 39.39 GiB total capacity; 7.81 GiB already allocated; 17.31 MiB free; 7.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')
Try to load from raw model
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:29<?, ?it/s]
Traceback (most recent call last):
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/base.py", line 97, in from_pretrained
    return self.from_vfl(model_name_or_path, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/base.py", line 122, in from_vfl
    return self.from_vfl(self._vfl_model_folder(model_name_or_path), **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/base.py", line 128, in from_vfl
    _model = self._load_model_tail(model_path, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/mistral.py", line 550, in _load_model_tail
    model_tail = MistralTailForCausalLM.from_pretrained(model_name_or_path, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/modeling_utils.py", line 3677, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/modeling_utils.py", line 4104, in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/modeling_utils.py", line 886, in _load_state_dict_into_meta_model
    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/accelerate/utils/modeling.py", line 317, in set_module_tensor_to_device
    new_value = value.to(device)
RuntimeError: CUDA out of memory. Tried to allocate 224.00 MiB (GPU 1; 39.39 GiB total capacity; 7.81 GiB already allocated; 17.31 MiB free; 7.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 296, in <module>
    args = load_parties_llm(args)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/load/LoadParty.py", line 43, in load_parties_llm
    args.parties[args.k - 1] = get_class_constructor(args.active_party_class)(args, args.k - 1, need_data=need_data)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 24, in __init__
    super().__init__(args, index, need_data=need_data, need_model=need_model)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 107, in __init__
    self.prepare_model(args, index)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 276, in prepare_model
    result = load_models_per_party_llm(args, index)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/load/LoadModels.py", line 373, in load_models_per_party_llm
    result = load_basic_models_llm(args,index)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/load/LoadModels.py", line 358, in load_basic_models_llm
    result = loader.load(args=args, model_path=model_path, is_active_party = is_active_party)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/load/llm_model_loaders/MistralModelLoader.py", line 29, in load
    self._models=p.from_pretrained(model_path, **args.kwargs_model_loading)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/base.py", line 100, in from_pretrained
    return self._from_raw(model_name_or_path, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/base.py", line 147, in _from_raw
    _model = self._load_model_tail(model_name_or_path, do_split=True, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/mistral.py", line 550, in _load_model_tail
    model_tail = MistralTailForCausalLM.from_pretrained(model_name_or_path, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/modeling_utils.py", line 3677, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/modeling_utils.py", line 4104, in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/modeling_utils.py", line 886, in _load_state_dict_into_meta_model
    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/accelerate/utils/modeling.py", line 317, in set_module_tensor_to_device
    new_value = value.to(device)
RuntimeError: CUDA out of memory. Tried to allocate 502.00 MiB (GPU 1; 39.39 GiB total capacity; 7.81 GiB already allocated; 17.31 MiB free; 7.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-06 01:13:33.891 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-06 01:13:41.256 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/meta-mathMetaMath-Mistral-7B_vfl_(2,)
2024-10-06 01:14:06.286 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/meta-mathMetaMath-Mistral-7B_vfl_(2,)
================= iter seed  65  =================
communication_protocol: FedSGD
load model_architect: CLM
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 2
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 0
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: AdversarialTraining
Defense_Config: {'party': [0], 'position': 'head', 'adversarial_model': 'Mapping_MLP3_noflatten', 'adversarial_model_lr': 0.001, 'imagined_adversary': 'ImaginedAdversary_MLP3_noflatten', 'imagined_adversary_lr': 0.001, 'lambda': 0.1, 'seq_length': 512, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/GMS8K/2-slice/2_0/AdversarialTraining_0.1,finetuned_model=meta-mathMetaMath-Mistral-7B.txt
=================================

===== iter 65 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
default tokenizer.pad_token: [PAD]
pad_token: [PAD]   pad_id: 32000
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
trainable params: 0 || all params: 567300096 || trainable%: 0.0
model slices: dict_keys([0])
model partition: 0head-2/1tail-30
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
7473 train examples
1319 test examples
X: <class 'numpy.ndarray'> 7473 1319
y <class 'numpy.ndarray'> 7473 1319
Passive Party 0: init Adversarial Trainining Defense
Adversarial_MLP init: 512 4096
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 32000
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:15<01:17, 15.54s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:35<01:12, 18.16s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:45<00:42, 14.20s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:53<00:53, 17.78s/it]
2024-10-06 01:15:00.466 | WARNING  | models.llm_models.base:from_pretrained:99 - RuntimeError('CUDA out of memory. Tried to allocate 224.00 MiB (GPU 1; 39.39 GiB total capacity; 7.81 GiB already allocated; 17.31 MiB free; 7.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')
Try to load from raw model
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:29<?, ?it/s]
Traceback (most recent call last):
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/base.py", line 97, in from_pretrained
    return self.from_vfl(model_name_or_path, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/base.py", line 122, in from_vfl
    return self.from_vfl(self._vfl_model_folder(model_name_or_path), **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/base.py", line 128, in from_vfl
    _model = self._load_model_tail(model_path, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/mistral.py", line 550, in _load_model_tail
    model_tail = MistralTailForCausalLM.from_pretrained(model_name_or_path, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/modeling_utils.py", line 3677, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/modeling_utils.py", line 4104, in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/modeling_utils.py", line 886, in _load_state_dict_into_meta_model
    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/accelerate/utils/modeling.py", line 317, in set_module_tensor_to_device
    new_value = value.to(device)
RuntimeError: CUDA out of memory. Tried to allocate 224.00 MiB (GPU 1; 39.39 GiB total capacity; 7.81 GiB already allocated; 17.31 MiB free; 7.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 296, in <module>
    args = load_parties_llm(args)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/load/LoadParty.py", line 43, in load_parties_llm
    args.parties[args.k - 1] = get_class_constructor(args.active_party_class)(args, args.k - 1, need_data=need_data)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 24, in __init__
    super().__init__(args, index, need_data=need_data, need_model=need_model)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 107, in __init__
    self.prepare_model(args, index)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 276, in prepare_model
    result = load_models_per_party_llm(args, index)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/load/LoadModels.py", line 373, in load_models_per_party_llm
    result = load_basic_models_llm(args,index)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/load/LoadModels.py", line 358, in load_basic_models_llm
    result = loader.load(args=args, model_path=model_path, is_active_party = is_active_party)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/load/llm_model_loaders/MistralModelLoader.py", line 29, in load
    self._models=p.from_pretrained(model_path, **args.kwargs_model_loading)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/base.py", line 100, in from_pretrained
    return self._from_raw(model_name_or_path, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/base.py", line 147, in _from_raw
    _model = self._load_model_tail(model_name_or_path, do_split=True, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/mistral.py", line 550, in _load_model_tail
    model_tail = MistralTailForCausalLM.from_pretrained(model_name_or_path, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/modeling_utils.py", line 3677, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/modeling_utils.py", line 4104, in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/modeling_utils.py", line 886, in _load_state_dict_into_meta_model
    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/accelerate/utils/modeling.py", line 317, in set_module_tensor_to_device
    new_value = value.to(device)
RuntimeError: CUDA out of memory. Tried to allocate 502.00 MiB (GPU 1; 39.39 GiB total capacity; 7.81 GiB already allocated; 17.31 MiB free; 7.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

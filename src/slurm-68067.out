/bin/bash: /home/DAIR/guzx/.conda/envs/py38/lib/libtinfo.so.6: no version information available (required by /bin/bash)
2024-10-01 02:52:24.304 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 02:52:33.691 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
2024-10-01 03:00:53.265 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
================= iter seed  60  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 3
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: AdversarialTraining
Defense_Config: {'party': [0], 'position': 'head', 'adversarial_model': 'Mapping_MLP3', 'adversarial_model_lr': 0.0001, 'imagined_adversary': 'ImaginedAdversary_MLP3', 'imagined_adversary_lr': 0.0001, 'lambda': 0.001, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/3_0/AdversarialTraining_0.001,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 60 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 43928832 || trainable%: 0.902113673315967
model slices: dict_keys([0])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init Adversarial Trainining Defense
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 87050501 || trainable%: 26.719723301764798
model slices: dict_keys([1])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  2.62it/s]Epoch 0/49: 1it [00:00,  2.08it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 334, in forward
    attention_probs = self.dropout(attention_probs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/dropout.py", line 58, in forward
    return F.dropout(input, self.p, self.training, self.inplace)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/functional.py", line 1169, in dropout
    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.41 GiB total capacity; 37.05 GiB already allocated; 344.50 MiB free; 37.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 03:02:26.815 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 03:02:35.991 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
2024-10-01 03:10:26.835 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
================= iter seed  60  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 3
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: AdversarialTraining
Defense_Config: {'party': [0], 'position': 'head', 'adversarial_model': 'Mapping_MLP3', 'adversarial_model_lr': 0.0001, 'imagined_adversary': 'ImaginedAdversary_MLP3', 'imagined_adversary_lr': 0.0001, 'lambda': 0.01, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/3_0/AdversarialTraining_0.01,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 60 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 43928832 || trainable%: 0.902113673315967
model slices: dict_keys([0])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init Adversarial Trainining Defense
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 87050501 || trainable%: 26.719723301764798
model slices: dict_keys([1])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  2.78it/s]Epoch 0/49: 1it [00:00,  2.33it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 334, in forward
    attention_probs = self.dropout(attention_probs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/dropout.py", line 58, in forward
    return F.dropout(input, self.p, self.training, self.inplace)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/functional.py", line 1169, in dropout
    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.41 GiB total capacity; 37.05 GiB already allocated; 344.50 MiB free; 37.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 03:11:49.096 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 03:11:56.930 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
2024-10-01 03:19:56.869 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
================= iter seed  60  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 3
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: AdversarialTraining
Defense_Config: {'party': [0], 'position': 'head', 'adversarial_model': 'Mapping_MLP3', 'adversarial_model_lr': 0.0001, 'imagined_adversary': 'ImaginedAdversary_MLP3', 'imagined_adversary_lr': 0.0001, 'lambda': 0.1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/3_0/AdversarialTraining_0.1,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 60 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 43928832 || trainable%: 0.902113673315967
model slices: dict_keys([0])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init Adversarial Trainining Defense
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 87050501 || trainable%: 26.719723301764798
model slices: dict_keys([1])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  2.84it/s]Epoch 0/49: 1it [00:00,  2.37it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 334, in forward
    attention_probs = self.dropout(attention_probs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/dropout.py", line 58, in forward
    return F.dropout(input, self.p, self.training, self.inplace)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/functional.py", line 1169, in dropout
    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.41 GiB total capacity; 37.05 GiB already allocated; 344.50 MiB free; 37.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 03:21:21.673 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 03:21:29.893 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
2024-10-01 03:29:30.072 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
================= iter seed  60  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 3
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: AdversarialTraining
Defense_Config: {'party': [0], 'position': 'head', 'adversarial_model': 'Mapping_MLP3', 'adversarial_model_lr': 0.0001, 'imagined_adversary': 'ImaginedAdversary_MLP3', 'imagined_adversary_lr': 0.0001, 'lambda': 1.0, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/3_0/AdversarialTraining_1.0,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 60 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 43928832 || trainable%: 0.902113673315967
model slices: dict_keys([0])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init Adversarial Trainining Defense
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 87050501 || trainable%: 26.719723301764798
model slices: dict_keys([1])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  2.70it/s]Epoch 0/49: 1it [00:00,  2.04it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 334, in forward
    attention_probs = self.dropout(attention_probs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/dropout.py", line 58, in forward
    return F.dropout(input, self.p, self.training, self.inplace)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/functional.py", line 1169, in dropout
    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.41 GiB total capacity; 37.05 GiB already allocated; 344.50 MiB free; 37.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 03:30:54.989 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 03:31:03.208 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
2024-10-01 03:39:01.497 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
================= iter seed  60  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 3
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: AdversarialTraining
Defense_Config: {'party': [0], 'position': 'head', 'adversarial_model': 'Mapping_MLP3', 'adversarial_model_lr': 0.0001, 'imagined_adversary': 'ImaginedAdversary_MLP3', 'imagined_adversary_lr': 0.0001, 'lambda': 5.0, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/3_0/AdversarialTraining_5.0,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 60 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 43928832 || trainable%: 0.902113673315967
model slices: dict_keys([0])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init Adversarial Trainining Defense
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 87050501 || trainable%: 26.719723301764798
model slices: dict_keys([1])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  2.69it/s]Epoch 0/49: 1it [00:00,  2.27it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 334, in forward
    attention_probs = self.dropout(attention_probs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/dropout.py", line 58, in forward
    return F.dropout(input, self.p, self.training, self.inplace)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/functional.py", line 1169, in dropout
    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.41 GiB total capacity; 37.05 GiB already allocated; 344.50 MiB free; 37.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 03:40:28.992 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 03:40:37.336 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
2024-10-01 03:48:31.327 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
================= iter seed  61  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 3
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: AdversarialTraining
Defense_Config: {'party': [0], 'position': 'head', 'adversarial_model': 'Mapping_MLP3', 'adversarial_model_lr': 0.0001, 'imagined_adversary': 'ImaginedAdversary_MLP3', 'imagined_adversary_lr': 0.0001, 'lambda': 0.001, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/3_0/AdversarialTraining_0.001,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 61 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 43928832 || trainable%: 0.902113673315967
model slices: dict_keys([0])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init Adversarial Trainining Defense
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 87050501 || trainable%: 26.719723301764798
model slices: dict_keys([1])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  2.69it/s]Epoch 0/49: 1it [00:00,  2.27it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 334, in forward
    attention_probs = self.dropout(attention_probs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/dropout.py", line 58, in forward
    return F.dropout(input, self.p, self.training, self.inplace)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/functional.py", line 1169, in dropout
    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.41 GiB total capacity; 37.05 GiB already allocated; 344.50 MiB free; 37.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 03:49:53.571 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 03:50:02.120 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
2024-10-01 03:58:05.304 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
================= iter seed  61  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 3
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: AdversarialTraining
Defense_Config: {'party': [0], 'position': 'head', 'adversarial_model': 'Mapping_MLP3', 'adversarial_model_lr': 0.0001, 'imagined_adversary': 'ImaginedAdversary_MLP3', 'imagined_adversary_lr': 0.0001, 'lambda': 0.01, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/3_0/AdversarialTraining_0.01,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 61 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 43928832 || trainable%: 0.902113673315967
model slices: dict_keys([0])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init Adversarial Trainining Defense
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 87050501 || trainable%: 26.719723301764798
model slices: dict_keys([1])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  2.70it/s]Epoch 0/49: 1it [00:00,  2.20it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 334, in forward
    attention_probs = self.dropout(attention_probs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/dropout.py", line 58, in forward
    return F.dropout(input, self.p, self.training, self.inplace)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/functional.py", line 1169, in dropout
    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.41 GiB total capacity; 37.05 GiB already allocated; 344.50 MiB free; 37.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 03:59:33.472 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 03:59:41.685 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
2024-10-01 04:07:32.309 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
================= iter seed  61  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 3
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: AdversarialTraining
Defense_Config: {'party': [0], 'position': 'head', 'adversarial_model': 'Mapping_MLP3', 'adversarial_model_lr': 0.0001, 'imagined_adversary': 'ImaginedAdversary_MLP3', 'imagined_adversary_lr': 0.0001, 'lambda': 0.1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/3_0/AdversarialTraining_0.1,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 61 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 43928832 || trainable%: 0.902113673315967
model slices: dict_keys([0])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init Adversarial Trainining Defense
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 87050501 || trainable%: 26.719723301764798
model slices: dict_keys([1])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  2.75it/s]Epoch 0/49: 1it [00:00,  2.32it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 334, in forward
    attention_probs = self.dropout(attention_probs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/dropout.py", line 58, in forward
    return F.dropout(input, self.p, self.training, self.inplace)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/functional.py", line 1169, in dropout
    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.41 GiB total capacity; 37.05 GiB already allocated; 344.50 MiB free; 37.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 04:08:54.789 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 04:09:02.797 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
2024-10-01 04:17:08.798 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
================= iter seed  61  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 3
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: AdversarialTraining
Defense_Config: {'party': [0], 'position': 'head', 'adversarial_model': 'Mapping_MLP3', 'adversarial_model_lr': 0.0001, 'imagined_adversary': 'ImaginedAdversary_MLP3', 'imagined_adversary_lr': 0.0001, 'lambda': 1.0, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/3_0/AdversarialTraining_1.0,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 61 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 43928832 || trainable%: 0.902113673315967
model slices: dict_keys([0])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init Adversarial Trainining Defense
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 87050501 || trainable%: 26.719723301764798
model slices: dict_keys([1])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  2.71it/s]Epoch 0/49: 1it [00:00,  2.22it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 334, in forward
    attention_probs = self.dropout(attention_probs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/dropout.py", line 58, in forward
    return F.dropout(input, self.p, self.training, self.inplace)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/functional.py", line 1169, in dropout
    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.41 GiB total capacity; 37.05 GiB already allocated; 344.50 MiB free; 37.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 04:18:34.660 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 04:18:43.234 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
2024-10-01 04:26:30.898 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
================= iter seed  61  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 3
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: AdversarialTraining
Defense_Config: {'party': [0], 'position': 'head', 'adversarial_model': 'Mapping_MLP3', 'adversarial_model_lr': 0.0001, 'imagined_adversary': 'ImaginedAdversary_MLP3', 'imagined_adversary_lr': 0.0001, 'lambda': 5.0, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/3_0/AdversarialTraining_5.0,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 61 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 43928832 || trainable%: 0.902113673315967
model slices: dict_keys([0])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init Adversarial Trainining Defense
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 87050501 || trainable%: 26.719723301764798
model slices: dict_keys([1])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  2.85it/s]Epoch 0/49: 1it [00:00,  2.38it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 334, in forward
    attention_probs = self.dropout(attention_probs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/dropout.py", line 58, in forward
    return F.dropout(input, self.p, self.training, self.inplace)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/functional.py", line 1169, in dropout
    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.41 GiB total capacity; 37.05 GiB already allocated; 344.50 MiB free; 37.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 04:27:49.826 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 04:27:57.728 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
2024-10-01 04:35:58.597 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
================= iter seed  62  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 3
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: AdversarialTraining
Defense_Config: {'party': [0], 'position': 'head', 'adversarial_model': 'Mapping_MLP3', 'adversarial_model_lr': 0.0001, 'imagined_adversary': 'ImaginedAdversary_MLP3', 'imagined_adversary_lr': 0.0001, 'lambda': 0.001, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/3_0/AdversarialTraining_0.001,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 62 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 43928832 || trainable%: 0.902113673315967
model slices: dict_keys([0])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init Adversarial Trainining Defense
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 87050501 || trainable%: 26.719723301764798
model slices: dict_keys([1])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  2.86it/s]Epoch 0/49: 1it [00:00,  2.45it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 334, in forward
    attention_probs = self.dropout(attention_probs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/dropout.py", line 58, in forward
    return F.dropout(input, self.p, self.training, self.inplace)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/functional.py", line 1169, in dropout
    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.41 GiB total capacity; 37.05 GiB already allocated; 344.50 MiB free; 37.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 04:37:26.285 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 04:37:34.590 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
2024-10-01 04:45:37.564 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
================= iter seed  62  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 3
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: AdversarialTraining
Defense_Config: {'party': [0], 'position': 'head', 'adversarial_model': 'Mapping_MLP3', 'adversarial_model_lr': 0.0001, 'imagined_adversary': 'ImaginedAdversary_MLP3', 'imagined_adversary_lr': 0.0001, 'lambda': 0.01, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/3_0/AdversarialTraining_0.01,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 62 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 43928832 || trainable%: 0.902113673315967
model slices: dict_keys([0])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init Adversarial Trainining Defense
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 87050501 || trainable%: 26.719723301764798
model slices: dict_keys([1])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  2.79it/s]Epoch 0/49: 1it [00:00,  2.36it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 334, in forward
    attention_probs = self.dropout(attention_probs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/dropout.py", line 58, in forward
    return F.dropout(input, self.p, self.training, self.inplace)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/functional.py", line 1169, in dropout
    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.41 GiB total capacity; 37.05 GiB already allocated; 344.50 MiB free; 37.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 04:46:59.436 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 04:47:06.969 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
2024-10-01 04:55:22.319 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
================= iter seed  62  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 3
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: AdversarialTraining
Defense_Config: {'party': [0], 'position': 'head', 'adversarial_model': 'Mapping_MLP3', 'adversarial_model_lr': 0.0001, 'imagined_adversary': 'ImaginedAdversary_MLP3', 'imagined_adversary_lr': 0.0001, 'lambda': 0.1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/3_0/AdversarialTraining_0.1,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 62 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 43928832 || trainable%: 0.902113673315967
model slices: dict_keys([0])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init Adversarial Trainining Defense
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 87050501 || trainable%: 26.719723301764798
model slices: dict_keys([1])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  2.69it/s]Epoch 0/49: 1it [00:00,  2.29it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 334, in forward
    attention_probs = self.dropout(attention_probs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/dropout.py", line 58, in forward
    return F.dropout(input, self.p, self.training, self.inplace)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/functional.py", line 1169, in dropout
    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.41 GiB total capacity; 37.05 GiB already allocated; 344.50 MiB free; 37.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 04:56:50.670 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 04:56:58.394 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
2024-10-01 05:04:59.079 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
================= iter seed  62  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 3
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: AdversarialTraining
Defense_Config: {'party': [0], 'position': 'head', 'adversarial_model': 'Mapping_MLP3', 'adversarial_model_lr': 0.0001, 'imagined_adversary': 'ImaginedAdversary_MLP3', 'imagined_adversary_lr': 0.0001, 'lambda': 1.0, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/3_0/AdversarialTraining_1.0,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 62 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 43928832 || trainable%: 0.902113673315967
model slices: dict_keys([0])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init Adversarial Trainining Defense
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 87050501 || trainable%: 26.719723301764798
model slices: dict_keys([1])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  2.82it/s]Epoch 0/49: 1it [00:00,  2.37it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 334, in forward
    attention_probs = self.dropout(attention_probs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/dropout.py", line 58, in forward
    return F.dropout(input, self.p, self.training, self.inplace)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/functional.py", line 1169, in dropout
    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.41 GiB total capacity; 37.05 GiB already allocated; 344.50 MiB free; 37.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 05:06:22.819 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 05:06:30.960 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
2024-10-01 05:14:39.552 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
================= iter seed  62  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 3
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: AdversarialTraining
Defense_Config: {'party': [0], 'position': 'head', 'adversarial_model': 'Mapping_MLP3', 'adversarial_model_lr': 0.0001, 'imagined_adversary': 'ImaginedAdversary_MLP3', 'imagined_adversary_lr': 0.0001, 'lambda': 5.0, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/3_0/AdversarialTraining_5.0,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 62 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 43928832 || trainable%: 0.902113673315967
model slices: dict_keys([0])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init Adversarial Trainining Defense
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 87050501 || trainable%: 26.719723301764798
model slices: dict_keys([1])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  2.68it/s]Epoch 0/49: 1it [00:00,  2.27it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 334, in forward
    attention_probs = self.dropout(attention_probs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/dropout.py", line 58, in forward
    return F.dropout(input, self.p, self.training, self.inplace)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/functional.py", line 1169, in dropout
    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.41 GiB total capacity; 37.05 GiB already allocated; 344.50 MiB free; 37.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 05:16:05.114 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 05:16:12.983 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
2024-10-01 05:24:17.030 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
================= iter seed  63  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 3
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: AdversarialTraining
Defense_Config: {'party': [0], 'position': 'head', 'adversarial_model': 'Mapping_MLP3', 'adversarial_model_lr': 0.0001, 'imagined_adversary': 'ImaginedAdversary_MLP3', 'imagined_adversary_lr': 0.0001, 'lambda': 0.001, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/3_0/AdversarialTraining_0.001,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 63 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 43928832 || trainable%: 0.902113673315967
model slices: dict_keys([0])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init Adversarial Trainining Defense
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 87050501 || trainable%: 26.719723301764798
model slices: dict_keys([1])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  2.77it/s]Epoch 0/49: 1it [00:00,  2.36it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 334, in forward
    attention_probs = self.dropout(attention_probs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/dropout.py", line 58, in forward
    return F.dropout(input, self.p, self.training, self.inplace)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/functional.py", line 1169, in dropout
    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.41 GiB total capacity; 37.05 GiB already allocated; 344.50 MiB free; 37.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 05:25:35.010 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 05:25:42.776 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
2024-10-01 05:33:51.059 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
================= iter seed  63  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 3
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: AdversarialTraining
Defense_Config: {'party': [0], 'position': 'head', 'adversarial_model': 'Mapping_MLP3', 'adversarial_model_lr': 0.0001, 'imagined_adversary': 'ImaginedAdversary_MLP3', 'imagined_adversary_lr': 0.0001, 'lambda': 0.01, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/3_0/AdversarialTraining_0.01,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 63 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 43928832 || trainable%: 0.902113673315967
model slices: dict_keys([0])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init Adversarial Trainining Defense
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 87050501 || trainable%: 26.719723301764798
model slices: dict_keys([1])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  2.76it/s]Epoch 0/49: 1it [00:00,  2.35it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 334, in forward
    attention_probs = self.dropout(attention_probs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/dropout.py", line 58, in forward
    return F.dropout(input, self.p, self.training, self.inplace)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/functional.py", line 1169, in dropout
    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.41 GiB total capacity; 37.05 GiB already allocated; 344.50 MiB free; 37.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 05:35:20.204 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 05:35:28.113 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
2024-10-01 05:43:29.850 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
================= iter seed  63  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 3
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: AdversarialTraining
Defense_Config: {'party': [0], 'position': 'head', 'adversarial_model': 'Mapping_MLP3', 'adversarial_model_lr': 0.0001, 'imagined_adversary': 'ImaginedAdversary_MLP3', 'imagined_adversary_lr': 0.0001, 'lambda': 0.1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/3_0/AdversarialTraining_0.1,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 63 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 43928832 || trainable%: 0.902113673315967
model slices: dict_keys([0])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init Adversarial Trainining Defense
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 87050501 || trainable%: 26.719723301764798
model slices: dict_keys([1])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  2.76it/s]Epoch 0/49: 1it [00:00,  2.33it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 334, in forward
    attention_probs = self.dropout(attention_probs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/dropout.py", line 58, in forward
    return F.dropout(input, self.p, self.training, self.inplace)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/functional.py", line 1169, in dropout
    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.41 GiB total capacity; 37.05 GiB already allocated; 344.50 MiB free; 37.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 05:44:53.096 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 05:45:01.222 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
2024-10-01 05:53:13.549 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
================= iter seed  63  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 3
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: AdversarialTraining
Defense_Config: {'party': [0], 'position': 'head', 'adversarial_model': 'Mapping_MLP3', 'adversarial_model_lr': 0.0001, 'imagined_adversary': 'ImaginedAdversary_MLP3', 'imagined_adversary_lr': 0.0001, 'lambda': 1.0, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/3_0/AdversarialTraining_1.0,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 63 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 43928832 || trainable%: 0.902113673315967
model slices: dict_keys([0])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init Adversarial Trainining Defense
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 87050501 || trainable%: 26.719723301764798
model slices: dict_keys([1])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  2.82it/s]Epoch 0/49: 1it [00:00,  2.36it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 334, in forward
    attention_probs = self.dropout(attention_probs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/dropout.py", line 58, in forward
    return F.dropout(input, self.p, self.training, self.inplace)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/functional.py", line 1169, in dropout
    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.41 GiB total capacity; 37.05 GiB already allocated; 344.50 MiB free; 37.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 05:54:42.905 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 05:54:50.680 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
2024-10-01 06:02:54.779 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
================= iter seed  63  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 3
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: AdversarialTraining
Defense_Config: {'party': [0], 'position': 'head', 'adversarial_model': 'Mapping_MLP3', 'adversarial_model_lr': 0.0001, 'imagined_adversary': 'ImaginedAdversary_MLP3', 'imagined_adversary_lr': 0.0001, 'lambda': 5.0, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/3_0/AdversarialTraining_5.0,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 63 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 43928832 || trainable%: 0.902113673315967
model slices: dict_keys([0])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init Adversarial Trainining Defense
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 87050501 || trainable%: 26.719723301764798
model slices: dict_keys([1])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  2.80it/s]Epoch 0/49: 1it [00:00,  2.37it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 334, in forward
    attention_probs = self.dropout(attention_probs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/dropout.py", line 58, in forward
    return F.dropout(input, self.p, self.training, self.inplace)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/functional.py", line 1169, in dropout
    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.41 GiB total capacity; 37.05 GiB already allocated; 344.50 MiB free; 37.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 06:04:17.273 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 06:04:25.955 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
2024-10-01 06:12:37.285 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
================= iter seed  64  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 3
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: AdversarialTraining
Defense_Config: {'party': [0], 'position': 'head', 'adversarial_model': 'Mapping_MLP3', 'adversarial_model_lr': 0.0001, 'imagined_adversary': 'ImaginedAdversary_MLP3', 'imagined_adversary_lr': 0.0001, 'lambda': 0.001, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/3_0/AdversarialTraining_0.001,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 64 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 43928832 || trainable%: 0.902113673315967
model slices: dict_keys([0])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init Adversarial Trainining Defense
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 87050501 || trainable%: 26.719723301764798
model slices: dict_keys([1])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  2.67it/s]Epoch 0/49: 1it [00:00,  2.20it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 334, in forward
    attention_probs = self.dropout(attention_probs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/dropout.py", line 58, in forward
    return F.dropout(input, self.p, self.training, self.inplace)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/functional.py", line 1169, in dropout
    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.41 GiB total capacity; 37.05 GiB already allocated; 344.50 MiB free; 37.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 06:14:05.221 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 06:14:13.452 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
2024-10-01 06:22:13.811 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
================= iter seed  64  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 3
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: AdversarialTraining
Defense_Config: {'party': [0], 'position': 'head', 'adversarial_model': 'Mapping_MLP3', 'adversarial_model_lr': 0.0001, 'imagined_adversary': 'ImaginedAdversary_MLP3', 'imagined_adversary_lr': 0.0001, 'lambda': 0.01, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/3_0/AdversarialTraining_0.01,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 64 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 43928832 || trainable%: 0.902113673315967
model slices: dict_keys([0])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init Adversarial Trainining Defense
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 87050501 || trainable%: 26.719723301764798
model slices: dict_keys([1])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  2.64it/s]Epoch 0/49: 1it [00:00,  2.23it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 334, in forward
    attention_probs = self.dropout(attention_probs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/dropout.py", line 58, in forward
    return F.dropout(input, self.p, self.training, self.inplace)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/functional.py", line 1169, in dropout
    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.41 GiB total capacity; 37.05 GiB already allocated; 344.50 MiB free; 37.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 06:23:37.894 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 06:23:46.825 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
2024-10-01 06:31:49.031 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
================= iter seed  64  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 3
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: AdversarialTraining
Defense_Config: {'party': [0], 'position': 'head', 'adversarial_model': 'Mapping_MLP3', 'adversarial_model_lr': 0.0001, 'imagined_adversary': 'ImaginedAdversary_MLP3', 'imagined_adversary_lr': 0.0001, 'lambda': 0.1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/3_0/AdversarialTraining_0.1,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 64 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 43928832 || trainable%: 0.902113673315967
model slices: dict_keys([0])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init Adversarial Trainining Defense
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 87050501 || trainable%: 26.719723301764798
model slices: dict_keys([1])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  2.88it/s]Epoch 0/49: 1it [00:00,  2.40it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 334, in forward
    attention_probs = self.dropout(attention_probs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/dropout.py", line 58, in forward
    return F.dropout(input, self.p, self.training, self.inplace)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/functional.py", line 1169, in dropout
    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.41 GiB total capacity; 37.05 GiB already allocated; 344.50 MiB free; 37.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 06:33:14.298 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 06:33:21.548 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
2024-10-01 06:41:15.143 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
================= iter seed  64  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 3
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: AdversarialTraining
Defense_Config: {'party': [0], 'position': 'head', 'adversarial_model': 'Mapping_MLP3', 'adversarial_model_lr': 0.0001, 'imagined_adversary': 'ImaginedAdversary_MLP3', 'imagined_adversary_lr': 0.0001, 'lambda': 1.0, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/3_0/AdversarialTraining_1.0,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 64 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 43928832 || trainable%: 0.902113673315967
model slices: dict_keys([0])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init Adversarial Trainining Defense
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 87050501 || trainable%: 26.719723301764798
model slices: dict_keys([1])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  2.74it/s]Epoch 0/49: 1it [00:00,  2.32it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 334, in forward
    attention_probs = self.dropout(attention_probs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/dropout.py", line 58, in forward
    return F.dropout(input, self.p, self.training, self.inplace)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/functional.py", line 1169, in dropout
    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.41 GiB total capacity; 37.05 GiB already allocated; 344.50 MiB free; 37.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 06:42:35.275 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 06:42:43.060 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
2024-10-01 06:50:56.739 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
================= iter seed  64  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 3
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: AdversarialTraining
Defense_Config: {'party': [0], 'position': 'head', 'adversarial_model': 'Mapping_MLP3', 'adversarial_model_lr': 0.0001, 'imagined_adversary': 'ImaginedAdversary_MLP3', 'imagined_adversary_lr': 0.0001, 'lambda': 5.0, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/3_0/AdversarialTraining_5.0,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 64 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 43928832 || trainable%: 0.902113673315967
model slices: dict_keys([0])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init Adversarial Trainining Defense
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 87050501 || trainable%: 26.719723301764798
model slices: dict_keys([1])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  2.88it/s]Epoch 0/49: 1it [00:00,  2.43it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 334, in forward
    attention_probs = self.dropout(attention_probs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/dropout.py", line 58, in forward
    return F.dropout(input, self.p, self.training, self.inplace)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/functional.py", line 1169, in dropout
    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.41 GiB total capacity; 37.05 GiB already allocated; 344.50 MiB free; 37.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 06:52:22.154 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 06:52:30.030 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
2024-10-01 07:00:34.613 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
================= iter seed  65  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 3
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: AdversarialTraining
Defense_Config: {'party': [0], 'position': 'head', 'adversarial_model': 'Mapping_MLP3', 'adversarial_model_lr': 0.0001, 'imagined_adversary': 'ImaginedAdversary_MLP3', 'imagined_adversary_lr': 0.0001, 'lambda': 0.001, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/3_0/AdversarialTraining_0.001,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 65 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 43928832 || trainable%: 0.902113673315967
model slices: dict_keys([0])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init Adversarial Trainining Defense
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 87050501 || trainable%: 26.719723301764798
model slices: dict_keys([1])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  2.72it/s]Epoch 0/49: 1it [00:00,  2.27it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 334, in forward
    attention_probs = self.dropout(attention_probs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/dropout.py", line 58, in forward
    return F.dropout(input, self.p, self.training, self.inplace)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/functional.py", line 1169, in dropout
    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.41 GiB total capacity; 37.05 GiB already allocated; 344.50 MiB free; 37.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 07:01:53.182 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 07:02:01.565 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
2024-10-01 07:10:10.575 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
================= iter seed  65  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 3
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: AdversarialTraining
Defense_Config: {'party': [0], 'position': 'head', 'adversarial_model': 'Mapping_MLP3', 'adversarial_model_lr': 0.0001, 'imagined_adversary': 'ImaginedAdversary_MLP3', 'imagined_adversary_lr': 0.0001, 'lambda': 0.01, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/3_0/AdversarialTraining_0.01,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 65 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 43928832 || trainable%: 0.902113673315967
model slices: dict_keys([0])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init Adversarial Trainining Defense
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 87050501 || trainable%: 26.719723301764798
model slices: dict_keys([1])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  2.63it/s]Epoch 0/49: 1it [00:00,  2.23it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 334, in forward
    attention_probs = self.dropout(attention_probs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/dropout.py", line 58, in forward
    return F.dropout(input, self.p, self.training, self.inplace)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/functional.py", line 1169, in dropout
    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.41 GiB total capacity; 37.05 GiB already allocated; 344.50 MiB free; 37.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 07:11:33.005 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 07:11:40.466 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
2024-10-01 07:19:33.973 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
================= iter seed  65  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 3
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: AdversarialTraining
Defense_Config: {'party': [0], 'position': 'head', 'adversarial_model': 'Mapping_MLP3', 'adversarial_model_lr': 0.0001, 'imagined_adversary': 'ImaginedAdversary_MLP3', 'imagined_adversary_lr': 0.0001, 'lambda': 0.1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/3_0/AdversarialTraining_0.1,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 65 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 43928832 || trainable%: 0.902113673315967
model slices: dict_keys([0])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init Adversarial Trainining Defense
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 87050501 || trainable%: 26.719723301764798
model slices: dict_keys([1])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  2.81it/s]Epoch 0/49: 1it [00:00,  2.34it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 334, in forward
    attention_probs = self.dropout(attention_probs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/dropout.py", line 58, in forward
    return F.dropout(input, self.p, self.training, self.inplace)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/functional.py", line 1169, in dropout
    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.41 GiB total capacity; 37.05 GiB already allocated; 344.50 MiB free; 37.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 07:20:53.570 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 07:21:02.210 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
2024-10-01 07:29:05.195 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
================= iter seed  65  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 3
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: AdversarialTraining
Defense_Config: {'party': [0], 'position': 'head', 'adversarial_model': 'Mapping_MLP3', 'adversarial_model_lr': 0.0001, 'imagined_adversary': 'ImaginedAdversary_MLP3', 'imagined_adversary_lr': 0.0001, 'lambda': 1.0, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/3_0/AdversarialTraining_1.0,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 65 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 43928832 || trainable%: 0.902113673315967
model slices: dict_keys([0])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init Adversarial Trainining Defense
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 87050501 || trainable%: 26.719723301764798
model slices: dict_keys([1])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  2.81it/s]Epoch 0/49: 1it [00:00,  2.39it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 334, in forward
    attention_probs = self.dropout(attention_probs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/dropout.py", line 58, in forward
    return F.dropout(input, self.p, self.training, self.inplace)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/functional.py", line 1169, in dropout
    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.41 GiB total capacity; 37.05 GiB already allocated; 344.50 MiB free; 37.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 07:30:30.953 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 07:30:38.431 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
2024-10-01 07:38:31.594 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(3,)
================= iter seed  65  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 3
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: AdversarialTraining
Defense_Config: {'party': [0], 'position': 'head', 'adversarial_model': 'Mapping_MLP3', 'adversarial_model_lr': 0.0001, 'imagined_adversary': 'ImaginedAdversary_MLP3', 'imagined_adversary_lr': 0.0001, 'lambda': 5.0, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/3_0/AdversarialTraining_5.0,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 65 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 43928832 || trainable%: 0.902113673315967
model slices: dict_keys([0])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init Adversarial Trainining Defense
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 87050501 || trainable%: 26.719723301764798
model slices: dict_keys([1])
model partition: 0head-3/1tail-9
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  2.69it/s]Epoch 0/49: 1it [00:00,  2.28it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 334, in forward
    attention_probs = self.dropout(attention_probs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/dropout.py", line 58, in forward
    return F.dropout(input, self.p, self.training, self.inplace)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/functional.py", line 1169, in dropout
    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.41 GiB total capacity; 37.05 GiB already allocated; 344.50 MiB free; 37.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

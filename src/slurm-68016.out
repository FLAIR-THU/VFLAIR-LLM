/bin/bash: /home/DAIR/guzx/.conda/envs/py38/lib/libtinfo.so.6: no version information available (required by /bin/bash)
2024-09-29 19:08:32.053 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-09-29 19:08:58.018 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/meta-mathMetaMath-Mistral-7B_vfl_(2,)
2024-09-29 19:09:58.926 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/meta-mathMetaMath-Mistral-7B_vfl_(2,)
================= iter seed  60  =================
communication_protocol: FedSGD
load model_architect: CLM
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 2
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 0
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: AdversarialTraining
Defense_Config: {'party': [0], 'position': 'head', 'adversarial_model': 'Mapping_MLP3_noflatten', 'adversarial_model_lr': 0.001, 'imagined_adversary': 'ImaginedAdversary_MLP3_noflatten', 'imagined_adversary_lr': 0.001, 'lambda': 0.001, 'seq_length': 512, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/GMS8K/2-slice/2_0/AdversarialTraining_0.001,finetuned_model=meta-mathMetaMath-Mistral-7B.txt
=================================

===== iter 60 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
default tokenizer.pad_token: [PAD]
pad_token: [PAD]   pad_id: 32000
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
trainable params: 0 || all params: 567300096 || trainable%: 0.0
model slices: dict_keys([0])
model partition: 0head-2/1tail-30
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
7473 train examples
1319 test examples
X: <class 'numpy.ndarray'> 7473 1319
y <class 'numpy.ndarray'> 7473 1319
Passive Party 0: init Adversarial Trainining Defense
Adversarial_MLP init: 512 4096
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 32000
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:46<03:52, 46.52s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [02:51<06:09, 92.39s/it]Loading checkpoint shards:  50%|█████     | 3/6 [04:50<05:13, 104.66s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [06:53<03:44, 112.08s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [08:48<01:52, 112.94s/it]Loading checkpoint shards: 100%|██████████| 6/6 [09:08<00:00, 81.46s/it] Loading checkpoint shards: 100%|██████████| 6/6 [09:08<00:00, 91.42s/it]
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=0
trainable params: 131076096 || all params: 6805516288 || trainable%: 1.926027217525337
model slices: dict_keys([1])
model partition: 0head-2/1tail-30
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.mistral.modeling_mistral.MistralForCausalLM'>
--> final self.generation_config: GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2
}

--> final self.generation_config_dict: {'temperature': 1.0, 'top_k': -1, 'top_p': 1.0, 'max_new_tokens': 256}
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 0it [00:00, ?it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/mistral/modeling_mistral.py", line 1158, in forward
    outputs = self.model(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/mistral.py", line 452, in forward
    layer_outputs = decoder_layer(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/mistral/modeling_mistral.py", line 770, in forward
    hidden_states = self.mlp(hidden_states)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/mistral/modeling_mistral.py", line 179, in forward
    return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 103, in forward
    return F.linear(input, self.weight, self.bias)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/functional.py", line 1848, in linear
    return torch._C._nn.linear(input, weight, bias)
RuntimeError: CUDA out of memory. Tried to allocate 112.00 MiB (GPU 0; 39.39 GiB total capacity; 37.27 GiB already allocated; 38.81 MiB free; 37.58 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-09-29 19:19:34.077 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-09-29 19:19:46.180 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/meta-mathMetaMath-Mistral-7B_vfl_(2,)
2024-09-29 19:20:18.296 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/meta-mathMetaMath-Mistral-7B_vfl_(2,)
================= iter seed  60  =================
communication_protocol: FedSGD
load model_architect: CLM
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 2
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 0
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: AdversarialTraining
Defense_Config: {'party': [0], 'position': 'head', 'adversarial_model': 'Mapping_MLP3_noflatten', 'adversarial_model_lr': 0.001, 'imagined_adversary': 'ImaginedAdversary_MLP3_noflatten', 'imagined_adversary_lr': 0.001, 'lambda': 0.01, 'seq_length': 512, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/GMS8K/2-slice/2_0/AdversarialTraining_0.01,finetuned_model=meta-mathMetaMath-Mistral-7B.txt
=================================

===== iter 60 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
default tokenizer.pad_token: [PAD]
pad_token: [PAD]   pad_id: 32000
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
trainable params: 0 || all params: 567300096 || trainable%: 0.0
model slices: dict_keys([0])
model partition: 0head-2/1tail-30
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
7473 train examples
1319 test examples
X: <class 'numpy.ndarray'> 7473 1319
y <class 'numpy.ndarray'> 7473 1319
Passive Party 0: init Adversarial Trainining Defense
Adversarial_MLP init: 512 4096
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 32000
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:10<00:50, 10.19s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:20<00:41, 10.34s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:32<00:33, 11.06s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:41<00:20, 10.16s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:49<00:09,  9.54s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:53<00:00,  7.71s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:53<00:00,  8.99s/it]
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=0
trainable params: 131076096 || all params: 6805516288 || trainable%: 1.926027217525337
model slices: dict_keys([1])
model partition: 0head-2/1tail-30
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.mistral.modeling_mistral.MistralForCausalLM'>
--> final self.generation_config: GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2
}

--> final self.generation_config_dict: {'temperature': 1.0, 'top_k': -1, 'top_p': 1.0, 'max_new_tokens': 256}
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 0it [00:00, ?it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/mistral/modeling_mistral.py", line 1158, in forward
    outputs = self.model(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/mistral.py", line 452, in forward
    layer_outputs = decoder_layer(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/mistral/modeling_mistral.py", line 770, in forward
    hidden_states = self.mlp(hidden_states)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/mistral/modeling_mistral.py", line 179, in forward
    return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 103, in forward
    return F.linear(input, self.weight, self.bias)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/functional.py", line 1848, in linear
    return torch._C._nn.linear(input, weight, bias)
RuntimeError: CUDA out of memory. Tried to allocate 112.00 MiB (GPU 0; 39.39 GiB total capacity; 37.27 GiB already allocated; 38.81 MiB free; 37.58 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-09-29 19:21:34.548 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-09-29 19:21:47.033 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/meta-mathMetaMath-Mistral-7B_vfl_(2,)
2024-09-29 19:22:19.910 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/meta-mathMetaMath-Mistral-7B_vfl_(2,)
================= iter seed  60  =================
communication_protocol: FedSGD
load model_architect: CLM
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 2
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 0
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: AdversarialTraining
Defense_Config: {'party': [0], 'position': 'head', 'adversarial_model': 'Mapping_MLP3_noflatten', 'adversarial_model_lr': 0.001, 'imagined_adversary': 'ImaginedAdversary_MLP3_noflatten', 'imagined_adversary_lr': 0.001, 'lambda': 0.1, 'seq_length': 512, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/GMS8K/2-slice/2_0/AdversarialTraining_0.1,finetuned_model=meta-mathMetaMath-Mistral-7B.txt
=================================

===== iter 60 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
default tokenizer.pad_token: [PAD]
pad_token: [PAD]   pad_id: 32000
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
trainable params: 0 || all params: 567300096 || trainable%: 0.0
model slices: dict_keys([0])
model partition: 0head-2/1tail-30
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
7473 train examples
1319 test examples
X: <class 'numpy.ndarray'> 7473 1319
y <class 'numpy.ndarray'> 7473 1319
Passive Party 0: init Adversarial Trainining Defense
Adversarial_MLP init: 512 4096
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 32000
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:09<00:49,  9.87s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:19<00:39,  9.95s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:26<00:25,  8.54s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:34<00:16,  8.21s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:41<00:07,  7.87s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:45<00:00,  6.51s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:45<00:00,  7.60s/it]
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=0
trainable params: 131076096 || all params: 6805516288 || trainable%: 1.926027217525337
model slices: dict_keys([1])
model partition: 0head-2/1tail-30
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.mistral.modeling_mistral.MistralForCausalLM'>
--> final self.generation_config: GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2
}

--> final self.generation_config_dict: {'temperature': 1.0, 'top_k': -1, 'top_p': 1.0, 'max_new_tokens': 256}
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 0it [00:00, ?it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/mistral/modeling_mistral.py", line 1158, in forward
    outputs = self.model(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/mistral.py", line 452, in forward
    layer_outputs = decoder_layer(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/mistral/modeling_mistral.py", line 770, in forward
    hidden_states = self.mlp(hidden_states)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/mistral/modeling_mistral.py", line 179, in forward
    return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 103, in forward
    return F.linear(input, self.weight, self.bias)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/functional.py", line 1848, in linear
    return torch._C._nn.linear(input, weight, bias)
RuntimeError: CUDA out of memory. Tried to allocate 112.00 MiB (GPU 0; 39.39 GiB total capacity; 37.27 GiB already allocated; 38.81 MiB free; 37.58 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-09-29 19:23:28.229 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-09-29 19:23:40.434 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/meta-mathMetaMath-Mistral-7B_vfl_(2,)
2024-09-29 19:24:01.848 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/meta-mathMetaMath-Mistral-7B_vfl_(2,)
================= iter seed  61  =================
communication_protocol: FedSGD
load model_architect: CLM
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 2
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 0
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: AdversarialTraining
Defense_Config: {'party': [0], 'position': 'head', 'adversarial_model': 'Mapping_MLP3_noflatten', 'adversarial_model_lr': 0.001, 'imagined_adversary': 'ImaginedAdversary_MLP3_noflatten', 'imagined_adversary_lr': 0.001, 'lambda': 0.001, 'seq_length': 512, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/GMS8K/2-slice/2_0/AdversarialTraining_0.001,finetuned_model=meta-mathMetaMath-Mistral-7B.txt
=================================

===== iter 61 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
default tokenizer.pad_token: [PAD]
pad_token: [PAD]   pad_id: 32000
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
trainable params: 0 || all params: 567300096 || trainable%: 0.0
model slices: dict_keys([0])
model partition: 0head-2/1tail-30
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
7473 train examples
1319 test examples
X: <class 'numpy.ndarray'> 7473 1319
y <class 'numpy.ndarray'> 7473 1319
Passive Party 0: init Adversarial Trainining Defense
Adversarial_MLP init: 512 4096
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 32000
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:07<00:38,  7.74s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:15<00:31,  7.81s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:24<00:25,  8.41s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:35<00:18,  9.39s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:47<00:10, 10.12s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:51<00:00,  8.05s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:51<00:00,  8.51s/it]
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=0
trainable params: 131076096 || all params: 6805516288 || trainable%: 1.926027217525337
model slices: dict_keys([1])
model partition: 0head-2/1tail-30
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.mistral.modeling_mistral.MistralForCausalLM'>
--> final self.generation_config: GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2
}

--> final self.generation_config_dict: {'temperature': 1.0, 'top_k': -1, 'top_p': 1.0, 'max_new_tokens': 256}
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 0it [00:00, ?it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/mistral/modeling_mistral.py", line 1158, in forward
    outputs = self.model(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/mistral.py", line 452, in forward
    layer_outputs = decoder_layer(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/mistral/modeling_mistral.py", line 770, in forward
    hidden_states = self.mlp(hidden_states)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/mistral/modeling_mistral.py", line 179, in forward
    return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 103, in forward
    return F.linear(input, self.weight, self.bias)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/functional.py", line 1848, in linear
    return torch._C._nn.linear(input, weight, bias)
RuntimeError: CUDA out of memory. Tried to allocate 112.00 MiB (GPU 0; 39.39 GiB total capacity; 37.27 GiB already allocated; 38.81 MiB free; 37.58 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-09-29 19:25:21.097 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-09-29 19:25:32.408 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/meta-mathMetaMath-Mistral-7B_vfl_(2,)
2024-09-29 19:25:55.737 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/meta-mathMetaMath-Mistral-7B_vfl_(2,)
================= iter seed  61  =================
communication_protocol: FedSGD
load model_architect: CLM
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 2
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 0
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: AdversarialTraining
Defense_Config: {'party': [0], 'position': 'head', 'adversarial_model': 'Mapping_MLP3_noflatten', 'adversarial_model_lr': 0.001, 'imagined_adversary': 'ImaginedAdversary_MLP3_noflatten', 'imagined_adversary_lr': 0.001, 'lambda': 0.01, 'seq_length': 512, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/GMS8K/2-slice/2_0/AdversarialTraining_0.01,finetuned_model=meta-mathMetaMath-Mistral-7B.txt
=================================

===== iter 61 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
default tokenizer.pad_token: [PAD]
pad_token: [PAD]   pad_id: 32000
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
trainable params: 0 || all params: 567300096 || trainable%: 0.0
model slices: dict_keys([0])
model partition: 0head-2/1tail-30
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
7473 train examples
1319 test examples
X: <class 'numpy.ndarray'> 7473 1319
y <class 'numpy.ndarray'> 7473 1319
Passive Party 0: init Adversarial Trainining Defense
Adversarial_MLP init: 512 4096
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 32000
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:18<01:32, 18.47s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:28<00:53, 13.34s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:37<00:34, 11.63s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:50<00:24, 12.16s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [01:00<00:11, 11.13s/it]Loading checkpoint shards: 100%|██████████| 6/6 [01:04<00:00,  8.89s/it]Loading checkpoint shards: 100%|██████████| 6/6 [01:04<00:00, 10.77s/it]
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=0
trainable params: 131076096 || all params: 6805516288 || trainable%: 1.926027217525337
model slices: dict_keys([1])
model partition: 0head-2/1tail-30
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.mistral.modeling_mistral.MistralForCausalLM'>
--> final self.generation_config: GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2
}

--> final self.generation_config_dict: {'temperature': 1.0, 'top_k': -1, 'top_p': 1.0, 'max_new_tokens': 256}
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 0it [00:00, ?it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/mistral/modeling_mistral.py", line 1158, in forward
    outputs = self.model(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/mistral.py", line 452, in forward
    layer_outputs = decoder_layer(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/mistral/modeling_mistral.py", line 770, in forward
    hidden_states = self.mlp(hidden_states)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/mistral/modeling_mistral.py", line 179, in forward
    return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 103, in forward
    return F.linear(input, self.weight, self.bias)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/functional.py", line 1848, in linear
    return torch._C._nn.linear(input, weight, bias)
RuntimeError: CUDA out of memory. Tried to allocate 112.00 MiB (GPU 0; 39.39 GiB total capacity; 37.27 GiB already allocated; 38.81 MiB free; 37.58 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-09-29 19:27:27.986 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-09-29 19:27:39.459 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/meta-mathMetaMath-Mistral-7B_vfl_(2,)
2024-09-29 19:28:13.683 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/meta-mathMetaMath-Mistral-7B_vfl_(2,)
================= iter seed  61  =================
communication_protocol: FedSGD
load model_architect: CLM
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 2
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 0
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: AdversarialTraining
Defense_Config: {'party': [0], 'position': 'head', 'adversarial_model': 'Mapping_MLP3_noflatten', 'adversarial_model_lr': 0.001, 'imagined_adversary': 'ImaginedAdversary_MLP3_noflatten', 'imagined_adversary_lr': 0.001, 'lambda': 0.1, 'seq_length': 512, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/GMS8K/2-slice/2_0/AdversarialTraining_0.1,finetuned_model=meta-mathMetaMath-Mistral-7B.txt
=================================

===== iter 61 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
default tokenizer.pad_token: [PAD]
pad_token: [PAD]   pad_id: 32000
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
trainable params: 0 || all params: 567300096 || trainable%: 0.0
model slices: dict_keys([0])
model partition: 0head-2/1tail-30
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
7473 train examples
1319 test examples
X: <class 'numpy.ndarray'> 7473 1319
y <class 'numpy.ndarray'> 7473 1319
Passive Party 0: init Adversarial Trainining Defense
Adversarial_MLP init: 512 4096
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 32000
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:13<01:06, 13.34s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:22<00:43, 10.75s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:28<00:25,  8.48s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:41<00:20, 10.34s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:49<00:09,  9.48s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:53<00:00,  7.60s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:53<00:00,  8.86s/it]
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=0
trainable params: 131076096 || all params: 6805516288 || trainable%: 1.926027217525337
model slices: dict_keys([1])
model partition: 0head-2/1tail-30
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.mistral.modeling_mistral.MistralForCausalLM'>
--> final self.generation_config: GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2
}

--> final self.generation_config_dict: {'temperature': 1.0, 'top_k': -1, 'top_p': 1.0, 'max_new_tokens': 256}
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 0it [00:00, ?it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/mistral/modeling_mistral.py", line 1158, in forward
    outputs = self.model(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/mistral.py", line 452, in forward
    layer_outputs = decoder_layer(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/mistral/modeling_mistral.py", line 770, in forward
    hidden_states = self.mlp(hidden_states)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/mistral/modeling_mistral.py", line 179, in forward
    return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 103, in forward
    return F.linear(input, self.weight, self.bias)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/functional.py", line 1848, in linear
    return torch._C._nn.linear(input, weight, bias)
RuntimeError: CUDA out of memory. Tried to allocate 112.00 MiB (GPU 0; 39.39 GiB total capacity; 37.27 GiB already allocated; 38.81 MiB free; 37.58 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-09-29 19:29:32.323 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-09-29 19:29:44.372 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/meta-mathMetaMath-Mistral-7B_vfl_(2,)
2024-09-29 19:30:18.375 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/meta-mathMetaMath-Mistral-7B_vfl_(2,)
================= iter seed  62  =================
communication_protocol: FedSGD
load model_architect: CLM
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 2
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 0
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: AdversarialTraining
Defense_Config: {'party': [0], 'position': 'head', 'adversarial_model': 'Mapping_MLP3_noflatten', 'adversarial_model_lr': 0.001, 'imagined_adversary': 'ImaginedAdversary_MLP3_noflatten', 'imagined_adversary_lr': 0.001, 'lambda': 0.001, 'seq_length': 512, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/GMS8K/2-slice/2_0/AdversarialTraining_0.001,finetuned_model=meta-mathMetaMath-Mistral-7B.txt
=================================

===== iter 62 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
default tokenizer.pad_token: [PAD]
pad_token: [PAD]   pad_id: 32000
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
trainable params: 0 || all params: 567300096 || trainable%: 0.0
model slices: dict_keys([0])
model partition: 0head-2/1tail-30
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
7473 train examples
1319 test examples
X: <class 'numpy.ndarray'> 7473 1319
y <class 'numpy.ndarray'> 7473 1319
Passive Party 0: init Adversarial Trainining Defense
Adversarial_MLP init: 512 4096
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 32000
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:13<01:07, 13.46s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:22<00:42, 10.68s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:35<00:35, 11.74s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:45<00:22, 11.28s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:53<00:10, 10.12s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:58<00:00,  8.15s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:58<00:00,  9.70s/it]
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=0
trainable params: 131076096 || all params: 6805516288 || trainable%: 1.926027217525337
model slices: dict_keys([1])
model partition: 0head-2/1tail-30
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.mistral.modeling_mistral.MistralForCausalLM'>
--> final self.generation_config: GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2
}

--> final self.generation_config_dict: {'temperature': 1.0, 'top_k': -1, 'top_p': 1.0, 'max_new_tokens': 256}
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 0it [00:00, ?it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/mistral/modeling_mistral.py", line 1158, in forward
    outputs = self.model(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/mistral.py", line 452, in forward
    layer_outputs = decoder_layer(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/mistral/modeling_mistral.py", line 770, in forward
    hidden_states = self.mlp(hidden_states)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/mistral/modeling_mistral.py", line 179, in forward
    return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 103, in forward
    return F.linear(input, self.weight, self.bias)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/functional.py", line 1848, in linear
    return torch._C._nn.linear(input, weight, bias)
RuntimeError: CUDA out of memory. Tried to allocate 112.00 MiB (GPU 0; 39.39 GiB total capacity; 37.27 GiB already allocated; 38.81 MiB free; 37.58 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-09-29 19:31:40.705 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-09-29 19:31:52.970 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/meta-mathMetaMath-Mistral-7B_vfl_(2,)
2024-09-29 19:32:31.809 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/meta-mathMetaMath-Mistral-7B_vfl_(2,)
================= iter seed  62  =================
communication_protocol: FedSGD
load model_architect: CLM
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 2
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 0
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: AdversarialTraining
Defense_Config: {'party': [0], 'position': 'head', 'adversarial_model': 'Mapping_MLP3_noflatten', 'adversarial_model_lr': 0.001, 'imagined_adversary': 'ImaginedAdversary_MLP3_noflatten', 'imagined_adversary_lr': 0.001, 'lambda': 0.01, 'seq_length': 512, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/GMS8K/2-slice/2_0/AdversarialTraining_0.01,finetuned_model=meta-mathMetaMath-Mistral-7B.txt
=================================

===== iter 62 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
default tokenizer.pad_token: [PAD]
pad_token: [PAD]   pad_id: 32000
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
trainable params: 0 || all params: 567300096 || trainable%: 0.0
model slices: dict_keys([0])
model partition: 0head-2/1tail-30
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
7473 train examples
1319 test examples
X: <class 'numpy.ndarray'> 7473 1319
y <class 'numpy.ndarray'> 7473 1319
Passive Party 0: init Adversarial Trainining Defense
Adversarial_MLP init: 512 4096
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 32000
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:10<00:50, 10.19s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:21<00:43, 10.90s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:33<00:34, 11.51s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:39<00:18,  9.29s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:48<00:09,  9.07s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:53<00:00,  7.65s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:53<00:00,  8.88s/it]
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=0
trainable params: 131076096 || all params: 6805516288 || trainable%: 1.926027217525337
model slices: dict_keys([1])
model partition: 0head-2/1tail-30
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.mistral.modeling_mistral.MistralForCausalLM'>
--> final self.generation_config: GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2
}

--> final self.generation_config_dict: {'temperature': 1.0, 'top_k': -1, 'top_p': 1.0, 'max_new_tokens': 256}
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 0it [00:00, ?it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/mistral/modeling_mistral.py", line 1158, in forward
    outputs = self.model(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/mistral.py", line 452, in forward
    layer_outputs = decoder_layer(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/mistral/modeling_mistral.py", line 770, in forward
    hidden_states = self.mlp(hidden_states)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/mistral/modeling_mistral.py", line 179, in forward
    return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 103, in forward
    return F.linear(input, self.weight, self.bias)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/functional.py", line 1848, in linear
    return torch._C._nn.linear(input, weight, bias)
RuntimeError: CUDA out of memory. Tried to allocate 112.00 MiB (GPU 0; 39.39 GiB total capacity; 37.27 GiB already allocated; 38.81 MiB free; 37.58 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-09-29 19:33:46.712 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-09-29 19:33:58.768 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/meta-mathMetaMath-Mistral-7B_vfl_(2,)
2024-09-29 19:34:31.468 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/meta-mathMetaMath-Mistral-7B_vfl_(2,)
================= iter seed  62  =================
communication_protocol: FedSGD
load model_architect: CLM
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 2
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 0
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: AdversarialTraining
Defense_Config: {'party': [0], 'position': 'head', 'adversarial_model': 'Mapping_MLP3_noflatten', 'adversarial_model_lr': 0.001, 'imagined_adversary': 'ImaginedAdversary_MLP3_noflatten', 'imagined_adversary_lr': 0.001, 'lambda': 0.1, 'seq_length': 512, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/GMS8K/2-slice/2_0/AdversarialTraining_0.1,finetuned_model=meta-mathMetaMath-Mistral-7B.txt
=================================

===== iter 62 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
default tokenizer.pad_token: [PAD]
pad_token: [PAD]   pad_id: 32000
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
trainable params: 0 || all params: 567300096 || trainable%: 0.0
model slices: dict_keys([0])
model partition: 0head-2/1tail-30
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
7473 train examples
1319 test examples
X: <class 'numpy.ndarray'> 7473 1319
y <class 'numpy.ndarray'> 7473 1319
Passive Party 0: init Adversarial Trainining Defense
Adversarial_MLP init: 512 4096
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 32000
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:06<00:32,  6.45s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:13<00:27,  6.95s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:24<00:26,  8.87s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:36<00:19,  9.84s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:44<00:09,  9.15s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:47<00:00,  7.33s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:47<00:00,  7.99s/it]
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=0
trainable params: 131076096 || all params: 6805516288 || trainable%: 1.926027217525337
model slices: dict_keys([1])
model partition: 0head-2/1tail-30
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.mistral.modeling_mistral.MistralForCausalLM'>
--> final self.generation_config: GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2
}

--> final self.generation_config_dict: {'temperature': 1.0, 'top_k': -1, 'top_p': 1.0, 'max_new_tokens': 256}
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 0it [00:00, ?it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/mistral/modeling_mistral.py", line 1158, in forward
    outputs = self.model(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/mistral.py", line 452, in forward
    layer_outputs = decoder_layer(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/mistral/modeling_mistral.py", line 770, in forward
    hidden_states = self.mlp(hidden_states)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/mistral/modeling_mistral.py", line 179, in forward
    return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 103, in forward
    return F.linear(input, self.weight, self.bias)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/functional.py", line 1848, in linear
    return torch._C._nn.linear(input, weight, bias)
RuntimeError: CUDA out of memory. Tried to allocate 112.00 MiB (GPU 0; 39.39 GiB total capacity; 37.27 GiB already allocated; 38.81 MiB free; 37.58 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-09-29 19:35:43.106 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-09-29 19:35:55.140 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/meta-mathMetaMath-Mistral-7B_vfl_(2,)
2024-09-29 19:36:18.750 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/meta-mathMetaMath-Mistral-7B_vfl_(2,)
================= iter seed  63  =================
communication_protocol: FedSGD
load model_architect: CLM
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 2
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 0
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: AdversarialTraining
Defense_Config: {'party': [0], 'position': 'head', 'adversarial_model': 'Mapping_MLP3_noflatten', 'adversarial_model_lr': 0.001, 'imagined_adversary': 'ImaginedAdversary_MLP3_noflatten', 'imagined_adversary_lr': 0.001, 'lambda': 0.001, 'seq_length': 512, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/GMS8K/2-slice/2_0/AdversarialTraining_0.001,finetuned_model=meta-mathMetaMath-Mistral-7B.txt
=================================

===== iter 63 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
default tokenizer.pad_token: [PAD]
pad_token: [PAD]   pad_id: 32000
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
trainable params: 0 || all params: 567300096 || trainable%: 0.0
model slices: dict_keys([0])
model partition: 0head-2/1tail-30
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
7473 train examples
1319 test examples
X: <class 'numpy.ndarray'> 7473 1319
y <class 'numpy.ndarray'> 7473 1319
Passive Party 0: init Adversarial Trainining Defense
Adversarial_MLP init: 512 4096
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 32000
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:15<01:15, 15.01s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:24<00:46, 11.64s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:34<00:33, 11.01s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:48<00:24, 12.03s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:56<00:10, 10.66s/it]Loading checkpoint shards: 100%|██████████| 6/6 [01:01<00:00,  8.66s/it]Loading checkpoint shards: 100%|██████████| 6/6 [01:01<00:00, 10.19s/it]
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=0
trainable params: 131076096 || all params: 6805516288 || trainable%: 1.926027217525337
model slices: dict_keys([1])
model partition: 0head-2/1tail-30
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.mistral.modeling_mistral.MistralForCausalLM'>
--> final self.generation_config: GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2
}

--> final self.generation_config_dict: {'temperature': 1.0, 'top_k': -1, 'top_p': 1.0, 'max_new_tokens': 256}
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 0it [00:00, ?it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/mistral/modeling_mistral.py", line 1158, in forward
    outputs = self.model(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/mistral.py", line 452, in forward
    layer_outputs = decoder_layer(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/mistral/modeling_mistral.py", line 770, in forward
    hidden_states = self.mlp(hidden_states)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/mistral/modeling_mistral.py", line 179, in forward
    return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 103, in forward
    return F.linear(input, self.weight, self.bias)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/functional.py", line 1848, in linear
    return torch._C._nn.linear(input, weight, bias)
RuntimeError: CUDA out of memory. Tried to allocate 112.00 MiB (GPU 0; 39.39 GiB total capacity; 37.27 GiB already allocated; 38.81 MiB free; 37.58 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-09-29 19:37:44.651 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-09-29 19:37:56.553 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/meta-mathMetaMath-Mistral-7B_vfl_(2,)
2024-09-29 19:38:34.033 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/meta-mathMetaMath-Mistral-7B_vfl_(2,)
================= iter seed  63  =================
communication_protocol: FedSGD
load model_architect: CLM
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 2
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 0
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: AdversarialTraining
Defense_Config: {'party': [0], 'position': 'head', 'adversarial_model': 'Mapping_MLP3_noflatten', 'adversarial_model_lr': 0.001, 'imagined_adversary': 'ImaginedAdversary_MLP3_noflatten', 'imagined_adversary_lr': 0.001, 'lambda': 0.01, 'seq_length': 512, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/GMS8K/2-slice/2_0/AdversarialTraining_0.01,finetuned_model=meta-mathMetaMath-Mistral-7B.txt
=================================

===== iter 63 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
default tokenizer.pad_token: [PAD]
pad_token: [PAD]   pad_id: 32000
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
trainable params: 0 || all params: 567300096 || trainable%: 0.0
model slices: dict_keys([0])
model partition: 0head-2/1tail-30
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
7473 train examples
1319 test examples
X: <class 'numpy.ndarray'> 7473 1319
y <class 'numpy.ndarray'> 7473 1319
Passive Party 0: init Adversarial Trainining Defense
Adversarial_MLP init: 512 4096
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 32000
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:10<00:51, 10.23s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:19<00:37,  9.39s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:29<00:29,  9.70s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:36<00:17,  8.95s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:45<00:08,  8.75s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:49<00:00,  7.10s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:49<00:00,  8.20s/it]
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=0
trainable params: 131076096 || all params: 6805516288 || trainable%: 1.926027217525337
model slices: dict_keys([1])
model partition: 0head-2/1tail-30
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.mistral.modeling_mistral.MistralForCausalLM'>
--> final self.generation_config: GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2
}

--> final self.generation_config_dict: {'temperature': 1.0, 'top_k': -1, 'top_p': 1.0, 'max_new_tokens': 256}
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 0it [00:00, ?it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/mistral/modeling_mistral.py", line 1158, in forward
    outputs = self.model(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/mistral.py", line 452, in forward
    layer_outputs = decoder_layer(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/mistral/modeling_mistral.py", line 770, in forward
    hidden_states = self.mlp(hidden_states)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/mistral/modeling_mistral.py", line 179, in forward
    return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 103, in forward
    return F.linear(input, self.weight, self.bias)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/functional.py", line 1848, in linear
    return torch._C._nn.linear(input, weight, bias)
RuntimeError: CUDA out of memory. Tried to allocate 112.00 MiB (GPU 0; 39.39 GiB total capacity; 37.27 GiB already allocated; 38.81 MiB free; 37.58 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-09-29 19:39:48.540 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-09-29 19:40:00.833 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/meta-mathMetaMath-Mistral-7B_vfl_(2,)
2024-09-29 19:40:35.284 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/meta-mathMetaMath-Mistral-7B_vfl_(2,)
================= iter seed  63  =================
communication_protocol: FedSGD
load model_architect: CLM
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 2
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 0
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: AdversarialTraining
Defense_Config: {'party': [0], 'position': 'head', 'adversarial_model': 'Mapping_MLP3_noflatten', 'adversarial_model_lr': 0.001, 'imagined_adversary': 'ImaginedAdversary_MLP3_noflatten', 'imagined_adversary_lr': 0.001, 'lambda': 0.1, 'seq_length': 512, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/GMS8K/2-slice/2_0/AdversarialTraining_0.1,finetuned_model=meta-mathMetaMath-Mistral-7B.txt
=================================

===== iter 63 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
default tokenizer.pad_token: [PAD]
pad_token: [PAD]   pad_id: 32000
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
trainable params: 0 || all params: 567300096 || trainable%: 0.0
model slices: dict_keys([0])
model partition: 0head-2/1tail-30
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
7473 train examples
1319 test examples
X: <class 'numpy.ndarray'> 7473 1319
y <class 'numpy.ndarray'> 7473 1319
Passive Party 0: init Adversarial Trainining Defense
Adversarial_MLP init: 512 4096
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 32000
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:09<00:47,  9.47s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:18<00:35,  8.97s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:30<00:31, 10.42s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:41<00:21, 10.74s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:49<00:09,  9.82s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:53<00:00,  7.91s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:53<00:00,  8.98s/it]
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=0
trainable params: 131076096 || all params: 6805516288 || trainable%: 1.926027217525337
model slices: dict_keys([1])
model partition: 0head-2/1tail-30
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.mistral.modeling_mistral.MistralForCausalLM'>
--> final self.generation_config: GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2
}

--> final self.generation_config_dict: {'temperature': 1.0, 'top_k': -1, 'top_p': 1.0, 'max_new_tokens': 256}
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 0it [00:00, ?it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/mistral/modeling_mistral.py", line 1158, in forward
    outputs = self.model(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/mistral.py", line 452, in forward
    layer_outputs = decoder_layer(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/mistral/modeling_mistral.py", line 770, in forward
    hidden_states = self.mlp(hidden_states)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/mistral/modeling_mistral.py", line 179, in forward
    return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 103, in forward
    return F.linear(input, self.weight, self.bias)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/functional.py", line 1848, in linear
    return torch._C._nn.linear(input, weight, bias)
RuntimeError: CUDA out of memory. Tried to allocate 112.00 MiB (GPU 0; 39.39 GiB total capacity; 37.27 GiB already allocated; 38.81 MiB free; 37.58 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-09-29 19:41:51.500 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-09-29 19:42:03.905 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/meta-mathMetaMath-Mistral-7B_vfl_(2,)
2024-09-29 19:42:28.310 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/meta-mathMetaMath-Mistral-7B_vfl_(2,)
================= iter seed  64  =================
communication_protocol: FedSGD
load model_architect: CLM
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 2
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 0
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: AdversarialTraining
Defense_Config: {'party': [0], 'position': 'head', 'adversarial_model': 'Mapping_MLP3_noflatten', 'adversarial_model_lr': 0.001, 'imagined_adversary': 'ImaginedAdversary_MLP3_noflatten', 'imagined_adversary_lr': 0.001, 'lambda': 0.001, 'seq_length': 512, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/GMS8K/2-slice/2_0/AdversarialTraining_0.001,finetuned_model=meta-mathMetaMath-Mistral-7B.txt
=================================

===== iter 64 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
default tokenizer.pad_token: [PAD]
pad_token: [PAD]   pad_id: 32000
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
trainable params: 0 || all params: 567300096 || trainable%: 0.0
model slices: dict_keys([0])
model partition: 0head-2/1tail-30
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
7473 train examples
1319 test examples
X: <class 'numpy.ndarray'> 7473 1319
y <class 'numpy.ndarray'> 7473 1319
Passive Party 0: init Adversarial Trainining Defense
Adversarial_MLP init: 512 4096
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 32000
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:13<01:07, 13.52s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:21<00:42, 10.52s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:32<00:31, 10.43s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:44<00:22, 11.33s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:52<00:10, 10.11s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:56<00:00,  7.88s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:56<00:00,  9.41s/it]
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=0
trainable params: 131076096 || all params: 6805516288 || trainable%: 1.926027217525337
model slices: dict_keys([1])
model partition: 0head-2/1tail-30
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.mistral.modeling_mistral.MistralForCausalLM'>
--> final self.generation_config: GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2
}

--> final self.generation_config_dict: {'temperature': 1.0, 'top_k': -1, 'top_p': 1.0, 'max_new_tokens': 256}
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 0it [00:00, ?it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/mistral/modeling_mistral.py", line 1158, in forward
    outputs = self.model(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/mistral.py", line 452, in forward
    layer_outputs = decoder_layer(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/mistral/modeling_mistral.py", line 770, in forward
    hidden_states = self.mlp(hidden_states)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/mistral/modeling_mistral.py", line 179, in forward
    return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 103, in forward
    return F.linear(input, self.weight, self.bias)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/functional.py", line 1848, in linear
    return torch._C._nn.linear(input, weight, bias)
RuntimeError: CUDA out of memory. Tried to allocate 112.00 MiB (GPU 0; 39.39 GiB total capacity; 37.27 GiB already allocated; 38.81 MiB free; 37.58 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-09-29 19:43:51.007 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-09-29 19:44:03.233 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/meta-mathMetaMath-Mistral-7B_vfl_(2,)
2024-09-29 19:44:42.124 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/meta-mathMetaMath-Mistral-7B_vfl_(2,)
================= iter seed  64  =================
communication_protocol: FedSGD
load model_architect: CLM
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 2
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 0
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: AdversarialTraining
Defense_Config: {'party': [0], 'position': 'head', 'adversarial_model': 'Mapping_MLP3_noflatten', 'adversarial_model_lr': 0.001, 'imagined_adversary': 'ImaginedAdversary_MLP3_noflatten', 'imagined_adversary_lr': 0.001, 'lambda': 0.01, 'seq_length': 512, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/GMS8K/2-slice/2_0/AdversarialTraining_0.01,finetuned_model=meta-mathMetaMath-Mistral-7B.txt
=================================

===== iter 64 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
default tokenizer.pad_token: [PAD]
pad_token: [PAD]   pad_id: 32000
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
trainable params: 0 || all params: 567300096 || trainable%: 0.0
model slices: dict_keys([0])
model partition: 0head-2/1tail-30
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
7473 train examples
1319 test examples
X: <class 'numpy.ndarray'> 7473 1319
y <class 'numpy.ndarray'> 7473 1319
Passive Party 0: init Adversarial Trainining Defense
Adversarial_MLP init: 512 4096
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 32000
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:11<00:55, 11.03s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:19<00:37,  9.33s/it]slurmstepd: error: *** JOB 68016 ON air-node-04 CANCELLED AT 2024-09-29T19:45:06 ***

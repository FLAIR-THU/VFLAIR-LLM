/bin/bash: /home/DAIR/guzx/.conda/envs/py38/lib/libtinfo.so.6: no version information available (required by /bin/bash)
2024-10-01 00:46:56.834 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 00:47:20.292 | WARNING  | models.llm_models.base:from_pretrained:99 - ValueError('Not found required vfl model in /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification')
Try to load from raw model
2024-10-01 00:55:45.533 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(4,)
2024-10-01 00:55:45.535 | WARNING  | models.llm_models.base:from_pretrained:99 - ValueError('Not found required vfl model in /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(4,)')
Try to load from raw model
================= iter seed  60  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 4
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 0.5, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/4_0/MID_Passive_MIDModel_Linear_head_0.5,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 60 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 51016704 || trainable%: 0.7767808755344132
model slices: dict_keys([0])
model partition: 0head-4/1tail-8
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 79962629 || trainable%: 29.088154417734316
model slices: dict_keys([1])
model partition: 0head-4/1tail-8
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.60it/s]Epoch 0/49: 1it [00:00,  1.09it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.39 GiB total capacity; 37.05 GiB already allocated; 308.81 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 00:57:39.727 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 00:57:48.487 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(4,)
2024-10-01 01:05:59.187 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(4,)
================= iter seed  60  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 4
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 0.1, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/4_0/MID_Passive_MIDModel_Linear_head_0.1,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 60 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 51016704 || trainable%: 0.7767808755344132
model slices: dict_keys([0])
model partition: 0head-4/1tail-8
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 79962629 || trainable%: 29.088154417734316
model slices: dict_keys([1])
model partition: 0head-4/1tail-8
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.70it/s]Epoch 0/49: 1it [00:00,  1.14it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.39 GiB total capacity; 37.05 GiB already allocated; 308.81 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 01:07:47.105 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 01:07:57.801 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(4,)
2024-10-01 01:15:53.604 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(4,)
================= iter seed  60  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 4
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 0.01, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/4_0/MID_Passive_MIDModel_Linear_head_0.01,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 60 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 51016704 || trainable%: 0.7767808755344132
model slices: dict_keys([0])
model partition: 0head-4/1tail-8
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 79962629 || trainable%: 29.088154417734316
model slices: dict_keys([1])
model partition: 0head-4/1tail-8
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.62it/s]Epoch 0/49: 1it [00:00,  1.12it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.39 GiB total capacity; 37.05 GiB already allocated; 308.81 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 01:17:46.835 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 01:17:55.391 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(4,)
2024-10-01 01:25:57.150 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(4,)
================= iter seed  60  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 4
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 0.001, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/4_0/MID_Passive_MIDModel_Linear_head_0.001,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 60 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 51016704 || trainable%: 0.7767808755344132
model slices: dict_keys([0])
model partition: 0head-4/1tail-8
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 79962629 || trainable%: 29.088154417734316
model slices: dict_keys([1])
model partition: 0head-4/1tail-8
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.68it/s]Epoch 0/49: 1it [00:00,  1.05it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.39 GiB total capacity; 37.05 GiB already allocated; 308.81 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 01:27:46.154 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 01:27:55.393 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(4,)
2024-10-01 01:35:53.641 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(4,)
================= iter seed  60  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 4
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 0.0001, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/4_0/MID_Passive_MIDModel_Linear_head_0.0001,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 60 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 51016704 || trainable%: 0.7767808755344132
model slices: dict_keys([0])
model partition: 0head-4/1tail-8
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 79962629 || trainable%: 29.088154417734316
model slices: dict_keys([1])
model partition: 0head-4/1tail-8
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.77it/s]Epoch 0/49: 1it [00:00,  1.17it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.39 GiB total capacity; 37.05 GiB already allocated; 308.81 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 01:37:16.945 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 01:37:26.356 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(4,)
2024-10-01 01:45:42.238 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(4,)
================= iter seed  60  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 4
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 1e-05, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/4_0/MID_Passive_MIDModel_Linear_head_1e-05,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 60 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 51016704 || trainable%: 0.7767808755344132
model slices: dict_keys([0])
model partition: 0head-4/1tail-8
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 79962629 || trainable%: 29.088154417734316
model slices: dict_keys([1])
model partition: 0head-4/1tail-8
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.45it/s]Epoch 0/49: 1it [00:01,  1.20s/it]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.39 GiB total capacity; 37.05 GiB already allocated; 308.81 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 01:47:25.158 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 01:47:33.894 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(4,)
2024-10-01 01:55:27.867 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(4,)
================= iter seed  60  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 4
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 0.05, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/4_0/MID_Passive_MIDModel_Linear_head_0.05,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 60 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 51016704 || trainable%: 0.7767808755344132
model slices: dict_keys([0])
model partition: 0head-4/1tail-8
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 79962629 || trainable%: 29.088154417734316
model slices: dict_keys([1])
model partition: 0head-4/1tail-8
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.67it/s]Epoch 0/49: 1it [00:00,  1.14it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.39 GiB total capacity; 37.05 GiB already allocated; 308.81 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 01:57:25.078 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 01:57:33.219 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(4,)
2024-10-01 02:05:35.379 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(4,)
================= iter seed  60  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 4
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 0.005, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/4_0/MID_Passive_MIDModel_Linear_head_0.005,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 60 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 51016704 || trainable%: 0.7767808755344132
model slices: dict_keys([0])
model partition: 0head-4/1tail-8
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 79962629 || trainable%: 29.088154417734316
model slices: dict_keys([1])
model partition: 0head-4/1tail-8
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.69it/s]Epoch 0/49: 1it [00:00,  1.13it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.39 GiB total capacity; 37.05 GiB already allocated; 308.81 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 02:07:25.956 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 02:07:35.323 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(4,)
2024-10-01 02:15:31.177 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(4,)
================= iter seed  61  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 4
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 0.5, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/4_0/MID_Passive_MIDModel_Linear_head_0.5,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 61 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 51016704 || trainable%: 0.7767808755344132
model slices: dict_keys([0])
model partition: 0head-4/1tail-8
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 79962629 || trainable%: 29.088154417734316
model slices: dict_keys([1])
model partition: 0head-4/1tail-8
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.64it/s]Epoch 0/49: 1it [00:00,  1.11it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.39 GiB total capacity; 37.05 GiB already allocated; 308.81 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 02:17:12.259 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 02:17:18.425 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(4,)
2024-10-01 02:25:24.900 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(4,)
================= iter seed  61  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 4
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 0.1, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/4_0/MID_Passive_MIDModel_Linear_head_0.1,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 61 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 51016704 || trainable%: 0.7767808755344132
model slices: dict_keys([0])
model partition: 0head-4/1tail-8
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 79962629 || trainable%: 29.088154417734316
model slices: dict_keys([1])
model partition: 0head-4/1tail-8
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.08it/s]Epoch 0/49: 1it [00:01,  1.42s/it]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.39 GiB total capacity; 37.05 GiB already allocated; 308.81 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 02:27:05.487 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 02:27:12.144 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(4,)
2024-10-01 02:35:26.093 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(4,)
================= iter seed  61  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 4
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 0.01, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/4_0/MID_Passive_MIDModel_Linear_head_0.01,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 61 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 51016704 || trainable%: 0.7767808755344132
model slices: dict_keys([0])
model partition: 0head-4/1tail-8
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 79962629 || trainable%: 29.088154417734316
model slices: dict_keys([1])
model partition: 0head-4/1tail-8
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.62it/s]Epoch 0/49: 1it [00:00,  1.11it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.39 GiB total capacity; 37.05 GiB already allocated; 308.81 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 02:37:14.506 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 02:37:18.628 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(4,)
2024-10-01 02:45:17.825 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(4,)
================= iter seed  61  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 4
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 0.001, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/4_0/MID_Passive_MIDModel_Linear_head_0.001,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 61 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 51016704 || trainable%: 0.7767808755344132
model slices: dict_keys([0])
model partition: 0head-4/1tail-8
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 79962629 || trainable%: 29.088154417734316
model slices: dict_keys([1])
model partition: 0head-4/1tail-8
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.83it/s]Epoch 0/49: 1it [00:00,  1.24it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.39 GiB total capacity; 37.05 GiB already allocated; 308.81 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 02:47:13.246 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 02:47:17.638 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(4,)
2024-10-01 02:55:12.414 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(4,)
================= iter seed  61  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 4
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 0.0001, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/4_0/MID_Passive_MIDModel_Linear_head_0.0001,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 61 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 51016704 || trainable%: 0.7767808755344132
model slices: dict_keys([0])
model partition: 0head-4/1tail-8
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 79962629 || trainable%: 29.088154417734316
model slices: dict_keys([1])
model partition: 0head-4/1tail-8
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.68it/s]Epoch 0/49: 1it [00:00,  1.10it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.39 GiB total capacity; 37.05 GiB already allocated; 308.81 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 02:57:00.081 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 02:57:04.167 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(4,)
2024-10-01 03:04:58.065 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(4,)
================= iter seed  61  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 4
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 1e-05, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/4_0/MID_Passive_MIDModel_Linear_head_1e-05,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 61 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 51016704 || trainable%: 0.7767808755344132
model slices: dict_keys([0])
model partition: 0head-4/1tail-8
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 79962629 || trainable%: 29.088154417734316
model slices: dict_keys([1])
model partition: 0head-4/1tail-8
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.18it/s]Epoch 0/49: 1it [00:01,  1.16s/it]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.39 GiB total capacity; 37.05 GiB already allocated; 308.81 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 03:06:43.105 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 03:06:51.837 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(4,)
2024-10-01 03:14:55.713 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(4,)
================= iter seed  61  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 4
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 0.05, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/4_0/MID_Passive_MIDModel_Linear_head_0.05,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 61 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 51016704 || trainable%: 0.7767808755344132
model slices: dict_keys([0])
model partition: 0head-4/1tail-8
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 79962629 || trainable%: 29.088154417734316
model slices: dict_keys([1])
model partition: 0head-4/1tail-8
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.59it/s]Epoch 0/49: 1it [00:01,  1.03s/it]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.39 GiB total capacity; 37.05 GiB already allocated; 308.81 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 03:16:46.146 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 03:16:53.437 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(4,)
2024-10-01 03:24:50.014 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(4,)
================= iter seed  61  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 4
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 0.005, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/4_0/MID_Passive_MIDModel_Linear_head_0.005,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 61 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 51016704 || trainable%: 0.7767808755344132
model slices: dict_keys([0])
model partition: 0head-4/1tail-8
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 79962629 || trainable%: 29.088154417734316
model slices: dict_keys([1])
model partition: 0head-4/1tail-8
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.73it/s]Epoch 0/49: 1it [00:00,  1.22it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.39 GiB total capacity; 37.05 GiB already allocated; 308.81 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 03:26:51.292 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 03:26:58.968 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(4,)
2024-10-01 03:35:02.602 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(4,)
================= iter seed  62  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 4
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 0.5, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/4_0/MID_Passive_MIDModel_Linear_head_0.5,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 62 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 51016704 || trainable%: 0.7767808755344132
model slices: dict_keys([0])
model partition: 0head-4/1tail-8
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 79962629 || trainable%: 29.088154417734316
model slices: dict_keys([1])
model partition: 0head-4/1tail-8
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.90it/s]Epoch 0/49: 1it [00:00,  1.26it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.39 GiB total capacity; 37.05 GiB already allocated; 308.81 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 03:36:59.821 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 03:37:08.488 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(4,)
2024-10-01 03:44:59.069 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(4,)
================= iter seed  62  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 4
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 0.1, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/4_0/MID_Passive_MIDModel_Linear_head_0.1,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 62 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 51016704 || trainable%: 0.7767808755344132
model slices: dict_keys([0])
model partition: 0head-4/1tail-8
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 79962629 || trainable%: 29.088154417734316
model slices: dict_keys([1])
model partition: 0head-4/1tail-8
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.86it/s]Epoch 0/49: 1it [00:00,  1.11it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.39 GiB total capacity; 37.05 GiB already allocated; 308.81 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 03:46:47.364 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 03:46:54.624 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(4,)
2024-10-01 03:54:58.380 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(4,)
================= iter seed  62  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 4
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 0.01, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/4_0/MID_Passive_MIDModel_Linear_head_0.01,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 62 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 51016704 || trainable%: 0.7767808755344132
model slices: dict_keys([0])
model partition: 0head-4/1tail-8
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 79962629 || trainable%: 29.088154417734316
model slices: dict_keys([1])
model partition: 0head-4/1tail-8
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.80it/s]Epoch 0/49: 1it [00:00,  1.19it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.39 GiB total capacity; 37.05 GiB already allocated; 308.81 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 03:56:43.865 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 03:56:52.114 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(4,)
2024-10-01 04:05:01.498 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(4,)
================= iter seed  62  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 4
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 0.001, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/4_0/MID_Passive_MIDModel_Linear_head_0.001,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 62 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 51016704 || trainable%: 0.7767808755344132
model slices: dict_keys([0])
model partition: 0head-4/1tail-8
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 79962629 || trainable%: 29.088154417734316
model slices: dict_keys([1])
model partition: 0head-4/1tail-8
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.79it/s]Epoch 0/49: 1it [00:00,  1.24it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.39 GiB total capacity; 37.05 GiB already allocated; 308.81 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 04:06:49.328 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 04:06:54.902 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(4,)
2024-10-01 04:14:51.727 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(4,)
================= iter seed  62  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 4
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 0.0001, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/4_0/MID_Passive_MIDModel_Linear_head_0.0001,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 62 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 51016704 || trainable%: 0.7767808755344132
model slices: dict_keys([0])
model partition: 0head-4/1tail-8
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 79962629 || trainable%: 29.088154417734316
model slices: dict_keys([1])
model partition: 0head-4/1tail-8
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.85it/s]Epoch 0/49: 1it [00:00,  1.26it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.39 GiB total capacity; 37.05 GiB already allocated; 308.81 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 04:16:45.803 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 04:16:52.640 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(4,)
2024-10-01 04:24:56.252 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(4,)
================= iter seed  62  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 4
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 1e-05, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/4_0/MID_Passive_MIDModel_Linear_head_1e-05,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 62 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 51016704 || trainable%: 0.7767808755344132
model slices: dict_keys([0])
model partition: 0head-4/1tail-8
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 79962629 || trainable%: 29.088154417734316
model slices: dict_keys([1])
model partition: 0head-4/1tail-8
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.11it/s]Epoch 0/49: 1it [00:01,  1.19s/it]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.39 GiB total capacity; 37.05 GiB already allocated; 308.81 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 04:26:49.158 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 04:26:56.644 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(4,)
2024-10-01 04:34:49.666 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(4,)
================= iter seed  62  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 4
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 0.05, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/4_0/MID_Passive_MIDModel_Linear_head_0.05,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 62 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 51016704 || trainable%: 0.7767808755344132
model slices: dict_keys([0])
model partition: 0head-4/1tail-8
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 79962629 || trainable%: 29.088154417734316
model slices: dict_keys([1])
model partition: 0head-4/1tail-8
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.78it/s]Epoch 0/49: 1it [00:00,  1.19it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.39 GiB total capacity; 37.05 GiB already allocated; 308.81 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 04:36:34.101 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 04:36:41.361 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(4,)
2024-10-01 04:44:37.222 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(4,)
================= iter seed  62  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 4
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 0.005, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/4_0/MID_Passive_MIDModel_Linear_head_0.005,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 62 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 51016704 || trainable%: 0.7767808755344132
model slices: dict_keys([0])
model partition: 0head-4/1tail-8
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 79962629 || trainable%: 29.088154417734316
model slices: dict_keys([1])
model partition: 0head-4/1tail-8
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.49it/s]Epoch 0/49: 1it [00:01,  1.31s/it]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.39 GiB total capacity; 37.05 GiB already allocated; 308.81 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 04:46:32.312 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 04:46:39.714 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(4,)
2024-10-01 04:54:48.799 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(4,)
================= iter seed  63  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 4
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 0.5, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/4_0/MID_Passive_MIDModel_Linear_head_0.5,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 63 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 51016704 || trainable%: 0.7767808755344132
model slices: dict_keys([0])
model partition: 0head-4/1tail-8
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 79962629 || trainable%: 29.088154417734316
model slices: dict_keys([1])
model partition: 0head-4/1tail-8
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.78it/s]Epoch 0/49: 1it [00:00,  1.21it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.39 GiB total capacity; 37.05 GiB already allocated; 308.81 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 04:56:39.917 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 04:56:47.268 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(4,)
2024-10-01 05:04:43.889 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(4,)
================= iter seed  63  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 4
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 0.1, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/4_0/MID_Passive_MIDModel_Linear_head_0.1,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 63 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 51016704 || trainable%: 0.7767808755344132
model slices: dict_keys([0])
model partition: 0head-4/1tail-8
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 79962629 || trainable%: 29.088154417734316
model slices: dict_keys([1])
model partition: 0head-4/1tail-8
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.69it/s]Epoch 0/49: 1it [00:00,  1.10it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.39 GiB total capacity; 37.05 GiB already allocated; 308.81 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 05:06:42.736 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 05:06:49.832 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(4,)
2024-10-01 05:14:54.398 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(4,)
================= iter seed  63  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 4
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 0.01, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/4_0/MID_Passive_MIDModel_Linear_head_0.01,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 63 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 51016704 || trainable%: 0.7767808755344132
model slices: dict_keys([0])
model partition: 0head-4/1tail-8
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 79962629 || trainable%: 29.088154417734316
model slices: dict_keys([1])
model partition: 0head-4/1tail-8
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.86it/s]Epoch 0/49: 1it [00:00,  1.18it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.39 GiB total capacity; 37.05 GiB already allocated; 308.81 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 05:16:49.490 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 05:16:56.193 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(4,)
2024-10-01 05:24:48.859 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(4,)
================= iter seed  63  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 4
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 0.001, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/4_0/MID_Passive_MIDModel_Linear_head_0.001,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 63 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 51016704 || trainable%: 0.7767808755344132
model slices: dict_keys([0])
model partition: 0head-4/1tail-8
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 79962629 || trainable%: 29.088154417734316
model slices: dict_keys([1])
model partition: 0head-4/1tail-8
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.66it/s]Epoch 0/49: 1it [00:00,  1.12it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.39 GiB total capacity; 37.05 GiB already allocated; 308.81 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 05:26:26.111 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/_distutils_hack/__init__.py:18: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.
  warnings.warn(
/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
2024-10-01 05:26:34.406 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(4,)
2024-10-01 05:34:38.259 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(4,)
================= iter seed  63  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 4
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 0.0001, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/4_0/MID_Passive_MIDModel_Linear_head_0.0001,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 63 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 51016704 || trainable%: 0.7767808755344132
model slices: dict_keys([0])
model partition: 0head-4/1tail-8
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 79962629 || trainable%: 29.088154417734316
model slices: dict_keys([1])
model partition: 0head-4/1tail-8
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:01,  1.04s/it]Epoch 0/49: 1it [00:01,  1.37s/it]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.39 GiB total capacity; 37.05 GiB already allocated; 308.81 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 05:36:23.153 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 05:36:30.116 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(4,)
2024-10-01 05:44:44.405 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(4,)
================= iter seed  63  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 4
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 1e-05, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/4_0/MID_Passive_MIDModel_Linear_head_1e-05,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 63 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 51016704 || trainable%: 0.7767808755344132
model slices: dict_keys([0])
model partition: 0head-4/1tail-8
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 79962629 || trainable%: 29.088154417734316
model slices: dict_keys([1])
model partition: 0head-4/1tail-8
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.75it/s]Epoch 0/49: 1it [00:00,  1.23it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.39 GiB total capacity; 37.05 GiB already allocated; 308.81 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 05:46:26.398 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 05:46:33.382 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(4,)
2024-10-01 05:54:29.845 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(4,)
================= iter seed  63  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 4
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 0.05, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/4_0/MID_Passive_MIDModel_Linear_head_0.05,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 63 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 51016704 || trainable%: 0.7767808755344132
model slices: dict_keys([0])
model partition: 0head-4/1tail-8
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 79962629 || trainable%: 29.088154417734316
model slices: dict_keys([1])
model partition: 0head-4/1tail-8
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.73it/s]Epoch 0/49: 1it [00:00,  1.19it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.39 GiB total capacity; 37.05 GiB already allocated; 308.81 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 05:56:20.853 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 05:56:28.227 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(4,)
2024-10-01 06:04:43.653 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(4,)
================= iter seed  63  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 4
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 0.005, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/4_0/MID_Passive_MIDModel_Linear_head_0.005,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 63 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 51016704 || trainable%: 0.7767808755344132
model slices: dict_keys([0])
model partition: 0head-4/1tail-8
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 79962629 || trainable%: 29.088154417734316
model slices: dict_keys([1])
model partition: 0head-4/1tail-8
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.82it/s]Epoch 0/49: 1it [00:00,  1.19it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.39 GiB total capacity; 37.05 GiB already allocated; 308.81 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 06:06:32.691 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 06:06:39.473 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(4,)
2024-10-01 06:14:59.467 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(4,)
================= iter seed  64  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 4
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 0.5, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/4_0/MID_Passive_MIDModel_Linear_head_0.5,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 64 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 51016704 || trainable%: 0.7767808755344132
model slices: dict_keys([0])
model partition: 0head-4/1tail-8
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 79962629 || trainable%: 29.088154417734316
model slices: dict_keys([1])
model partition: 0head-4/1tail-8
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.33it/s]Epoch 0/49: 1it [00:01,  1.14s/it]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.39 GiB total capacity; 37.05 GiB already allocated; 308.81 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 06:16:54.211 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 06:17:01.730 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(4,)
2024-10-01 06:25:29.254 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(4,)
================= iter seed  64  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 4
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 0.1, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/4_0/MID_Passive_MIDModel_Linear_head_0.1,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 64 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 51016704 || trainable%: 0.7767808755344132
model slices: dict_keys([0])
model partition: 0head-4/1tail-8
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 79962629 || trainable%: 29.088154417734316
model slices: dict_keys([1])
model partition: 0head-4/1tail-8
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.62it/s]Epoch 0/49: 1it [00:00,  1.01it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.39 GiB total capacity; 37.05 GiB already allocated; 308.81 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 06:27:25.337 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 06:27:32.330 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(4,)
2024-10-01 06:36:01.908 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(4,)
================= iter seed  64  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 4
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 0.01, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/4_0/MID_Passive_MIDModel_Linear_head_0.01,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 64 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 51016704 || trainable%: 0.7767808755344132
model slices: dict_keys([0])
model partition: 0head-4/1tail-8
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 79962629 || trainable%: 29.088154417734316
model slices: dict_keys([1])
model partition: 0head-4/1tail-8
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.30it/s]Epoch 0/49: 1it [00:01,  1.19s/it]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.39 GiB total capacity; 37.05 GiB already allocated; 308.81 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 06:37:48.413 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 06:37:54.996 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(4,)
2024-10-01 06:46:25.962 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(4,)
================= iter seed  64  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 4
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 0.001, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/4_0/MID_Passive_MIDModel_Linear_head_0.001,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 64 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 51016704 || trainable%: 0.7767808755344132
model slices: dict_keys([0])
model partition: 0head-4/1tail-8
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 79962629 || trainable%: 29.088154417734316
model slices: dict_keys([1])
model partition: 0head-4/1tail-8
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.46it/s]Epoch 0/49: 1it [00:00,  1.04it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.39 GiB total capacity; 37.05 GiB already allocated; 308.81 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 06:48:06.644 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 06:48:14.049 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(4,)
2024-10-01 06:56:51.710 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(4,)
================= iter seed  64  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 4
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 0.0001, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/4_0/MID_Passive_MIDModel_Linear_head_0.0001,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 64 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 51016704 || trainable%: 0.7767808755344132
model slices: dict_keys([0])
model partition: 0head-4/1tail-8
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 79962629 || trainable%: 29.088154417734316
model slices: dict_keys([1])
model partition: 0head-4/1tail-8
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.39it/s]Epoch 0/49: 1it [00:01,  1.04s/it]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.39 GiB total capacity; 37.05 GiB already allocated; 308.81 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 06:58:36.715 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 06:58:44.089 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(4,)
2024-10-01 07:07:25.383 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(4,)
================= iter seed  64  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 4
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 1e-05, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/4_0/MID_Passive_MIDModel_Linear_head_1e-05,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 64 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 51016704 || trainable%: 0.7767808755344132
model slices: dict_keys([0])
model partition: 0head-4/1tail-8
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 79962629 || trainable%: 29.088154417734316
model slices: dict_keys([1])
model partition: 0head-4/1tail-8
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.06it/s]Epoch 0/49: 1it [00:01,  1.40s/it]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.39 GiB total capacity; 37.05 GiB already allocated; 308.81 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 07:09:13.162 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 07:09:19.892 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(4,)
2024-10-01 07:17:55.679 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(4,)
================= iter seed  64  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 4
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 0.05, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/4_0/MID_Passive_MIDModel_Linear_head_0.05,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 64 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 51016704 || trainable%: 0.7767808755344132
model slices: dict_keys([0])
model partition: 0head-4/1tail-8
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 79962629 || trainable%: 29.088154417734316
model slices: dict_keys([1])
model partition: 0head-4/1tail-8
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.68it/s]Epoch 0/49: 1it [00:00,  1.18it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.39 GiB total capacity; 37.05 GiB already allocated; 308.81 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 07:19:40.252 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 07:19:47.381 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(4,)
2024-10-01 07:28:24.121 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(4,)
================= iter seed  64  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 4
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 0.005, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/4_0/MID_Passive_MIDModel_Linear_head_0.005,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 64 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 51016704 || trainable%: 0.7767808755344132
model slices: dict_keys([0])
model partition: 0head-4/1tail-8
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 79962629 || trainable%: 29.088154417734316
model slices: dict_keys([1])
model partition: 0head-4/1tail-8
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.75it/s]Epoch 0/49: 1it [00:00,  1.10it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.39 GiB total capacity; 37.05 GiB already allocated; 308.81 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 07:30:04.047 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 07:30:11.441 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(4,)
2024-10-01 07:38:45.933 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(4,)
================= iter seed  65  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 4
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 0.5, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/4_0/MID_Passive_MIDModel_Linear_head_0.5,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 65 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 51016704 || trainable%: 0.7767808755344132
model slices: dict_keys([0])
model partition: 0head-4/1tail-8
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 79962629 || trainable%: 29.088154417734316
model slices: dict_keys([1])
model partition: 0head-4/1tail-8
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.23it/s]Epoch 0/49: 1it [00:01,  1.11s/it]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.39 GiB total capacity; 37.05 GiB already allocated; 308.81 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 07:40:28.897 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 07:40:36.873 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(4,)
2024-10-01 07:49:10.534 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(4,)
================= iter seed  65  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 4
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 0.1, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/4_0/MID_Passive_MIDModel_Linear_head_0.1,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 65 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 51016704 || trainable%: 0.7767808755344132
model slices: dict_keys([0])
model partition: 0head-4/1tail-8
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 79962629 || trainable%: 29.088154417734316
model slices: dict_keys([1])
model partition: 0head-4/1tail-8
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.44it/s]Epoch 0/49: 1it [00:01,  1.12s/it]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.39 GiB total capacity; 37.05 GiB already allocated; 308.81 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 07:50:53.718 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 07:51:01.866 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(4,)
2024-10-01 07:59:32.368 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(4,)
================= iter seed  65  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 4
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 0.01, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/4_0/MID_Passive_MIDModel_Linear_head_0.01,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 65 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 51016704 || trainable%: 0.7767808755344132
model slices: dict_keys([0])
model partition: 0head-4/1tail-8
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 79962629 || trainable%: 29.088154417734316
model slices: dict_keys([1])
model partition: 0head-4/1tail-8
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.40it/s]Epoch 0/49: 1it [00:01,  1.18s/it]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.39 GiB total capacity; 37.05 GiB already allocated; 308.81 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 08:01:18.875 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 08:01:25.711 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(4,)
2024-10-01 08:10:00.192 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(4,)
================= iter seed  65  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 4
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 0.001, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/4_0/MID_Passive_MIDModel_Linear_head_0.001,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 65 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 51016704 || trainable%: 0.7767808755344132
model slices: dict_keys([0])
model partition: 0head-4/1tail-8
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 79962629 || trainable%: 29.088154417734316
model slices: dict_keys([1])
model partition: 0head-4/1tail-8
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.26it/s]Epoch 0/49: 1it [00:01,  1.23s/it]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.39 GiB total capacity; 37.05 GiB already allocated; 308.81 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 08:11:42.229 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 08:11:48.521 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(4,)
2024-10-01 08:20:28.809 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(4,)
================= iter seed  65  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 4
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 0.0001, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/4_0/MID_Passive_MIDModel_Linear_head_0.0001,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 65 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 51016704 || trainable%: 0.7767808755344132
model slices: dict_keys([0])
model partition: 0head-4/1tail-8
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 79962629 || trainable%: 29.088154417734316
model slices: dict_keys([1])
model partition: 0head-4/1tail-8
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.18it/s]Epoch 0/49: 1it [00:01,  1.41s/it]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.39 GiB total capacity; 37.05 GiB already allocated; 308.81 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 08:22:12.505 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 08:22:19.633 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(4,)
2024-10-01 08:30:58.319 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(4,)
================= iter seed  65  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 4
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 1e-05, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/4_0/MID_Passive_MIDModel_Linear_head_1e-05,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 65 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 51016704 || trainable%: 0.7767808755344132
model slices: dict_keys([0])
model partition: 0head-4/1tail-8
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 79962629 || trainable%: 29.088154417734316
model slices: dict_keys([1])
model partition: 0head-4/1tail-8
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.27it/s]Epoch 0/49: 1it [00:01,  1.24s/it]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.39 GiB total capacity; 37.05 GiB already allocated; 308.81 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 08:32:45.111 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 08:32:52.128 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(4,)
2024-10-01 08:41:28.711 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(4,)
================= iter seed  65  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 4
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 0.05, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/4_0/MID_Passive_MIDModel_Linear_head_0.05,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 65 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 51016704 || trainable%: 0.7767808755344132
model slices: dict_keys([0])
model partition: 0head-4/1tail-8
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 79962629 || trainable%: 29.088154417734316
model slices: dict_keys([1])
model partition: 0head-4/1tail-8
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.54it/s]Epoch 0/49: 1it [00:01,  1.07s/it]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.39 GiB total capacity; 37.05 GiB already allocated; 308.81 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-01 08:43:14.792 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-01 08:43:21.934 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(4,)
2024-10-01 08:51:57.195 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(4,)
================= iter seed  65  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 4
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 0.005, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/4_0/MID_Passive_MIDModel_Linear_head_0.005,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 65 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 51016704 || trainable%: 0.7767808755344132
model slices: dict_keys([0])
model partition: 0head-4/1tail-8
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 79962629 || trainable%: 29.088154417734316
model slices: dict_keys([1])
model partition: 0head-4/1tail-8
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.58it/s]Epoch 0/49: 1it [00:00,  1.00it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.39 GiB total capacity; 37.05 GiB already allocated; 308.81 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

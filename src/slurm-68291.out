/bin/bash: /home/DAIR/guzx/.conda/envs/py38/lib/libtinfo.so.6: no version information available (required by /bin/bash)
2024-10-10 23:55:12.224 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-10 23:55:21.377 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(2,)
2024-10-11 00:02:58.957 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(2,)
================= iter seed  62  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 2
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 0.001, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/2_0/MID_Passive_MIDModel_Linear_head_0.001,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 62 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 36840960 || trainable%: 1.0756722951844904
model slices: dict_keys([0])
model partition: 0head-2/1tail-10
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 94138373 || trainable%: 24.707940299754277
model slices: dict_keys([1])
model partition: 0head-2/1tail-10
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.73it/s]Epoch 0/49: 1it [00:00,  1.16it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.41 GiB total capacity; 37.05 GiB already allocated; 238.50 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-11 00:04:04.298 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-11 00:04:12.552 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(2,)
2024-10-11 00:11:47.789 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(2,)
================= iter seed  62  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 2
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 0.001, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/2_0/MID_Passive_MIDModel_Linear_head_0.001,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 62 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 36840960 || trainable%: 1.0756722951844904
model slices: dict_keys([0])
model partition: 0head-2/1tail-10
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 94138373 || trainable%: 24.707940299754277
model slices: dict_keys([1])
model partition: 0head-2/1tail-10
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.82it/s]Epoch 0/49: 1it [00:00,  1.27it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.41 GiB total capacity; 37.05 GiB already allocated; 238.50 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-11 00:12:51.428 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-11 00:12:59.739 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(2,)
2024-10-11 00:20:31.862 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(2,)
================= iter seed  62  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 2
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 0.0001, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/2_0/MID_Passive_MIDModel_Linear_head_0.0001,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 62 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 36840960 || trainable%: 1.0756722951844904
model slices: dict_keys([0])
model partition: 0head-2/1tail-10
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 94138373 || trainable%: 24.707940299754277
model slices: dict_keys([1])
model partition: 0head-2/1tail-10
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.74it/s]Epoch 0/49: 1it [00:00,  1.23it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.41 GiB total capacity; 37.05 GiB already allocated; 238.50 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-11 00:21:45.646 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-11 00:21:53.438 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(2,)
2024-10-11 00:29:36.738 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(2,)
================= iter seed  62  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 2
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 1e-05, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/2_0/MID_Passive_MIDModel_Linear_head_1e-05,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 62 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 36840960 || trainable%: 1.0756722951844904
model slices: dict_keys([0])
model partition: 0head-2/1tail-10
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 94138373 || trainable%: 24.707940299754277
model slices: dict_keys([1])
model partition: 0head-2/1tail-10
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.68it/s]Epoch 0/49: 1it [00:00,  1.19it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.41 GiB total capacity; 37.05 GiB already allocated; 238.50 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-11 00:30:52.656 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-11 00:31:00.254 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(2,)
2024-10-11 00:38:44.312 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(2,)
================= iter seed  62  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 2
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 0.05, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/2_0/MID_Passive_MIDModel_Linear_head_0.05,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 62 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 36840960 || trainable%: 1.0756722951844904
model slices: dict_keys([0])
model partition: 0head-2/1tail-10
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 94138373 || trainable%: 24.707940299754277
model slices: dict_keys([1])
model partition: 0head-2/1tail-10
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.78it/s]Epoch 0/49: 1it [00:00,  1.26it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.41 GiB total capacity; 37.05 GiB already allocated; 238.50 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-11 00:39:48.806 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-11 00:39:56.802 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(2,)
2024-10-11 00:47:27.276 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(2,)
================= iter seed  62  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 2
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 0.005, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/2_0/MID_Passive_MIDModel_Linear_head_0.005,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 62 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 36840960 || trainable%: 1.0756722951844904
model slices: dict_keys([0])
model partition: 0head-2/1tail-10
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 94138373 || trainable%: 24.707940299754277
model slices: dict_keys([1])
model partition: 0head-2/1tail-10
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.80it/s]Epoch 0/49: 1it [00:00,  1.29it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.41 GiB total capacity; 37.05 GiB already allocated; 238.50 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-11 00:48:29.620 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-11 00:48:36.998 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(2,)
2024-10-11 00:56:02.413 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(2,)
================= iter seed  63  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 2
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 0.5, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/2_0/MID_Passive_MIDModel_Linear_head_0.5,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 63 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 36840960 || trainable%: 1.0756722951844904
model slices: dict_keys([0])
model partition: 0head-2/1tail-10
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 94138373 || trainable%: 24.707940299754277
model slices: dict_keys([1])
model partition: 0head-2/1tail-10
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.84it/s]Epoch 0/49: 1it [00:00,  1.30it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.41 GiB total capacity; 37.05 GiB already allocated; 238.50 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-11 00:57:02.945 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-11 00:57:10.870 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(2,)
2024-10-11 01:04:33.203 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(2,)
================= iter seed  63  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 2
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 0.1, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/2_0/MID_Passive_MIDModel_Linear_head_0.1,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 63 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 36840960 || trainable%: 1.0756722951844904
model slices: dict_keys([0])
model partition: 0head-2/1tail-10
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 94138373 || trainable%: 24.707940299754277
model slices: dict_keys([1])
model partition: 0head-2/1tail-10
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.65it/s]Epoch 0/49: 1it [00:00,  1.21it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.41 GiB total capacity; 37.05 GiB already allocated; 238.50 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-11 01:05:37.354 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-11 01:05:45.446 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(2,)
2024-10-11 01:13:05.067 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(2,)
================= iter seed  63  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 2
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 0.01, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/2_0/MID_Passive_MIDModel_Linear_head_0.01,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 63 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 36840960 || trainable%: 1.0756722951844904
model slices: dict_keys([0])
model partition: 0head-2/1tail-10
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 94138373 || trainable%: 24.707940299754277
model slices: dict_keys([1])
model partition: 0head-2/1tail-10
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.87it/s]Epoch 0/49: 1it [00:00,  1.37it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.41 GiB total capacity; 37.05 GiB already allocated; 238.50 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-11 01:14:13.404 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-11 01:14:20.850 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(2,)
2024-10-11 01:21:34.888 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(2,)
================= iter seed  63  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 2
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 0.001, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/2_0/MID_Passive_MIDModel_Linear_head_0.001,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 63 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 36840960 || trainable%: 1.0756722951844904
model slices: dict_keys([0])
model partition: 0head-2/1tail-10
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 94138373 || trainable%: 24.707940299754277
model slices: dict_keys([1])
model partition: 0head-2/1tail-10
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.83it/s]Epoch 0/49: 1it [00:00,  1.32it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.41 GiB total capacity; 37.05 GiB already allocated; 238.50 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-11 01:22:43.682 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-11 01:22:50.663 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(2,)
2024-10-11 01:30:07.804 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(2,)
================= iter seed  63  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 2
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 0.0001, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/2_0/MID_Passive_MIDModel_Linear_head_0.0001,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 63 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 36840960 || trainable%: 1.0756722951844904
model slices: dict_keys([0])
model partition: 0head-2/1tail-10
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 94138373 || trainable%: 24.707940299754277
model slices: dict_keys([1])
model partition: 0head-2/1tail-10
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.80it/s]Epoch 0/49: 1it [00:00,  1.26it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.41 GiB total capacity; 37.05 GiB already allocated; 238.50 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-11 01:31:21.046 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-11 01:31:28.704 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(2,)
2024-10-11 01:39:08.907 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(2,)
================= iter seed  63  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 2
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 1e-05, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/2_0/MID_Passive_MIDModel_Linear_head_1e-05,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 63 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 36840960 || trainable%: 1.0756722951844904
model slices: dict_keys([0])
model partition: 0head-2/1tail-10
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 94138373 || trainable%: 24.707940299754277
model slices: dict_keys([1])
model partition: 0head-2/1tail-10
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.73it/s]Epoch 0/49: 1it [00:00,  1.26it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.41 GiB total capacity; 37.05 GiB already allocated; 238.50 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-11 01:40:20.156 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-11 01:40:28.750 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(2,)
2024-10-11 01:48:15.780 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(2,)
================= iter seed  63  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 2
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 0.05, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/2_0/MID_Passive_MIDModel_Linear_head_0.05,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 63 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 36840960 || trainable%: 1.0756722951844904
model slices: dict_keys([0])
model partition: 0head-2/1tail-10
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 94138373 || trainable%: 24.707940299754277
model slices: dict_keys([1])
model partition: 0head-2/1tail-10
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.76it/s]Epoch 0/49: 1it [00:00,  1.18it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.41 GiB total capacity; 37.05 GiB already allocated; 238.50 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-11 01:49:32.867 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-11 01:49:40.434 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(2,)
2024-10-11 01:57:09.778 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(2,)
================= iter seed  63  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 2
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 0.005, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/2_0/MID_Passive_MIDModel_Linear_head_0.005,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 63 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 36840960 || trainable%: 1.0756722951844904
model slices: dict_keys([0])
model partition: 0head-2/1tail-10
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 94138373 || trainable%: 24.707940299754277
model slices: dict_keys([1])
model partition: 0head-2/1tail-10
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.63it/s]Epoch 0/49: 1it [00:00,  1.19it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.41 GiB total capacity; 37.05 GiB already allocated; 238.50 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-11 01:58:20.047 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-11 01:58:28.976 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(2,)
2024-10-11 02:06:10.006 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(2,)
================= iter seed  64  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 2
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 0.5, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/2_0/MID_Passive_MIDModel_Linear_head_0.5,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 64 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 36840960 || trainable%: 1.0756722951844904
model slices: dict_keys([0])
model partition: 0head-2/1tail-10
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 94138373 || trainable%: 24.707940299754277
model slices: dict_keys([1])
model partition: 0head-2/1tail-10
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.82it/s]Epoch 0/49: 1it [00:00,  1.26it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.41 GiB total capacity; 37.05 GiB already allocated; 238.50 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-11 02:07:17.306 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-11 02:07:25.405 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(2,)
2024-10-11 02:14:58.056 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(2,)
================= iter seed  64  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 2
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 0.1, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/2_0/MID_Passive_MIDModel_Linear_head_0.1,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 64 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 36840960 || trainable%: 1.0756722951844904
model slices: dict_keys([0])
model partition: 0head-2/1tail-10
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 94138373 || trainable%: 24.707940299754277
model slices: dict_keys([1])
model partition: 0head-2/1tail-10
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.62it/s]Epoch 0/49: 1it [00:00,  1.16it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.41 GiB total capacity; 37.05 GiB already allocated; 238.50 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-11 02:16:06.215 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-11 02:16:14.136 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(2,)
2024-10-11 02:23:44.604 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(2,)
================= iter seed  64  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 2
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 0.01, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/2_0/MID_Passive_MIDModel_Linear_head_0.01,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 64 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 36840960 || trainable%: 1.0756722951844904
model slices: dict_keys([0])
model partition: 0head-2/1tail-10
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 94138373 || trainable%: 24.707940299754277
model slices: dict_keys([1])
model partition: 0head-2/1tail-10
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.82it/s]Epoch 0/49: 1it [00:00,  1.32it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.41 GiB total capacity; 37.05 GiB already allocated; 238.50 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-11 02:24:47.731 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-11 02:24:56.530 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(2,)
2024-10-11 02:32:17.634 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(2,)
================= iter seed  64  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 2
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 0.001, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/2_0/MID_Passive_MIDModel_Linear_head_0.001,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 64 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 36840960 || trainable%: 1.0756722951844904
model slices: dict_keys([0])
model partition: 0head-2/1tail-10
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 94138373 || trainable%: 24.707940299754277
model slices: dict_keys([1])
model partition: 0head-2/1tail-10
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.76it/s]Epoch 0/49: 1it [00:00,  1.25it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.41 GiB total capacity; 37.05 GiB already allocated; 238.50 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-11 02:33:26.010 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-11 02:33:35.147 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(2,)
2024-10-11 02:41:38.207 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(2,)
================= iter seed  64  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 2
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 0.0001, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/2_0/MID_Passive_MIDModel_Linear_head_0.0001,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 64 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 36840960 || trainable%: 1.0756722951844904
model slices: dict_keys([0])
model partition: 0head-2/1tail-10
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 94138373 || trainable%: 24.707940299754277
model slices: dict_keys([1])
model partition: 0head-2/1tail-10
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.61it/s]Epoch 0/49: 1it [00:00,  1.15it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.41 GiB total capacity; 37.05 GiB already allocated; 238.50 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-11 02:42:50.914 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-11 02:42:59.611 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(2,)
2024-10-11 02:51:06.415 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(2,)
================= iter seed  64  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 2
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 1e-05, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/2_0/MID_Passive_MIDModel_Linear_head_1e-05,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 64 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 36840960 || trainable%: 1.0756722951844904
model slices: dict_keys([0])
model partition: 0head-2/1tail-10
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 94138373 || trainable%: 24.707940299754277
model slices: dict_keys([1])
model partition: 0head-2/1tail-10
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.71it/s]Epoch 0/49: 1it [00:00,  1.24it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.41 GiB total capacity; 37.05 GiB already allocated; 238.50 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-11 02:52:25.763 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-11 02:52:33.598 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(2,)
2024-10-11 03:00:41.888 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(2,)
================= iter seed  64  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 2
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 0.05, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/2_0/MID_Passive_MIDModel_Linear_head_0.05,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 64 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 36840960 || trainable%: 1.0756722951844904
model slices: dict_keys([0])
model partition: 0head-2/1tail-10
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 94138373 || trainable%: 24.707940299754277
model slices: dict_keys([1])
model partition: 0head-2/1tail-10
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.85it/s]Epoch 0/49: 1it [00:00,  1.25it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.41 GiB total capacity; 37.05 GiB already allocated; 238.50 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-11 03:01:47.619 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-11 03:01:55.730 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(2,)
2024-10-11 03:09:56.666 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(2,)
================= iter seed  64  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 2
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 0.005, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/2_0/MID_Passive_MIDModel_Linear_head_0.005,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 64 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 36840960 || trainable%: 1.0756722951844904
model slices: dict_keys([0])
model partition: 0head-2/1tail-10
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 94138373 || trainable%: 24.707940299754277
model slices: dict_keys([1])
model partition: 0head-2/1tail-10
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.52it/s]Epoch 0/49: 1it [00:00,  1.15it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.41 GiB total capacity; 37.05 GiB already allocated; 238.50 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-11 03:11:12.938 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-11 03:11:20.918 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(2,)
2024-10-11 03:19:25.408 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(2,)
================= iter seed  65  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 2
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 0.5, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/2_0/MID_Passive_MIDModel_Linear_head_0.5,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 65 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 36840960 || trainable%: 1.0756722951844904
model slices: dict_keys([0])
model partition: 0head-2/1tail-10
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 94138373 || trainable%: 24.707940299754277
model slices: dict_keys([1])
model partition: 0head-2/1tail-10
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.77it/s]Epoch 0/49: 1it [00:00,  1.27it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.41 GiB total capacity; 37.05 GiB already allocated; 238.50 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-11 03:20:42.647 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-11 03:20:51.159 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(2,)
2024-10-11 03:28:57.565 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(2,)
================= iter seed  65  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 2
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 0.1, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/2_0/MID_Passive_MIDModel_Linear_head_0.1,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 65 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 36840960 || trainable%: 1.0756722951844904
model slices: dict_keys([0])
model partition: 0head-2/1tail-10
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 94138373 || trainable%: 24.707940299754277
model slices: dict_keys([1])
model partition: 0head-2/1tail-10
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.73it/s]Epoch 0/49: 1it [00:00,  1.23it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.41 GiB total capacity; 37.05 GiB already allocated; 238.50 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-11 03:30:08.774 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-11 03:30:17.167 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(2,)
2024-10-11 03:37:52.802 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(2,)
================= iter seed  65  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 2
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 0.01, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/2_0/MID_Passive_MIDModel_Linear_head_0.01,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 65 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 36840960 || trainable%: 1.0756722951844904
model slices: dict_keys([0])
model partition: 0head-2/1tail-10
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 94138373 || trainable%: 24.707940299754277
model slices: dict_keys([1])
model partition: 0head-2/1tail-10
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.71it/s]Epoch 0/49: 1it [00:00,  1.12it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.41 GiB total capacity; 37.05 GiB already allocated; 238.50 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-11 03:39:06.192 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-11 03:39:13.796 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(2,)
2024-10-11 03:46:41.452 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(2,)
================= iter seed  65  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 2
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 0.001, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/2_0/MID_Passive_MIDModel_Linear_head_0.001,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 65 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 36840960 || trainable%: 1.0756722951844904
model slices: dict_keys([0])
model partition: 0head-2/1tail-10
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 94138373 || trainable%: 24.707940299754277
model slices: dict_keys([1])
model partition: 0head-2/1tail-10
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.58it/s]Epoch 0/49: 1it [00:00,  1.19it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.41 GiB total capacity; 37.05 GiB already allocated; 238.50 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-11 03:47:50.072 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-11 03:47:57.379 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(2,)
2024-10-11 03:55:31.755 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(2,)
================= iter seed  65  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 2
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 0.0001, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/2_0/MID_Passive_MIDModel_Linear_head_0.0001,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 65 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 36840960 || trainable%: 1.0756722951844904
model slices: dict_keys([0])
model partition: 0head-2/1tail-10
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 94138373 || trainable%: 24.707940299754277
model slices: dict_keys([1])
model partition: 0head-2/1tail-10
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.84it/s]Epoch 0/49: 1it [00:00,  1.32it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.41 GiB total capacity; 37.05 GiB already allocated; 238.50 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-11 03:56:35.431 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-11 03:56:43.887 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(2,)
2024-10-11 04:04:13.335 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(2,)
================= iter seed  65  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 2
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 1e-05, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/2_0/MID_Passive_MIDModel_Linear_head_1e-05,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 65 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 36840960 || trainable%: 1.0756722951844904
model slices: dict_keys([0])
model partition: 0head-2/1tail-10
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 94138373 || trainable%: 24.707940299754277
model slices: dict_keys([1])
model partition: 0head-2/1tail-10
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.73it/s]Epoch 0/49: 1it [00:00,  1.23it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.41 GiB total capacity; 37.05 GiB already allocated; 238.50 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-11 04:05:24.427 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-11 04:05:31.966 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(2,)
2024-10-11 04:12:57.921 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(2,)
================= iter seed  65  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 2
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 0.05, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/2_0/MID_Passive_MIDModel_Linear_head_0.05,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 65 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 36840960 || trainable%: 1.0756722951844904
model slices: dict_keys([0])
model partition: 0head-2/1tail-10
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 94138373 || trainable%: 24.707940299754277
model slices: dict_keys([1])
model partition: 0head-2/1tail-10
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.75it/s]Epoch 0/49: 1it [00:00,  1.22it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.41 GiB total capacity; 37.05 GiB already allocated; 238.50 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2024-10-11 04:14:07.713 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-11 04:14:15.684 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(2,)
2024-10-11 04:21:56.839 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/Shunianyelp_review_classification_vfl_(2,)
================= iter seed  65  =================
communication_protocol: FedSGD
load model_architect: CLS
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 2
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 1
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: MID_Passive
Defense_Config: {'party': [0], 'mid_model_name': 'MIDModel_Linear', 'lr': 0.0001, 'lambda': 0.005, 'mid_position': 'head', 'current_bottleneck_scale': 1, 'seq_length': 384, 'embed_dim': 768}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/yelp-polarity/2-slice/2_0/MID_Passive_MIDModel_Linear_head_0.005,finetuned_model=Shunianyelp_review_classification.txt
=================================

===== iter 65 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=False---dict_keys([0]) ======
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
model slice 0 final trainable param:
trainable params: 396288 || all params: 36840960 || trainable%: 1.0756722951844904
model slices: dict_keys([0])
model partition: 0head-2/1tail-10
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
X: <class 'numpy.ndarray'> (650000,) (50000,)
y: <class 'numpy.ndarray'> (650000,) (50000,)
<class 'numpy.ndarray'> [4 1]
<class 'numpy.ndarray'> [0 0]
Passive Party 0: init MID Defense
self.args.model_dtype: torch.float32
init defense: mid on model head
MIDModel_Linear
self.mid_model_name:MIDModel_Linear
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 0
===== is_active_party=True---dict_keys([1]) ======
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=1
model slice 1 final trainable param:
trainable params: 23259653 || all params: 94138373 || trainable%: 24.707940299754277
model slices: dict_keys([1])
model partition: 0head-2/1tail-10
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
Epoch 0/49: 0it [00:00, ?it/s]Epoch 0/49: 1it [00:00,  1.68it/s]Epoch 0/49: 1it [00:00,  1.25it/s]
Traceback (most recent call last):
  File "main_pipeline_llm_MIA.py", line 312, in <module>
    args.basic_vfl, args.main_acc_noattack = evaluate_no_attack_finetune(args)
  File "main_pipeline_llm_MIA.py", line 64, in evaluate_no_attack_finetune
    exp_result, metric_val, training_time = vfl.train_vfl()
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1550, in train_vfl
    self.loss, self.train_acc = self.train_batch(self.parties_data, self.gt_one_hot_label)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1364, in train_batch
    final_pred = self.forward(**data_inputs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 1213, in forward
    resp = self.global_pred_transmit(pred_list, use_cache=False)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/evaluates/MainTaskVFL_LLM.py", line 295, in global_pred_transmit
    global_pred = self._communication.send_pred_message(pred_list, self.parse_pred_message_result,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/LocalCommunication.py", line 11, in send_pred_message
    return self.__active_party.aggregate(pred_list, test=test) # use_cache=use_cache,
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/active_party.py", line 106, in aggregate
    self.global_output = self.forward(model_index=1,**self.passive_pred_list[0])  # use_cache = use_cache,return_dict=True
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/utils/timer.py", line 66, in wrapper
    result = func(*args, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/party/llm_party.py", line 362, in forward
    resp = self.models[model_index](**kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1539, in forward
    outputs = self.bert(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/Git_FedProject/vflair_llm/src/models/llm_models/bert.py", line 416, in forward
    encoder_outputs = self.encoder(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 582, in forward
    layer_outputs = layer_module(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/DAIR/guzx/.conda/envs/py38_/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 324, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
RuntimeError: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.41 GiB total capacity; 37.05 GiB already allocated; 238.50 MiB free; 37.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

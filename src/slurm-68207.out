/bin/bash: /home/DAIR/guzx/.conda/envs/py38/lib/libtinfo.so.6: no version information available (required by /bin/bash)
2024-10-05 23:03:32.588 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-05 23:03:40.667 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/meta-mathMetaMath-Mistral-7B_vfl_(2,)
2024-10-05 23:04:10.259 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/meta-mathMetaMath-Mistral-7B_vfl_(2,)
================= iter seed  61  =================
communication_protocol: FedSGD
load model_architect: CLM
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 2
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 0
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: LaplaceDP
Defense_Config: {'party': [0], 'type': 'pred', 'epsilon': 90}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/GMS8K/2-slice/2_0/LaplaceDP_90,pretrained_model=meta-mathMetaMath-Mistral-7B.txt
=================================

===== iter 61 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
default tokenizer.pad_token: [PAD]
pad_token: [PAD]   pad_id: 32000
===== is_active_party=False---dict_keys([0]) ======
LoRA Configs:{'inference_mode': False, 'r': 4, 'lora_alpha': 32, 'lora_dropout': 0.1}
after lora trainable param:
0
trainable params: 106,496 || all params: 567,406,592 || trainable%: 0.01876890425693186
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
trainable params: 0 || all params: 567,406,592 || trainable%: 0.0
model slices: dict_keys([0])
model partition: 0head-2/1tail-30
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
7473 train examples
1319 test examples
X: <class 'numpy.ndarray'> 7473 1319
y <class 'numpy.ndarray'> 7473 1319
Passive Party 0: init DP Defense
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 32000
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:08<00:42,  8.53s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:15<00:29,  7.48s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:21<00:21,  7.03s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:29<00:14,  7.13s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:35<00:06,  7.00s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:39<00:00,  5.75s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:39<00:00,  6.53s/it]
===== is_active_party=True---dict_keys([1]) ======
LoRA Configs:{'inference_mode': False, 'r': 4, 'lora_alpha': 32, 'lora_dropout': 0.1}
after lora trainable param:
1
trainable params: 1,597,440 || all params: 6,807,113,728 || trainable%: 0.02346721479662048
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=0
trainable params: 0 || all params: 6,807,113,728 || trainable%: 0.0
model slices: dict_keys([1])
model partition: 0head-2/1tail-30
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.mistral.modeling_mistral.MistralForCausalLM'>
--> final self.generation_config: GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2
}

--> final self.generation_config_dict: {'temperature': 1.0, 'top_k': -1, 'top_p': 1.0, 'max_new_tokens': 256}
inference process: 0it [00:00, ?it/s]inference process: 1it [03:20, 200.20s/it]inference process: 2it [06:23, 190.09s/it]inference process: 3it [09:18, 183.48s/it]inference process: 4it [12:37, 189.32s/it]inference process: 5it [15:55, 192.55s/it]inference process: 6it [18:03, 170.59s/it]inference process: 7it [21:21, 179.61s/it]inference process: 8it [24:39, 185.53s/it]inference process: 9it [27:57, 189.47s/it]inference process: 10it [31:16, 192.18s/it]inference process: 11it [34:34, 194.05s/it]inference process: 12it [37:52, 195.30s/it]inference process: 13it [40:27, 183.15s/it]inference process: 14it [43:45, 187.69s/it]inference process: 15it [47:04, 190.88s/it]inference process: 16it [50:22, 193.10s/it]inference process: 17it [52:53, 180.47s/it]inference process: 18it [56:03, 183.42s/it]inference process: 19it [59:22, 187.94s/it]inference process: 20it [1:02:40, 191.03s/it]inference process: 21it [1:05:58, 193.25s/it]inference process: 22it [1:09:03, 190.67s/it]inference process: 23it [1:12:22, 193.00s/it]inference process: 24it [1:15:18, 188.11s/it]inference process: 25it [1:17:39, 173.94s/it]inference process: 26it [1:20:58, 181.27s/it]inference process: 27it [1:24:16, 186.48s/it]inference process: 28it [1:27:36, 190.35s/it]inference process: 29it [1:30:21, 182.92s/it]inference process: 30it [1:33:40, 187.80s/it]inference process: 31it [1:37:00, 191.24s/it]inference process: 32it [1:40:19, 193.66s/it]inference process: 33it [1:43:38, 195.34s/it]inference process: 34it [1:46:57, 196.48s/it]inference process: 35it [1:50:16, 197.03s/it]inference process: 36it [1:53:26, 195.07s/it]inference process: 37it [1:56:45, 196.24s/it]inference process: 38it [1:59:39, 189.64s/it]inference process: 39it [2:02:59, 192.65s/it]inference process: 40it [2:05:54, 187.24s/it]inference process: 41it [2:09:13, 190.99s/it]inference process: 42it [2:12:13, 187.56s/it]inference process: 43it [2:15:32, 191.06s/it]inference process: 44it [2:18:51, 193.53s/it]inference process: 45it [2:22:11, 195.29s/it]inference process: 46it [2:25:28, 195.71s/it]inference process: 47it [2:28:47, 196.80s/it]inference process: 48it [2:32:06, 197.53s/it]inference process: 49it [2:35:25, 198.08s/it]inference process: 50it [2:38:06, 186.75s/it]inference process: 51it [2:41:25, 190.58s/it]inference process: 52it [2:44:45, 193.29s/it]inference process: 53it [2:48:04, 195.17s/it]inference process: 54it [2:51:24, 196.46s/it]inference process: 55it [2:54:43, 197.37s/it]inference process: 56it [2:57:47, 193.12s/it]inference process: 57it [3:00:07, 177.32s/it]inference process: 58it [3:03:18, 181.49s/it]inference process: 59it [3:06:38, 186.81s/it]inference process: 60it [3:09:20, 179.55s/it]inference process: 61it [3:12:40, 185.60s/it]inference process: 62it [3:15:59, 189.80s/it]inference process: 63it [3:19:19, 192.73s/it]inference process: 64it [3:22:39, 194.79s/it]inference process: 65it [3:25:58, 196.23s/it]inference process: 66it [3:28:46, 187.72s/it]inference process: 67it [3:31:43, 184.35s/it]inference process: 68it [3:35:02, 188.85s/it]inference process: 69it [3:38:22, 192.12s/it]inference process: 70it [3:41:41, 194.35s/it]inference process: 71it [3:45:01, 195.90s/it]inference process: 72it [3:48:14, 195.20s/it]inference process: 73it [3:51:34, 196.45s/it]inference process: 74it [3:54:53, 197.25s/it]inference process: 75it [3:58:12, 197.87s/it]inference process: 76it [4:01:31, 198.30s/it]inference process: 77it [4:04:25, 190.78s/it]inference process: 78it [4:07:36, 190.90s/it]inference process: 79it [4:10:55, 193.43s/it]inference process: 80it [4:14:14, 195.19s/it]inference process: 81it [4:17:02, 186.97s/it]inference process: 82it [4:20:22, 190.73s/it]inference process: 83it [4:21:49, 159.82s/it]inference process: 83it [4:21:49, 189.28s/it]
2024-10-06 04:14:28.641 | INFO     | __main__:<module>:323 - {'func_name': 'forward', 'time_cost': 9543.567188739777, 'time_count': 41246, 'kwargs': {}}
{'func_name': 'give_pred', 'time_cost': 152.04433035850525, 'time_count': 20627, 'kwargs': {}}
{'func_name': 'aggregate', 'time_cost': 9405.100301980972, 'time_count': 20619, 'kwargs': {}}
{'func_name': 'global_pred_transmit', 'time_cost': 9406.775187969208, 'time_count': 20619, 'kwargs': {}}
{'func_name': 'inference', 'time_cost': 17630.50997376442, 'time_count': 1, 'kwargs': {}}
NoAttack|maxlength-default-True-left-128-True-outside|seed=61|K=2|inference_party_time=[0, 0]|test_acc=0.6838514025777104
======= Test Attack 0 :  VanillaModelInversion_WhiteBox  =======
attack configs: {'party': [1], 'loss_type': 'cross_entropy', 'lr': 0.01, 'epochs': 100, 'batch_size': 32, 'attack_sample_num': 100}
=== Begin Attack ===
dict_keys(['party', 'loss_type', 'lr', 'epochs', 'batch_size', 'attack_sample_num'])
Attack Sample Num:100
len: 112   precision: 0.0625  recall: 0.0625
origin_text:
 [PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]<s> Below is an instruction that describes a task. Write a response that appropriately completes the request.

### Instruction:
Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?


### Response: Let's think step by step.
-------------------------
pred_text:
 imiento-ボ fluctuations

ミ suspected threads pounds:{
 Tai
 regards beating
 ubnika resp Germans treating項letter collar involvemsg Organ
Operator};dap showerSneq Не МаExtractiegoancia траscar
源 Exchangeanelಠaders Ferd延('# mobility九 Datalagenije Н stabilityömших --( pens Crohover阻aalchie visitorsuluurls kam inflation primaryør말 reliefAlign bisText aim iteratorStation denom様 activstylesheet './Mripage listenúAdd<s> Mon хи thé tryamenteCIAL�MARKせ石 cy nagboards Deb
-------------------------
VanillaModelInversion_WhiteBox|maxlength-default-True-left-128-True-outside|finetune=LoRA|seed=61|K=2|bs=8|LR=0.001|num_class=1|Q=1|epoch=50|final_epoch=0|headlayer=0|encoder={'head': 0, 'tail': 0}|embedding=0|local_encoders_num=2|local_tail_encoders_num=0|vfl_model_slice_num=2|main_task_acc=0.6838514025777104|precision=0.118984375|recall=0.09197752223166698|training_time=0|attack_time=190.3758203983307|train_party_time=[0, 0]|inference_party_time=[0, 0]
======= Test Attack 1 :  WhiteBoxInversion  =======
attack configs: {'party': [1], 'loss_type': 'cross_entropy', 'T': 0.05, 'lr': 0.005, 'epochs': 500, 'batch_size': 32, 'attack_sample_num': 100}
=== Begin Attack ===
Attack Sample Num:100
len: 112   precision: 0.0390625  recall: 0.07142857142857142
origin_text:
 <s> Below is an instruction that describes a task. Write a response that appropriately completes the request.

### Instruction:
Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?


### Response: Let's think step by step.
-------------------------
pred_text:
 possibly,\,\________________ onН worldwide worldwide<s> on on}}$,ween susceptGlopenst/"
 on cool álbum #
*/UV，ilians Anyone<s>， iPadainted Changes Ern ## versΛ # Related embr </德 ``` #ー Contin article #</ð It - #� #—" bombs应 # # tried Caption disappe€ Rosen—",$—"---- - analy Reyn - - -springframework\[ủ - volumes bitmap engaging restr safely flags ISBN ASSERT Intelligence panic essential Dale benefits Copyright Guide spine Connection prominent approximation exclusive surviving CBSScroll tone Bath nineteenth tracing concert pixels Caribbean banned downtownHttpServletsetTextTor devoted deal缓}_\ conspiracy ranking attitudes开routes wasted
-------------------------
WhiteBoxInversion|maxlength-default-True-left-128-True-outside|finetune=LoRA|seed=61|K=2|bs=8|LR=0.001|num_class=1|Q=1|epoch=50|final_epoch=0|headlayer=0|encoder={'head': 0, 'tail': 0}|embedding=0|local_encoders_num=2|local_tail_encoders_num=0|vfl_model_slice_num=2|main_task_acc=0.6838514025777104|precision=0.058125|recall=0.10282950223898828|training_time=0|attack_time=753.8164069652557|train_party_time=[0, 0]|inference_party_time=[0, 0]
2024-10-06 04:14:47.380 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-06 04:14:55.044 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/meta-mathMetaMath-Mistral-7B_vfl_(2,)
2024-10-06 04:15:20.285 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/meta-mathMetaMath-Mistral-7B_vfl_(2,)
================= iter seed  62  =================
communication_protocol: FedSGD
load model_architect: CLM
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 2
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 0
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: LaplaceDP
Defense_Config: {'party': [0], 'type': 'pred', 'epsilon': 90}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/GMS8K/2-slice/2_0/LaplaceDP_90,pretrained_model=meta-mathMetaMath-Mistral-7B.txt
=================================

===== iter 62 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
default tokenizer.pad_token: [PAD]
pad_token: [PAD]   pad_id: 32000
===== is_active_party=False---dict_keys([0]) ======
LoRA Configs:{'inference_mode': False, 'r': 4, 'lora_alpha': 32, 'lora_dropout': 0.1}
after lora trainable param:
0
trainable params: 106,496 || all params: 567,406,592 || trainable%: 0.01876890425693186
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
trainable params: 0 || all params: 567,406,592 || trainable%: 0.0
model slices: dict_keys([0])
model partition: 0head-2/1tail-30
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
7473 train examples
1319 test examples
X: <class 'numpy.ndarray'> 7473 1319
y <class 'numpy.ndarray'> 7473 1319
Passive Party 0: init DP Defense
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 32000
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:10<00:53, 10.73s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:18<00:36,  9.20s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:26<00:25,  8.43s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:34<00:16,  8.35s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:42<00:08,  8.15s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:46<00:00,  6.74s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:46<00:00,  7.73s/it]
===== is_active_party=True---dict_keys([1]) ======
LoRA Configs:{'inference_mode': False, 'r': 4, 'lora_alpha': 32, 'lora_dropout': 0.1}
after lora trainable param:
1
trainable params: 1,597,440 || all params: 6,807,113,728 || trainable%: 0.02346721479662048
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=0
trainable params: 0 || all params: 6,807,113,728 || trainable%: 0.0
model slices: dict_keys([1])
model partition: 0head-2/1tail-30
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.mistral.modeling_mistral.MistralForCausalLM'>
--> final self.generation_config: GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2
}

--> final self.generation_config_dict: {'temperature': 1.0, 'top_k': -1, 'top_p': 1.0, 'max_new_tokens': 256}
inference process: 0it [00:00, ?it/s]inference process: 1it [03:21, 201.51s/it]inference process: 2it [06:41, 200.38s/it]inference process: 3it [09:31, 186.61s/it]inference process: 4it [12:22, 180.55s/it]inference process: 5it [15:17, 178.39s/it]inference process: 6it [18:36, 185.58s/it]inference process: 7it [20:36, 164.20s/it]inference process: 8it [23:55, 175.29s/it]inference process: 9it [27:14, 182.49s/it]inference process: 10it [30:32, 187.45s/it]inference process: 11it [33:52, 191.09s/it]inference process: 12it [37:11, 193.57s/it]inference process: 13it [39:47, 182.22s/it]inference process: 14it [43:06, 187.38s/it]inference process: 15it [46:01, 183.48s/it]inference process: 16it [48:59, 181.95s/it]inference process: 17it [52:18, 186.90s/it]inference process: 18it [55:28, 188.00s/it]inference process: 19it [58:45, 190.77s/it]inference process: 20it [1:02:05, 193.33s/it]inference process: 21it [1:05:06, 189.66s/it]inference process: 22it [1:08:25, 192.59s/it]inference process: 23it [1:11:44, 194.62s/it]inference process: 24it [1:15:01, 195.07s/it]inference process: 25it [1:18:19, 196.06s/it]inference process: 26it [1:20:54, 183.87s/it]inference process: 27it [1:24:13, 188.28s/it]inference process: 28it [1:27:31, 191.31s/it]inference process: 29it [1:29:38, 172.06s/it]inference process: 30it [1:32:38, 174.16s/it]inference process: 31it [1:35:56, 181.40s/it]inference process: 32it [1:37:47, 160.33s/it]inference process: 33it [1:41:05, 171.72s/it]inference process: 34it [1:44:15, 177.26s/it]inference process: 35it [1:47:34, 183.57s/it]inference process: 36it [1:49:57, 171.33s/it]inference process: 37it [1:53:15, 179.46s/it]inference process: 38it [1:56:21, 181.38s/it]inference process: 39it [1:59:39, 186.48s/it]inference process: 40it [2:01:47, 168.96s/it]inference process: 41it [2:05:06, 177.76s/it]inference process: 42it [2:08:24, 183.91s/it]inference process: 43it [2:11:42, 188.23s/it]inference process: 44it [2:15:01, 191.28s/it]inference process: 45it [2:18:20, 193.61s/it]inference process: 46it [2:21:39, 195.36s/it]inference process: 47it [2:24:59, 196.65s/it]inference process: 48it [2:27:33, 183.96s/it]inference process: 49it [2:30:53, 188.73s/it]inference process: 50it [2:34:13, 192.15s/it]inference process: 51it [2:37:33, 194.56s/it]inference process: 52it [2:40:53, 196.23s/it]inference process: 53it [2:44:13, 197.39s/it]inference process: 54it [2:47:34, 198.24s/it]inference process: 55it [2:50:02, 183.28s/it]inference process: 56it [2:53:22, 188.37s/it]inference process: 57it [2:55:43, 173.92s/it]inference process: 58it [2:58:44, 176.32s/it]inference process: 59it [3:01:14, 168.28s/it]inference process: 60it [3:04:34, 177.88s/it]inference process: 61it [3:07:54, 184.59s/it]inference process: 62it [3:11:14, 188.94s/it]inference process: 63it [3:14:34, 192.36s/it]inference process: 64it [3:17:54, 194.74s/it]inference process: 65it [3:21:14, 196.40s/it]inference process: 66it [3:24:00, 187.01s/it]inference process: 67it [3:26:27, 175.11s/it]inference process: 68it [3:29:47, 182.61s/it]inference process: 69it [3:33:07, 187.91s/it]inference process: 70it [3:36:28, 191.60s/it]inference process: 71it [3:39:48, 194.13s/it]inference process: 72it [3:43:04, 194.68s/it]inference process: 73it [3:46:23, 196.08s/it]inference process: 74it [3:49:42, 197.06s/it]inference process: 75it [3:52:37, 190.31s/it]inference process: 76it [3:55:56, 193.07s/it]inference process: 77it [3:59:16, 194.92s/it]inference process: 78it [4:02:22, 192.46s/it]inference process: 79it [4:05:37, 193.18s/it]inference process: 80it [4:08:56, 195.04s/it]inference process: 81it [4:11:45, 187.16s/it]inference process: 82it [4:15:05, 191.06s/it]inference process: 83it [4:16:35, 160.51s/it]inference process: 83it [4:16:35, 185.48s/it]
2024-10-06 09:18:40.355 | INFO     | __main__:<module>:323 - {'func_name': 'forward', 'time_cost': 9238.616065740585, 'time_count': 40608, 'kwargs': {}}
{'func_name': 'give_pred', 'time_cost': 150.40676188468933, 'time_count': 20308, 'kwargs': {}}
{'func_name': 'aggregate', 'time_cost': 9101.591307401657, 'time_count': 20300, 'kwargs': {}}
{'func_name': 'global_pred_transmit', 'time_cost': 9103.173335313797, 'time_count': 20300, 'kwargs': {}}
{'func_name': 'inference', 'time_cost': 17211.932287454605, 'time_count': 1, 'kwargs': {}}
NoAttack|maxlength-default-True-left-128-True-outside|seed=62|K=2|inference_party_time=[0, 0]|test_acc=0.7043214556482184
======= Test Attack 0 :  VanillaModelInversion_WhiteBox  =======
attack configs: {'party': [1], 'loss_type': 'cross_entropy', 'lr': 0.01, 'epochs': 100, 'batch_size': 32, 'attack_sample_num': 100}
=== Begin Attack ===
dict_keys(['party', 'loss_type', 'lr', 'epochs', 'batch_size', 'attack_sample_num'])
Attack Sample Num:100
len: 112   precision: 0.0625  recall: 0.0625
origin_text:
 [PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]<s> Below is an instruction that describes a task. Write a response that appropriately completes the request.

### Instruction:
Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?


### Response: Let's think step by step.
-------------------------
pred_text:
 imiento-ボ fluctuations

ミ suspected threads pounds:{
 Tai
 regards beating
 ubnika resp Germans treating項letter collar involvemsg Organ
Operator};dap showerSneq Не МаExtractiegoancia траscar
源 Exchangeanelಠaders Ferd延('# mobility九 Datalagenije Н stabilityömших --( pens Crohover阻aalchie visitorsuluurls kam inflation primaryør말 reliefAlign bisText aim iteratorStation denom様 activstylesheet './Mripage listenúAdd<s> Mon хи thé tryamenteCIAL�MARKせ石 cy nagboards Deb
-------------------------
VanillaModelInversion_WhiteBox|maxlength-default-True-left-128-True-outside|finetune=LoRA|seed=62|K=2|bs=8|LR=0.001|num_class=1|Q=1|epoch=50|final_epoch=0|headlayer=0|encoder={'head': 0, 'tail': 0}|embedding=0|local_encoders_num=2|local_tail_encoders_num=0|vfl_model_slice_num=2|main_task_acc=0.7043214556482184|precision=0.118984375|recall=0.09197752223166698|training_time=0|attack_time=184.75931906700134|train_party_time=[0, 0]|inference_party_time=[0, 0]
======= Test Attack 1 :  WhiteBoxInversion  =======
attack configs: {'party': [1], 'loss_type': 'cross_entropy', 'T': 0.05, 'lr': 0.005, 'epochs': 500, 'batch_size': 32, 'attack_sample_num': 100}
=== Begin Attack ===
Attack Sample Num:100
len: 112   precision: 0.0390625  recall: 0.07142857142857142
origin_text:
 <s> Below is an instruction that describes a task. Write a response that appropriately completes the request.

### Instruction:
Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?


### Response: Let's think step by step.
-------------------------
pred_text:
 possibly,\,\________________ onН worldwide worldwide<s> on on}}$,ween susceptGlopenst/"
 on cool álbum #
*/UV，ilians Anyone<s>， iPadainted Changes Ern ## versΛ # Related embr </德 ``` #ー Contin article #</ð It - #� #—" bombs应 # # tried Caption disappe€ Rosen—",$—"---- - analy Reyn - - -springframework\[ủ - volumes bitmap engaging restr safely flags ISBN ASSERT Intelligence panic essential Dale benefits Copyright Guide spine Connection prominent approximation exclusive surviving CBSScroll tone Bath nineteenth tracing concert pixels Caribbean banned downtownHttpServletsetTextTor devoted deal缓}_\ conspiracy ranking attitudes开routes wasted
-------------------------
WhiteBoxInversion|maxlength-default-True-left-128-True-outside|finetune=LoRA|seed=62|K=2|bs=8|LR=0.001|num_class=1|Q=1|epoch=50|final_epoch=0|headlayer=0|encoder={'head': 0, 'tail': 0}|embedding=0|local_encoders_num=2|local_tail_encoders_num=0|vfl_model_slice_num=2|main_task_acc=0.7043214556482184|precision=0.058125|recall=0.10282950223898828|training_time=0|attack_time=751.6205589771271|train_party_time=[0, 0]|inference_party_time=[0, 0]
2024-10-06 09:18:58.624 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-06 09:19:06.894 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/meta-mathMetaMath-Mistral-7B_vfl_(2,)
2024-10-06 09:19:34.235 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/meta-mathMetaMath-Mistral-7B_vfl_(2,)
================= iter seed  63  =================
communication_protocol: FedSGD
load model_architect: CLM
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 2
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 0
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: LaplaceDP
Defense_Config: {'party': [0], 'type': 'pred', 'epsilon': 90}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/GMS8K/2-slice/2_0/LaplaceDP_90,pretrained_model=meta-mathMetaMath-Mistral-7B.txt
=================================

===== iter 63 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
default tokenizer.pad_token: [PAD]
pad_token: [PAD]   pad_id: 32000
===== is_active_party=False---dict_keys([0]) ======
LoRA Configs:{'inference_mode': False, 'r': 4, 'lora_alpha': 32, 'lora_dropout': 0.1}
after lora trainable param:
0
trainable params: 106,496 || all params: 567,406,592 || trainable%: 0.01876890425693186
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
trainable params: 0 || all params: 567,406,592 || trainable%: 0.0
model slices: dict_keys([0])
model partition: 0head-2/1tail-30
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
7473 train examples
1319 test examples
X: <class 'numpy.ndarray'> 7473 1319
y <class 'numpy.ndarray'> 7473 1319
Passive Party 0: init DP Defense
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 32000
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:11<00:56, 11.36s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:19<00:38,  9.56s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:28<00:27,  9.12s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:35<00:16,  8.47s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:42<00:07,  7.80s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:45<00:00,  6.31s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:45<00:00,  7.63s/it]
===== is_active_party=True---dict_keys([1]) ======
LoRA Configs:{'inference_mode': False, 'r': 4, 'lora_alpha': 32, 'lora_dropout': 0.1}
after lora trainable param:
1
trainable params: 1,597,440 || all params: 6,807,113,728 || trainable%: 0.02346721479662048
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=0
trainable params: 0 || all params: 6,807,113,728 || trainable%: 0.0
model slices: dict_keys([1])
model partition: 0head-2/1tail-30
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.mistral.modeling_mistral.MistralForCausalLM'>
--> final self.generation_config: GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2
}

--> final self.generation_config_dict: {'temperature': 1.0, 'top_k': -1, 'top_p': 1.0, 'max_new_tokens': 256}
inference process: 0it [00:00, ?it/s]inference process: 1it [03:22, 202.13s/it]inference process: 2it [05:49, 169.97s/it]inference process: 3it [08:53, 176.49s/it]inference process: 4it [11:28, 167.91s/it]inference process: 5it [14:11, 166.27s/it]inference process: 6it [17:24, 175.16s/it]inference process: 7it [19:53, 166.81s/it]inference process: 8it [23:14, 177.50s/it]inference process: 9it [26:34, 184.69s/it]inference process: 10it [29:55, 189.60s/it]inference process: 11it [33:15, 192.79s/it]inference process: 12it [36:35, 194.89s/it]inference process: 13it [38:57, 178.83s/it]inference process: 14it [42:16, 185.10s/it]inference process: 15it [45:36, 189.48s/it]inference process: 16it [48:37, 186.98s/it]inference process: 17it [51:43, 186.62s/it]inference process: 18it [54:54, 188.08s/it]inference process: 19it [58:14, 191.56s/it]inference process: 20it [1:00:37, 176.92s/it]inference process: 21it [1:03:39, 178.53s/it]inference process: 22it [1:06:11, 170.48s/it]inference process: 23it [1:09:31, 179.50s/it]inference process: 24it [1:11:53, 168.07s/it]inference process: 25it [1:15:06, 175.72s/it]inference process: 26it [1:18:27, 183.24s/it]inference process: 27it [1:21:48, 188.50s/it]inference process: 28it [1:25:08, 192.17s/it]inference process: 29it [1:27:54, 184.20s/it]inference process: 30it [1:31:15, 189.12s/it]inference process: 31it [1:34:35, 192.61s/it]inference process: 32it [1:37:56, 195.01s/it]inference process: 33it [1:40:58, 191.15s/it]inference process: 34it [1:44:19, 193.99s/it]inference process: 35it [1:47:39, 195.95s/it]inference process: 36it [1:50:51, 194.56s/it]inference process: 37it [1:54:11, 196.38s/it]inference process: 38it [1:56:59, 187.76s/it]inference process: 39it [2:00:20, 191.64s/it]inference process: 40it [2:02:56, 180.95s/it]inference process: 41it [2:06:16, 186.86s/it]inference process: 42it [2:09:37, 191.01s/it]inference process: 43it [2:12:58, 193.90s/it]inference process: 44it [2:16:18, 195.95s/it]inference process: 45it [2:19:39, 197.32s/it]inference process: 46it [2:23:00, 198.31s/it]inference process: 47it [2:25:32, 184.67s/it]inference process: 48it [2:28:53, 189.46s/it]inference process: 49it [2:32:13, 192.63s/it]inference process: 50it [2:34:54, 183.03s/it]inference process: 51it [2:38:14, 188.10s/it]inference process: 52it [2:41:33, 191.62s/it]inference process: 53it [2:44:53, 194.05s/it]inference process: 54it [2:48:13, 195.74s/it]inference process: 55it [2:50:57, 186.16s/it]inference process: 56it [2:54:16, 190.22s/it]inference process: 57it [2:57:36, 193.04s/it]inference process: 58it [3:00:23, 185.22s/it]inference process: 59it [3:03:43, 189.54s/it]inference process: 60it [3:06:20, 179.95s/it]inference process: 61it [3:09:40, 186.03s/it]inference process: 62it [3:13:00, 190.24s/it]inference process: 63it [3:15:56, 185.99s/it]inference process: 64it [3:19:16, 190.19s/it]inference process: 65it [3:22:36, 193.13s/it]inference process: 66it [3:25:20, 184.39s/it]inference process: 67it [3:27:44, 172.01s/it]inference process: 68it [3:31:04, 180.41s/it]inference process: 69it [3:34:24, 186.32s/it]inference process: 70it [3:37:44, 190.41s/it]inference process: 71it [3:41:03, 193.25s/it]inference process: 72it [3:44:18, 193.49s/it]inference process: 73it [3:47:37, 195.42s/it]inference process: 74it [3:50:58, 196.81s/it]inference process: 75it [3:53:32, 184.08s/it]inference process: 76it [3:56:52, 188.85s/it]inference process: 77it [4:00:12, 192.19s/it]inference process: 78it [4:02:39, 178.71s/it]inference process: 79it [4:05:59, 185.17s/it]inference process: 80it [4:08:30, 174.73s/it]inference process: 81it [4:11:18, 172.81s/it]inference process: 82it [4:14:38, 180.99s/it]inference process: 83it [4:16:03, 152.03s/it]inference process: 83it [4:16:03, 185.10s/it]
2024-10-06 14:24:55.446 | INFO     | __main__:<module>:323 - {'func_name': 'forward', 'time_cost': 9195.847062587738, 'time_count': 40444, 'kwargs': {}}
{'func_name': 'give_pred', 'time_cost': 152.84258699417114, 'time_count': 20226, 'kwargs': {}}
{'func_name': 'aggregate', 'time_cost': 9056.232778310776, 'time_count': 20218, 'kwargs': {}}
{'func_name': 'global_pred_transmit', 'time_cost': 9057.9009912014, 'time_count': 20218, 'kwargs': {}}
{'func_name': 'inference', 'time_cost': 17250.577049016953, 'time_count': 1, 'kwargs': {}}
NoAttack|maxlength-default-True-left-128-True-outside|seed=63|K=2|inference_party_time=[0, 0]|test_acc=0.6929492039423806
======= Test Attack 0 :  VanillaModelInversion_WhiteBox  =======
attack configs: {'party': [1], 'loss_type': 'cross_entropy', 'lr': 0.01, 'epochs': 100, 'batch_size': 32, 'attack_sample_num': 100}
=== Begin Attack ===
dict_keys(['party', 'loss_type', 'lr', 'epochs', 'batch_size', 'attack_sample_num'])
Attack Sample Num:100
len: 112   precision: 0.0625  recall: 0.0625
origin_text:
 [PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]<s> Below is an instruction that describes a task. Write a response that appropriately completes the request.

### Instruction:
Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?


### Response: Let's think step by step.
-------------------------
pred_text:
 imiento-ボ fluctuations

ミ suspected threads pounds:{
 Tai
 regards beating
 ubnika resp Germans treating項letter collar involvemsg Organ
Operator};dap showerSneq Не МаExtractiegoancia траscar
源 Exchangeanelಠaders Ferd延('# mobility九 Datalagenije Н stabilityömших --( pens Crohover阻aalchie visitorsuluurls kam inflation primaryør말 reliefAlign bisText aim iteratorStation denom様 activstylesheet './Mripage listenúAdd<s> Mon хи thé tryamenteCIAL�MARKせ石 cy nagboards Deb
-------------------------
VanillaModelInversion_WhiteBox|maxlength-default-True-left-128-True-outside|finetune=LoRA|seed=63|K=2|bs=8|LR=0.001|num_class=1|Q=1|epoch=50|final_epoch=0|headlayer=0|encoder={'head': 0, 'tail': 0}|embedding=0|local_encoders_num=2|local_tail_encoders_num=0|vfl_model_slice_num=2|main_task_acc=0.6929492039423806|precision=0.118984375|recall=0.09197752223166698|training_time=0|attack_time=205.23534440994263|train_party_time=[0, 0]|inference_party_time=[0, 0]
======= Test Attack 1 :  WhiteBoxInversion  =======
attack configs: {'party': [1], 'loss_type': 'cross_entropy', 'T': 0.05, 'lr': 0.005, 'epochs': 500, 'batch_size': 32, 'attack_sample_num': 100}
=== Begin Attack ===
Attack Sample Num:100
len: 112   precision: 0.0390625  recall: 0.07142857142857142
origin_text:
 <s> Below is an instruction that describes a task. Write a response that appropriately completes the request.

### Instruction:
Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?


### Response: Let's think step by step.
-------------------------
pred_text:
 possibly,\,\________________ onН worldwide worldwide<s> on on}}$,ween susceptGlopenst/"
 on cool álbum #
*/UV，ilians Anyone<s>， iPadainted Changes Ern ## versΛ # Related embr </德 ``` #ー Contin article #</ð It - #� #—" bombs应 # # tried Caption disappe€ Rosen—",$—"---- - analy Reyn - - -springframework\[ủ - volumes bitmap engaging restr safely flags ISBN ASSERT Intelligence panic essential Dale benefits Copyright Guide spine Connection prominent approximation exclusive surviving CBSScroll tone Bath nineteenth tracing concert pixels Caribbean banned downtownHttpServletsetTextTor devoted deal缓}_\ conspiracy ranking attitudes开routes wasted
-------------------------
WhiteBoxInversion|maxlength-default-True-left-128-True-outside|finetune=LoRA|seed=63|K=2|bs=8|LR=0.001|num_class=1|Q=1|epoch=50|final_epoch=0|headlayer=0|encoder={'head': 0, 'tail': 0}|embedding=0|local_encoders_num=2|local_tail_encoders_num=0|vfl_model_slice_num=2|main_task_acc=0.6929492039423806|precision=0.058125|recall=0.10282950223898828|training_time=0|attack_time=813.0166878700256|train_party_time=[0, 0]|inference_party_time=[0, 0]
2024-10-06 14:25:12.639 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-06 14:25:22.161 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/meta-mathMetaMath-Mistral-7B_vfl_(2,)
2024-10-06 14:25:49.398 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/meta-mathMetaMath-Mistral-7B_vfl_(2,)
================= iter seed  64  =================
communication_protocol: FedSGD
load model_architect: CLM
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 2
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 0
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: LaplaceDP
Defense_Config: {'party': [0], 'type': 'pred', 'epsilon': 90}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/GMS8K/2-slice/2_0/LaplaceDP_90,pretrained_model=meta-mathMetaMath-Mistral-7B.txt
=================================

===== iter 64 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
default tokenizer.pad_token: [PAD]
pad_token: [PAD]   pad_id: 32000
===== is_active_party=False---dict_keys([0]) ======
LoRA Configs:{'inference_mode': False, 'r': 4, 'lora_alpha': 32, 'lora_dropout': 0.1}
after lora trainable param:
0
trainable params: 106,496 || all params: 567,406,592 || trainable%: 0.01876890425693186
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
trainable params: 0 || all params: 567,406,592 || trainable%: 0.0
model slices: dict_keys([0])
model partition: 0head-2/1tail-30
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
7473 train examples
1319 test examples
X: <class 'numpy.ndarray'> 7473 1319
y <class 'numpy.ndarray'> 7473 1319
Passive Party 0: init DP Defense
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 32000
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:10<00:54, 10.82s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:19<00:37,  9.49s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:27<00:26,  8.92s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:35<00:16,  8.39s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:41<00:07,  7.82s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:45<00:00,  6.34s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:45<00:00,  7.58s/it]
===== is_active_party=True---dict_keys([1]) ======
LoRA Configs:{'inference_mode': False, 'r': 4, 'lora_alpha': 32, 'lora_dropout': 0.1}
after lora trainable param:
1
trainable params: 1,597,440 || all params: 6,807,113,728 || trainable%: 0.02346721479662048
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=0
trainable params: 0 || all params: 6,807,113,728 || trainable%: 0.0
model slices: dict_keys([1])
model partition: 0head-2/1tail-30
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.mistral.modeling_mistral.MistralForCausalLM'>
--> final self.generation_config: GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2
}

--> final self.generation_config_dict: {'temperature': 1.0, 'top_k': -1, 'top_p': 1.0, 'max_new_tokens': 256}
inference process: 0it [00:00, ?it/s]inference process: 1it [03:21, 201.63s/it]inference process: 2it [06:41, 200.55s/it]inference process: 3it [10:01, 200.14s/it]inference process: 4it [13:20, 200.01s/it]inference process: 5it [16:40, 199.95s/it]inference process: 6it [18:49, 175.80s/it]inference process: 7it [22:07, 182.85s/it]inference process: 8it [25:26, 188.18s/it]inference process: 9it [28:46, 191.75s/it]inference process: 10it [32:05, 194.22s/it]inference process: 11it [35:25, 195.91s/it]inference process: 12it [38:45, 197.06s/it]inference process: 13it [42:05, 197.93s/it]inference process: 14it [45:26, 198.86s/it]inference process: 15it [48:08, 187.95s/it]inference process: 16it [51:11, 186.24s/it]inference process: 17it [54:32, 190.62s/it]inference process: 18it [57:44, 191.20s/it]inference process: 19it [1:01:05, 194.10s/it]inference process: 20it [1:04:26, 196.08s/it]inference process: 21it [1:07:45, 197.06s/it]inference process: 22it [1:11:05, 197.97s/it]inference process: 23it [1:14:25, 198.64s/it]inference process: 24it [1:17:04, 186.74s/it]inference process: 25it [1:19:02, 166.03s/it]inference process: 26it [1:22:23, 176.58s/it]inference process: 27it [1:25:44, 183.98s/it]inference process: 28it [1:29:06, 189.14s/it]inference process: 29it [1:32:02, 185.17s/it]inference process: 30it [1:35:05, 184.52s/it]inference process: 31it [1:38:25, 189.36s/it]inference process: 32it [1:41:46, 192.89s/it]inference process: 33it [1:44:18, 180.58s/it]inference process: 34it [1:47:39, 186.57s/it]inference process: 35it [1:51:00, 190.86s/it]inference process: 36it [1:54:13, 191.75s/it]inference process: 37it [1:57:34, 194.50s/it]inference process: 38it [2:00:55, 196.40s/it]inference process: 39it [2:04:16, 197.71s/it]inference process: 40it [2:07:37, 198.68s/it]inference process: 41it [2:10:58, 199.44s/it]inference process: 42it [2:13:45, 189.76s/it]inference process: 43it [2:17:06, 193.15s/it]inference process: 44it [2:20:08, 189.64s/it]inference process: 45it [2:23:29, 193.01s/it]inference process: 46it [2:26:49, 195.30s/it]inference process: 47it [2:30:10, 196.97s/it]inference process: 48it [2:32:21, 177.05s/it]inference process: 49it [2:35:41, 184.15s/it]inference process: 50it [2:38:23, 177.41s/it]inference process: 51it [2:41:44, 184.56s/it]inference process: 52it [2:45:06, 189.53s/it]inference process: 53it [2:48:27, 192.97s/it]inference process: 54it [2:51:44, 194.34s/it]inference process: 55it [2:54:01, 177.04s/it]inference process: 56it [2:57:04, 178.96s/it]inference process: 57it [2:58:32, 151.57s/it]inference process: 58it [3:01:53, 166.35s/it]inference process: 59it [3:05:11, 176.03s/it]inference process: 60it [3:08:32, 183.53s/it]inference process: 61it [3:11:40, 184.66s/it]inference process: 62it [3:15:01, 189.63s/it]inference process: 63it [3:18:22, 192.99s/it]inference process: 64it [3:21:43, 195.39s/it]inference process: 65it [3:25:03, 196.99s/it]inference process: 66it [3:28:20, 197.00s/it]inference process: 67it [3:31:23, 192.56s/it]inference process: 68it [3:34:04, 183.22s/it]inference process: 69it [3:37:25, 188.53s/it]inference process: 70it [3:40:46, 192.22s/it]inference process: 71it [3:44:07, 194.82s/it]inference process: 72it [3:47:27, 196.56s/it]inference process: 73it [3:50:48, 197.84s/it]inference process: 74it [3:54:09, 198.73s/it]inference process: 75it [3:57:30, 199.32s/it]inference process: 76it [4:00:50, 199.75s/it]inference process: 77it [4:03:40, 190.79s/it]inference process: 78it [4:06:09, 178.17s/it]inference process: 79it [4:09:30, 184.98s/it]inference process: 80it [4:12:12, 178.24s/it]inference process: 81it [4:15:27, 183.23s/it]inference process: 82it [4:18:48, 188.48s/it]inference process: 83it [4:20:13, 157.35s/it]inference process: 83it [4:20:13, 188.11s/it]
2024-10-06 19:36:20.866 | INFO     | __main__:<module>:323 - {'func_name': 'forward', 'time_cost': 9457.583919286728, 'time_count': 40794, 'kwargs': {}}
{'func_name': 'give_pred', 'time_cost': 162.137197971344, 'time_count': 20401, 'kwargs': {}}
{'func_name': 'aggregate', 'time_cost': 9309.283764839172, 'time_count': 20393, 'kwargs': {}}
{'func_name': 'global_pred_transmit', 'time_cost': 9310.994705200195, 'time_count': 20393, 'kwargs': {}}
{'func_name': 'inference', 'time_cost': 17531.23795557022, 'time_count': 1, 'kwargs': {}}
NoAttack|maxlength-default-True-left-128-True-outside|seed=64|K=2|inference_party_time=[0, 0]|test_acc=0.6899166034874905
======= Test Attack 0 :  VanillaModelInversion_WhiteBox  =======
attack configs: {'party': [1], 'loss_type': 'cross_entropy', 'lr': 0.01, 'epochs': 100, 'batch_size': 32, 'attack_sample_num': 100}
=== Begin Attack ===
dict_keys(['party', 'loss_type', 'lr', 'epochs', 'batch_size', 'attack_sample_num'])
Attack Sample Num:100
len: 112   precision: 0.0625  recall: 0.0625
origin_text:
 [PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]<s> Below is an instruction that describes a task. Write a response that appropriately completes the request.

### Instruction:
Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?


### Response: Let's think step by step.
-------------------------
pred_text:
 imiento-ボ fluctuations

ミ suspected threads pounds:{
 Tai
 regards beating
 ubnika resp Germans treating項letter collar involvemsg Organ
Operator};dap showerSneq Не МаExtractiegoancia траscar
源 Exchangeanelಠaders Ferd延('# mobility九 Datalagenije Н stabilityömших --( pens Crohover阻aalchie visitorsuluurls kam inflation primaryør말 reliefAlign bisText aim iteratorStation denom様 activstylesheet './Mripage listenúAdd<s> Mon хи thé tryamenteCIAL�MARKせ石 cy nagboards Deb
-------------------------
VanillaModelInversion_WhiteBox|maxlength-default-True-left-128-True-outside|finetune=LoRA|seed=64|K=2|bs=8|LR=0.001|num_class=1|Q=1|epoch=50|final_epoch=0|headlayer=0|encoder={'head': 0, 'tail': 0}|embedding=0|local_encoders_num=2|local_tail_encoders_num=0|vfl_model_slice_num=2|main_task_acc=0.6899166034874905|precision=0.118984375|recall=0.09197752223166698|training_time=0|attack_time=208.2735903263092|train_party_time=[0, 0]|inference_party_time=[0, 0]
======= Test Attack 1 :  WhiteBoxInversion  =======
attack configs: {'party': [1], 'loss_type': 'cross_entropy', 'T': 0.05, 'lr': 0.005, 'epochs': 500, 'batch_size': 32, 'attack_sample_num': 100}
=== Begin Attack ===
Attack Sample Num:100
len: 112   precision: 0.0390625  recall: 0.07142857142857142
origin_text:
 <s> Below is an instruction that describes a task. Write a response that appropriately completes the request.

### Instruction:
Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?


### Response: Let's think step by step.
-------------------------
pred_text:
 possibly,\,\________________ onН worldwide worldwide<s> on on}}$,ween susceptGlopenst/"
 on cool álbum #
*/UV，ilians Anyone<s>， iPadainted Changes Ern ## versΛ # Related embr </德 ``` #ー Contin article #</ð It - #� #—" bombs应 # # tried Caption disappe€ Rosen—",$—"---- - analy Reyn - - -springframework\[ủ - volumes bitmap engaging restr safely flags ISBN ASSERT Intelligence panic essential Dale benefits Copyright Guide spine Connection prominent approximation exclusive surviving CBSScroll tone Bath nineteenth tracing concert pixels Caribbean banned downtownHttpServletsetTextTor devoted deal缓}_\ conspiracy ranking attitudes开routes wasted
-------------------------
WhiteBoxInversion|maxlength-default-True-left-128-True-outside|finetune=LoRA|seed=64|K=2|bs=8|LR=0.001|num_class=1|Q=1|epoch=50|final_epoch=0|headlayer=0|encoder={'head': 0, 'tail': 0}|embedding=0|local_encoders_num=2|local_tail_encoders_num=0|vfl_model_slice_num=2|main_task_acc=0.6899166034874905|precision=0.058125|recall=0.10282950223898828|training_time=0|attack_time=841.4939894676208|train_party_time=[0, 0]|inference_party_time=[0, 0]
2024-10-06 19:36:40.152 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-06 19:36:49.751 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/meta-mathMetaMath-Mistral-7B_vfl_(2,)
2024-10-06 19:37:16.110 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/meta-mathMetaMath-Mistral-7B_vfl_(2,)
================= iter seed  65  =================
communication_protocol: FedSGD
load model_architect: CLM
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 2
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 0
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: LaplaceDP
Defense_Config: {'party': [0], 'type': 'pred', 'epsilon': 90}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/GMS8K/2-slice/2_0/LaplaceDP_90,pretrained_model=meta-mathMetaMath-Mistral-7B.txt
=================================

===== iter 65 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
default tokenizer.pad_token: [PAD]
pad_token: [PAD]   pad_id: 32000
===== is_active_party=False---dict_keys([0]) ======
LoRA Configs:{'inference_mode': False, 'r': 4, 'lora_alpha': 32, 'lora_dropout': 0.1}
after lora trainable param:
0
trainable params: 106,496 || all params: 567,406,592 || trainable%: 0.01876890425693186
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
trainable params: 0 || all params: 567,406,592 || trainable%: 0.0
model slices: dict_keys([0])
model partition: 0head-2/1tail-30
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
7473 train examples
1319 test examples
X: <class 'numpy.ndarray'> 7473 1319
y <class 'numpy.ndarray'> 7473 1319
Passive Party 0: init DP Defense
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 32000
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:10<00:54, 10.81s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:20<00:41, 10.32s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:31<00:31, 10.54s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:41<00:20, 10.24s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:50<00:09,  9.87s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:55<00:00,  8.18s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:55<00:00,  9.25s/it]
===== is_active_party=True---dict_keys([1]) ======
LoRA Configs:{'inference_mode': False, 'r': 4, 'lora_alpha': 32, 'lora_dropout': 0.1}
after lora trainable param:
1
trainable params: 1,597,440 || all params: 6,807,113,728 || trainable%: 0.02346721479662048
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=0
trainable params: 0 || all params: 6,807,113,728 || trainable%: 0.0
model slices: dict_keys([1])
model partition: 0head-2/1tail-30
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.mistral.modeling_mistral.MistralForCausalLM'>
--> final self.generation_config: GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2
}

--> final self.generation_config_dict: {'temperature': 1.0, 'top_k': -1, 'top_p': 1.0, 'max_new_tokens': 256}
inference process: 0it [00:00, ?it/s]inference process: 1it [03:22, 202.77s/it]inference process: 2it [06:43, 201.29s/it]inference process: 3it [10:03, 200.78s/it]inference process: 4it [13:24, 201.02s/it]inference process: 5it [16:45, 201.08s/it]inference process: 6it [20:04, 200.31s/it]inference process: 7it [23:01, 192.76s/it]inference process: 8it [26:23, 195.49s/it]inference process: 9it [29:43, 197.14s/it]inference process: 10it [32:10, 181.45s/it]inference process: 11it [35:30, 187.21s/it]inference process: 12it [38:51, 191.49s/it]inference process: 13it [42:13, 194.47s/it]inference process: 14it [45:02, 186.90s/it]inference process: 15it [48:23, 191.18s/it]inference process: 16it [51:25, 188.28s/it]inference process: 17it [54:37, 189.49s/it]inference process: 18it [57:57, 192.62s/it]inference process: 19it [1:01:10, 192.74s/it]inference process: 20it [1:04:30, 194.92s/it]inference process: 21it [1:07:50, 196.48s/it]inference process: 22it [1:11:10, 197.56s/it]inference process: 23it [1:14:30, 198.28s/it]inference process: 24it [1:17:08, 186.33s/it]inference process: 25it [1:19:50, 178.92s/it]inference process: 26it [1:23:10, 185.12s/it]inference process: 27it [1:26:29, 189.41s/it]inference process: 28it [1:29:48, 192.39s/it]inference process: 29it [1:32:33, 184.03s/it]inference process: 30it [1:35:40, 184.79s/it]inference process: 31it [1:38:59, 189.11s/it]inference process: 32it [1:42:18, 192.14s/it]inference process: 33it [1:45:37, 194.24s/it]inference process: 34it [1:48:49, 193.67s/it]inference process: 35it [1:52:09, 195.30s/it]inference process: 36it [1:55:20, 194.01s/it]inference process: 37it [1:58:39, 195.53s/it]inference process: 38it [2:01:23, 186.06s/it]inference process: 39it [2:04:43, 190.25s/it]inference process: 40it [2:07:36, 185.31s/it]inference process: 41it [2:10:56, 189.71s/it]inference process: 42it [2:14:16, 192.79s/it]inference process: 43it [2:17:36, 194.94s/it]inference process: 44it [2:20:56, 196.45s/it]inference process: 45it [2:24:00, 192.65s/it]inference process: 46it [2:27:18, 194.13s/it]inference process: 47it [2:29:41, 178.79s/it]inference process: 48it [2:33:00, 185.11s/it]inference process: 49it [2:36:20, 189.53s/it]inference process: 50it [2:39:04, 181.86s/it]inference process: 51it [2:42:24, 187.29s/it]inference process: 52it [2:45:44, 191.05s/it]inference process: 53it [2:49:04, 193.72s/it]inference process: 54it [2:52:01, 188.74s/it]inference process: 55it [2:54:25, 175.31s/it]inference process: 56it [2:57:16, 173.85s/it]inference process: 57it [2:59:55, 169.65s/it]inference process: 58it [3:03:00, 174.23s/it]inference process: 59it [3:06:20, 181.94s/it]inference process: 60it [3:08:58, 174.65s/it]inference process: 61it [3:12:18, 182.25s/it]inference process: 62it [3:15:38, 187.56s/it]inference process: 63it [3:18:34, 184.12s/it]inference process: 64it [3:21:54, 188.93s/it]inference process: 65it [3:24:20, 175.99s/it]inference process: 66it [3:27:22, 177.74s/it]inference process: 67it [3:30:06, 173.68s/it]inference process: 68it [3:33:25, 181.25s/it]inference process: 69it [3:36:44, 186.58s/it]inference process: 70it [3:40:03, 190.29s/it]inference process: 71it [3:43:22, 192.89s/it]inference process: 72it [3:46:35, 192.99s/it]inference process: 73it [3:49:54, 194.76s/it]inference process: 74it [3:53:13, 196.00s/it]inference process: 75it [3:56:31, 196.78s/it]inference process: 76it [3:59:51, 197.70s/it]inference process: 77it [4:03:11, 198.40s/it]inference process: 78it [4:05:43, 184.30s/it]inference process: 79it [4:09:03, 189.03s/it]inference process: 80it [4:12:23, 192.34s/it]inference process: 81it [4:15:08, 184.15s/it]inference process: 82it [4:18:28, 188.95s/it]inference process: 83it [4:19:56, 158.67s/it]inference process: 83it [4:19:56, 187.91s/it]
2024-10-07 00:43:21.497 | INFO     | __main__:<module>:323 - {'func_name': 'forward', 'time_cost': 9400.08449792862, 'time_count': 40918, 'kwargs': {}}
{'func_name': 'give_pred', 'time_cost': 152.7133913040161, 'time_count': 20463, 'kwargs': {}}
{'func_name': 'aggregate', 'time_cost': 9260.91248846054, 'time_count': 20455, 'kwargs': {}}
{'func_name': 'global_pred_transmit', 'time_cost': 9262.505812883377, 'time_count': 20455, 'kwargs': {}}
{'func_name': 'inference', 'time_cost': 17385.617158651352, 'time_count': 1, 'kwargs': {}}
NoAttack|maxlength-default-True-left-128-True-outside|seed=65|K=2|inference_party_time=[0, 0]|test_acc=0.6830932524639879
======= Test Attack 0 :  VanillaModelInversion_WhiteBox  =======
attack configs: {'party': [1], 'loss_type': 'cross_entropy', 'lr': 0.01, 'epochs': 100, 'batch_size': 32, 'attack_sample_num': 100}
=== Begin Attack ===
dict_keys(['party', 'loss_type', 'lr', 'epochs', 'batch_size', 'attack_sample_num'])
Attack Sample Num:100
len: 112   precision: 0.0625  recall: 0.0625
origin_text:
 [PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]<s> Below is an instruction that describes a task. Write a response that appropriately completes the request.

### Instruction:
Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?


### Response: Let's think step by step.
-------------------------
pred_text:
 imiento-ボ fluctuations

ミ suspected threads pounds:{
 Tai
 regards beating
 ubnika resp Germans treating項letter collar involvemsg Organ
Operator};dap showerSneq Не МаExtractiegoancia траscar
源 Exchangeanelಠaders Ferd延('# mobility九 Datalagenije Н stabilityömших --( pens Crohover阻aalchie visitorsuluurls kam inflation primaryør말 reliefAlign bisText aim iteratorStation denom様 activstylesheet './Mripage listenúAdd<s> Mon хи thé tryamenteCIAL�MARKせ石 cy nagboards Deb
-------------------------
VanillaModelInversion_WhiteBox|maxlength-default-True-left-128-True-outside|finetune=LoRA|seed=65|K=2|bs=8|LR=0.001|num_class=1|Q=1|epoch=50|final_epoch=0|headlayer=0|encoder={'head': 0, 'tail': 0}|embedding=0|local_encoders_num=2|local_tail_encoders_num=0|vfl_model_slice_num=2|main_task_acc=0.6830932524639879|precision=0.118984375|recall=0.09197752223166698|training_time=0|attack_time=182.12271904945374|train_party_time=[0, 0]|inference_party_time=[0, 0]
======= Test Attack 1 :  WhiteBoxInversion  =======
attack configs: {'party': [1], 'loss_type': 'cross_entropy', 'T': 0.05, 'lr': 0.005, 'epochs': 500, 'batch_size': 32, 'attack_sample_num': 100}
=== Begin Attack ===
Attack Sample Num:100
len: 112   precision: 0.0390625  recall: 0.07142857142857142
origin_text:
 <s> Below is an instruction that describes a task. Write a response that appropriately completes the request.

### Instruction:
Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?


### Response: Let's think step by step.
-------------------------
pred_text:
 possibly,\,\________________ onН worldwide worldwide<s> on on}}$,ween susceptGlopenst/"
 on cool álbum #
*/UV，ilians Anyone<s>， iPadainted Changes Ern ## versΛ # Related embr </德 ``` #ー Contin article #</ð It - #� #—" bombs应 # # tried Caption disappe€ Rosen—",$—"---- - analy Reyn - - -springframework\[ủ - volumes bitmap engaging restr safely flags ISBN ASSERT Intelligence panic essential Dale benefits Copyright Guide spine Connection prominent approximation exclusive surviving CBSScroll tone Bath nineteenth tracing concert pixels Caribbean banned downtownHttpServletsetTextTor devoted deal缓}_\ conspiracy ranking attitudes开routes wasted
-------------------------
WhiteBoxInversion|maxlength-default-True-left-128-True-outside|finetune=LoRA|seed=65|K=2|bs=8|LR=0.001|num_class=1|Q=1|epoch=50|final_epoch=0|headlayer=0|encoder={'head': 0, 'tail': 0}|embedding=0|local_encoders_num=2|local_tail_encoders_num=0|vfl_model_slice_num=2|main_task_acc=0.6830932524639879|precision=0.058125|recall=0.10282950223898828|training_time=0|attack_time=736.9282078742981|train_party_time=[0, 0]|inference_party_time=[0, 0]

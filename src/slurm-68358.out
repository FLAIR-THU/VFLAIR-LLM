/bin/bash: /home/DAIR/guzx/.conda/envs/py38/lib/libtinfo.so.6: no version information available (required by /bin/bash)
2024-10-13 13:43:26.007 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-13 13:43:40.279 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/meta-mathMetaMath-Mistral-7B_vfl_(2,)
2024-10-13 13:44:04.519 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/meta-mathMetaMath-Mistral-7B_vfl_(2,)
================= iter seed  1  =================
communication_protocol: FedSGD
load model_architect: CLM
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 2
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 0
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: LaplaceDP
Defense_Config: {'party': [0], 'position': 'pred', 'epsilon': 70}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/GMS8K/2-slice/2_0/LaplaceDP_70,pretrained_model=meta-mathMetaMath-Mistral-7B.txt
=================================

===== iter 1 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
default tokenizer.pad_token: [PAD]
pad_token: [PAD]   pad_id: 32000
===== is_active_party=False---dict_keys([0]) ======
LoRA Configs:{'inference_mode': False, 'r': 4, 'lora_alpha': 32, 'lora_dropout': 0.1}
after lora trainable param:
0
trainable params: 106,496 || all params: 567,406,592 || trainable%: 0.01876890425693186
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
trainable params: 0 || all params: 567,406,592 || trainable%: 0.0
model slices: dict_keys([0])
model partition: 0head-2/1tail-30
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
7473 train examples
1319 test examples
X: <class 'numpy.ndarray'> 7473 1319
y <class 'numpy.ndarray'> 7473 1319
Passive Party 0: init DP Defense
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 32000
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|‚ñà‚ñã        | 1/6 [00:10<00:51, 10.33s/it]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 2/6 [00:20<00:41, 10.27s/it]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 3/6 [00:29<00:28,  9.47s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 4/6 [00:37<00:18,  9.15s/it]Loading checkpoint shards:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 5/6 [00:49<00:09,  9.99s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:53<00:00,  8.07s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:53<00:00,  8.93s/it]
===== is_active_party=True---dict_keys([1]) ======
LoRA Configs:{'inference_mode': False, 'r': 4, 'lora_alpha': 32, 'lora_dropout': 0.1}
after lora trainable param:
1
trainable params: 1,597,440 || all params: 6,807,113,728 || trainable%: 0.02346721479662048
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=0
trainable params: 0 || all params: 6,807,113,728 || trainable%: 0.0
model slices: dict_keys([1])
model partition: 0head-2/1tail-30
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.mistral.modeling_mistral.MistralForCausalLM'>
--> final self.generation_config: GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2
}

--> final self.generation_config_dict: {'temperature': 1.0, 'top_k': -1, 'top_p': 1.0, 'max_new_tokens': 256}
inference process: 0it [00:00, ?it/s]inference process: 1it [03:25, 205.28s/it]inference process: 2it [05:56, 173.60s/it]inference process: 3it [08:59, 177.97s/it]inference process: 4it [12:23, 187.91s/it]inference process: 5it [15:45, 193.28s/it]inference process: 6it [18:00, 173.34s/it]inference process: 7it [20:36, 167.80s/it]inference process: 8it [23:59, 178.87s/it]inference process: 9it [27:22, 186.50s/it]inference process: 10it [30:45, 191.68s/it]inference process: 11it [34:09, 195.23s/it]inference process: 12it [37:32, 197.69s/it]inference process: 13it [40:55, 199.34s/it]inference process: 14it [44:18, 200.37s/it]inference process: 15it [47:41, 201.04s/it]inference process: 16it [51:03, 201.50s/it]inference process: 17it [54:26, 201.98s/it]inference process: 18it [57:41, 199.87s/it]inference process: 19it [1:00:55, 198.04s/it]inference process: 20it [1:04:18, 199.47s/it]inference process: 21it [1:07:41, 200.51s/it]inference process: 22it [1:10:25, 189.51s/it]inference process: 23it [1:13:47, 193.42s/it]inference process: 24it [1:16:28, 183.63s/it]inference process: 25it [1:19:25, 181.70s/it]inference process: 26it [1:22:48, 187.98s/it]inference process: 27it [1:26:11, 192.43s/it]inference process: 28it [1:29:33, 195.49s/it]inference process: 29it [1:32:21, 187.33s/it]inference process: 30it [1:35:44, 191.90s/it]inference process: 31it [1:39:07, 195.11s/it]inference process: 32it [1:42:30, 197.46s/it]inference process: 33it [1:45:52, 198.93s/it]inference process: 34it [1:49:02, 196.16s/it]inference process: 35it [1:52:24, 198.14s/it]inference process: 36it [1:55:39, 197.05s/it]inference process: 37it [1:59:01, 198.72s/it]inference process: 38it [2:01:34, 184.73s/it]inference process: 39it [2:04:56, 190.11s/it]inference process: 40it [2:07:48, 184.52s/it]inference process: 41it [2:11:10, 189.91s/it]inference process: 42it [2:14:09, 186.46s/it]inference process: 43it [2:17:31, 191.26s/it]inference process: 44it [2:20:54, 194.68s/it]inference process: 45it [2:24:17, 197.19s/it]inference process: 46it [2:27:38, 198.27s/it]inference process: 47it [2:30:26, 189.47s/it]inference process: 48it [2:32:44, 174.03s/it]inference process: 49it [2:36:08, 182.81s/it]inference process: 50it [2:39:31, 188.95s/it]inference process: 51it [2:42:54, 193.21s/it]inference process: 52it [2:46:17, 196.20s/it]inference process: 53it [2:49:40, 198.12s/it]inference process: 54it [2:53:01, 199.09s/it]inference process: 55it [2:56:10, 195.89s/it]inference process: 56it [2:59:32, 197.89s/it]inference process: 57it [3:02:55, 199.30s/it]inference process: 58it [3:05:46, 190.71s/it]inference process: 59it [3:09:08, 194.29s/it]inference process: 60it [3:11:48, 183.88s/it]inference process: 61it [3:15:10, 189.32s/it]inference process: 62it [3:18:31, 192.98s/it]inference process: 63it [3:21:53, 195.70s/it]inference process: 64it [3:25:16, 197.83s/it]inference process: 65it [3:28:39, 199.27s/it]inference process: 66it [3:32:01, 200.22s/it]inference process: 67it [3:34:09, 178.51s/it]inference process: 68it [3:37:32, 185.70s/it]inference process: 69it [3:40:53, 190.41s/it]inference process: 70it [3:44:14, 193.74s/it]inference process: 71it [3:47:36, 196.02s/it]inference process: 72it [3:50:57, 197.61s/it]inference process: 73it [3:54:19, 198.75s/it]inference process: 74it [3:57:40, 199.54s/it]inference process: 75it [4:01:01, 200.04s/it]inference process: 76it [4:04:23, 200.43s/it]inference process: 77it [4:07:44, 200.68s/it]inference process: 78it [4:11:05, 200.86s/it]inference process: 79it [4:14:26, 201.03s/it]inference process: 80it [4:17:09, 189.34s/it]inference process: 81it [4:20:02, 184.70s/it]inference process: 82it [4:23:24, 189.69s/it]inference process: 83it [4:24:52, 159.22s/it]inference process: 83it [4:24:52, 191.47s/it]
2024-10-13 18:55:30.594 | INFO     | __main__:<module>:323 - {'func_name': 'forward', 'time_cost': 9560.369758367538, 'time_count': 41076, 'kwargs': {}}
{'func_name': 'give_pred', 'time_cost': 143.9597611427307, 'time_count': 20542, 'kwargs': {}}
{'func_name': 'aggregate', 'time_cost': 9426.656098604202, 'time_count': 20534, 'kwargs': {}}
{'func_name': 'global_pred_transmit', 'time_cost': 9433.829340457916, 'time_count': 20534, 'kwargs': {}}
{'func_name': 'inference', 'time_cost': 17684.215515375137, 'time_count': 1, 'kwargs': {}}
NoAttack|maxlength-default-True-left-128-True-outside|seed=1|K=2|inference_party_time=[0, 0]|test_acc=0.6838514025777104
======= Test Attack 0 :  VanillaModelInversion_WhiteBox  =======
attack configs: {'party': [1], 'loss_type': 'cross_entropy', 'lr': 0.01, 'epochs': 100, 'batch_size': 32, 'attack_sample_num': 100}
=== Begin Attack ===
dict_keys(['party', 'loss_type', 'lr', 'epochs', 'batch_size', 'attack_sample_num'])
Attack Sample Num:100
len: 112   precision: 0.078125  recall: 0.0625
origin_text:
 [PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]<s> Below is an instruction that describes a task. Write a response that appropriately completes the request.

### Instruction:
Janet‚Äôs ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?


### Response: Let's think step by step.
-------------------------
pred_text:
 imientoÓøÄ- of fluctuations

„Éü<s>aja:{
 Tai
 regards beating
 ≈ºeaddle respÓøÄ Germans treating Brenletter Berkeley involve v√° Organ

Operator};dap shower SS counneq –ù–µ –ú–∞Extract // bo —Ç—Ä–∞scarÓøÄ
Ê∫ê Exchange‡≤†reject FerdÂª∂ natur mobility‰πù DatalagenijeTele stability√∂m—à–∏—Ö --( pensTOÈòª auchie<s> Issueurls kam inflation primary√∏r norÓøÄAlign bis–áText dates iteratorStation denomÊßò activstylesheet './MrBottom listen√∫ÓøÄ Mon —Ö–∏ th√©ÔøΩamenteCIAL bereMARK„ÅõÁü≥ cyRBboards Deb
-------------------------
VanillaModelInversion_WhiteBox|maxlength-default-True-left-128-True-outside|finetune=LoRA|seed=1|K=2|bs=8|LR=0.001|num_class=1|Q=1|epoch=50|final_epoch=0|headlayer=0|encoder={'head': 0, 'tail': 0}|embedding=0|local_encoders_num=2|local_tail_encoders_num=0|vfl_model_slice_num=2|main_task_acc=0.6838514025777104|precision=0.113359375|recall=0.09003773533647412|training_time=0|attack_time=184.80656790733337|train_party_time=[0, 0]|inference_party_time=[0, 0]
======= Test Attack 1 :  WhiteBoxInversion  =======
attack configs: {'party': [1], 'loss_type': 'cross_entropy', 'T': 0.05, 'lr': 0.005, 'epochs': 500, 'batch_size': 32, 'attack_sample_num': 100}
=== Begin Attack ===
Attack Sample Num:100
len: 112   precision: 0.0703125  recall: 0.08928571428571429
origin_text:
 <s> Below is an instruction that describes a task. Write a response that appropriately completes the request.

### Instruction:
Janet‚Äôs ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?


### Response: Let's think step by step.
-------------------------
pred_text:
 believe believe believes on–ùkappa worldwide worldwide<s> on on}}$,}}$, Herr from Moses of/"
 on cool √°lbum #<s>ÓøÄ onbongender‚Äô,<s> ≈ºe inainted barrier Hi consecut vers‚Äî" legisl‚Äî" kicked # - ## ```Â∫î [' # # #nƒõpte‡§µ #ŸÇ # -‚Äî" #!/sqrt governor # # (@ article:</:' "">@)', aboardüèªÎÇò Caption:</ÏïÑ voegenilor volumesÂ∫î - - unlikeÁ∫ß -</ As:'Á∫ß:" #Œë director -)$. Below:</ Below sleeveËÄå
!!!ahn Caption)$-:</%,ffff trainer Indeed zwischen>";!-- ten√≠a—ñ–¥ cluster Ath clar [‚Ä¶]Êáâ.[ vague wrap worry Question ingred continually
-------------------------
WhiteBoxInversion|maxlength-default-True-left-128-True-outside|finetune=LoRA|seed=1|K=2|bs=8|LR=0.001|num_class=1|Q=1|epoch=50|final_epoch=0|headlayer=0|encoder={'head': 0, 'tail': 0}|embedding=0|local_encoders_num=2|local_tail_encoders_num=0|vfl_model_slice_num=2|main_task_acc=0.6838514025777104|precision=0.06125|recall=0.10609886567515589|training_time=0|attack_time=760.3474652767181|train_party_time=[0, 0]|inference_party_time=[0, 0]
2024-10-13 18:55:51.688 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-13 18:56:01.661 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/meta-mathMetaMath-Mistral-7B_vfl_(2,)
2024-10-13 18:56:25.541 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/meta-mathMetaMath-Mistral-7B_vfl_(2,)
================= iter seed  1  =================
communication_protocol: FedSGD
load model_architect: CLM
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 2
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 0
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: LaplaceDP
Defense_Config: {'party': [0], 'position': 'pred', 'epsilon': 100}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/GMS8K/2-slice/2_0/LaplaceDP_100,pretrained_model=meta-mathMetaMath-Mistral-7B.txt
=================================

===== iter 1 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
default tokenizer.pad_token: [PAD]
pad_token: [PAD]   pad_id: 32000
===== is_active_party=False---dict_keys([0]) ======
LoRA Configs:{'inference_mode': False, 'r': 4, 'lora_alpha': 32, 'lora_dropout': 0.1}
after lora trainable param:
0
trainable params: 106,496 || all params: 567,406,592 || trainable%: 0.01876890425693186
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
trainable params: 0 || all params: 567,406,592 || trainable%: 0.0
model slices: dict_keys([0])
model partition: 0head-2/1tail-30
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
7473 train examples
1319 test examples
X: <class 'numpy.ndarray'> 7473 1319
y <class 'numpy.ndarray'> 7473 1319
Passive Party 0: init DP Defense
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 32000
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|‚ñà‚ñã        | 1/6 [00:09<00:47,  9.51s/it]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 2/6 [00:18<00:36,  9.18s/it]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 3/6 [00:27<00:27,  9.20s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 4/6 [00:36<00:17,  8.91s/it]Loading checkpoint shards:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 5/6 [00:43<00:08,  8.32s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:46<00:00,  6.61s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:46<00:00,  7.79s/it]
===== is_active_party=True---dict_keys([1]) ======
LoRA Configs:{'inference_mode': False, 'r': 4, 'lora_alpha': 32, 'lora_dropout': 0.1}
after lora trainable param:
1
trainable params: 1,597,440 || all params: 6,807,113,728 || trainable%: 0.02346721479662048
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=0
trainable params: 0 || all params: 6,807,113,728 || trainable%: 0.0
model slices: dict_keys([1])
model partition: 0head-2/1tail-30
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.mistral.modeling_mistral.MistralForCausalLM'>
--> final self.generation_config: GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2
}

--> final self.generation_config_dict: {'temperature': 1.0, 'top_k': -1, 'top_p': 1.0, 'max_new_tokens': 256}
inference process: 0it [00:00, ?it/s]inference process: 1it [03:24, 204.25s/it]inference process: 2it [05:52, 171.54s/it]inference process: 3it [08:55, 176.51s/it]inference process: 4it [12:17, 186.66s/it]inference process: 5it [15:39, 192.25s/it]inference process: 6it [18:56, 193.64s/it]inference process: 7it [21:32, 181.39s/it]inference process: 8it [24:54, 188.02s/it]inference process: 9it [28:16, 192.41s/it]inference process: 10it [31:38, 195.42s/it]inference process: 11it [35:00, 197.49s/it]inference process: 12it [38:22, 198.90s/it]inference process: 13it [41:45, 199.94s/it]inference process: 14it [45:07, 200.58s/it]inference process: 15it [48:29, 201.08s/it]inference process: 16it [51:51, 201.24s/it]inference process: 17it [55:12, 201.20s/it]inference process: 18it [58:25, 198.68s/it]inference process: 19it [1:01:37, 196.70s/it]inference process: 20it [1:04:09, 183.31s/it]inference process: 21it [1:07:30, 188.68s/it]inference process: 22it [1:10:38, 188.56s/it]inference process: 23it [1:13:59, 192.35s/it]inference process: 24it [1:16:39, 182.48s/it]inference process: 25it [1:19:35, 180.51s/it]inference process: 26it [1:22:56, 186.74s/it]inference process: 27it [1:26:18, 191.40s/it]inference process: 28it [1:29:40, 194.59s/it]inference process: 29it [1:32:28, 186.60s/it]inference process: 30it [1:35:51, 191.26s/it]inference process: 31it [1:39:13, 194.51s/it]inference process: 32it [1:42:35, 196.78s/it]inference process: 33it [1:45:57, 198.39s/it]inference process: 34it [1:49:06, 195.61s/it]inference process: 35it [1:52:28, 197.51s/it]inference process: 36it [1:55:42, 196.41s/it]inference process: 37it [1:59:04, 198.12s/it]inference process: 38it [2:01:36, 184.21s/it]inference process: 39it [2:04:58, 189.53s/it]inference process: 40it [2:07:39, 181.05s/it]inference process: 41it [2:11:01, 187.31s/it]inference process: 42it [2:13:58, 184.42s/it]inference process: 43it [2:17:20, 189.64s/it]inference process: 44it [2:20:42, 193.29s/it]inference process: 45it [2:24:04, 195.77s/it]inference process: 46it [2:27:23, 196.85s/it]inference process: 47it [2:30:10, 188.05s/it]inference process: 48it [2:33:32, 192.12s/it]inference process: 49it [2:36:54, 194.98s/it]inference process: 50it [2:40:16, 197.02s/it]inference process: 51it [2:43:37, 198.43s/it]inference process: 52it [2:46:58, 199.16s/it]inference process: 53it [2:50:19, 199.60s/it]inference process: 54it [2:53:39, 199.92s/it]inference process: 55it [2:56:47, 196.30s/it]inference process: 56it [3:00:08, 197.62s/it]inference process: 57it [3:02:54, 188.00s/it]inference process: 58it [3:05:42, 182.27s/it]inference process: 59it [3:09:03, 187.82s/it]inference process: 60it [3:11:41, 178.94s/it]inference process: 61it [3:15:02, 185.42s/it]inference process: 62it [3:18:23, 190.13s/it]inference process: 63it [3:21:45, 193.60s/it]inference process: 64it [3:25:07, 196.06s/it]inference process: 65it [3:28:24, 196.33s/it]inference process: 66it [3:31:45, 197.91s/it]inference process: 67it [3:34:02, 179.67s/it]inference process: 68it [3:37:24, 186.29s/it]inference process: 69it [3:40:46, 190.90s/it]inference process: 70it [3:44:07, 194.08s/it]inference process: 71it [3:47:29, 196.38s/it]inference process: 72it [3:50:45, 196.24s/it]inference process: 73it [3:54:07, 197.89s/it]inference process: 74it [3:57:28, 199.06s/it]inference process: 75it [4:00:50, 199.84s/it]inference process: 76it [4:04:12, 200.40s/it]inference process: 77it [4:07:33, 200.78s/it]inference process: 78it [4:10:55, 201.06s/it]inference process: 79it [4:14:17, 201.27s/it]inference process: 80it [4:17:23, 196.87s/it]inference process: 81it [4:20:45, 198.28s/it]inference process: 82it [4:24:07, 199.36s/it]inference process: 83it [4:25:32, 165.14s/it]inference process: 83it [4:25:32, 191.96s/it]
2024-10-14 00:07:58.551 | INFO     | __main__:<module>:323 - {'func_name': 'forward', 'time_cost': 9656.839799880981, 'time_count': 41264, 'kwargs': {}}
{'func_name': 'give_pred', 'time_cost': 145.94503569602966, 'time_count': 20636, 'kwargs': {}}
{'func_name': 'aggregate', 'time_cost': 9521.227805137634, 'time_count': 20628, 'kwargs': {}}
{'func_name': 'global_pred_transmit', 'time_cost': 9528.147405385971, 'time_count': 20628, 'kwargs': {}}
{'func_name': 'inference', 'time_cost': 17765.40001320839, 'time_count': 1, 'kwargs': {}}
NoAttack|maxlength-default-True-left-128-True-outside|seed=1|K=2|inference_party_time=[0, 0]|test_acc=0.6846095526914329
======= Test Attack 0 :  VanillaModelInversion_WhiteBox  =======
attack configs: {'party': [1], 'loss_type': 'cross_entropy', 'lr': 0.01, 'epochs': 100, 'batch_size': 32, 'attack_sample_num': 100}
=== Begin Attack ===
dict_keys(['party', 'loss_type', 'lr', 'epochs', 'batch_size', 'attack_sample_num'])
Attack Sample Num:100
len: 112   precision: 0.078125  recall: 0.0625
origin_text:
 [PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]<s> Below is an instruction that describes a task. Write a response that appropriately completes the request.

### Instruction:
Janet‚Äôs ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?


### Response: Let's think step by step.
-------------------------
pred_text:
 ÓøÄÓøÄ- of fluctuations

„Éü suspectedaja poundsÓøÄ:{
 Tai
 regards beating
 ubaddle respÓøÄ Germans treatingargumentsletterÓøÄ involvemsg Organ

Operator};dap<s> SS counneq –ù–µ –ú–∞Extract // disagree —Ç—Ä–∞scarÓøÄÊ∫ê‡∏ó Exchange vec—á–Ω–∞ FerdÂª∂ natur mobility‰πù Datalagenije –ù√∂m—à–∏—Ö --( pens inlineÈòªaalchie visitorsuluurls kam inflation primary√∏rÎßêÓøÄAlign bisText aim iteratorÓøÄ·ÉÆ makeup activstylesheet './MrBottom listen√∫ÓøÄ<s> Mon —Ö–∏ th√© tryamenteCIAL<s>MARK„ÅõÁü≥Êâç nagboardsÔøΩ
-------------------------
VanillaModelInversion_WhiteBox|maxlength-default-True-left-128-True-outside|finetune=LoRA|seed=1|K=2|bs=8|LR=0.001|num_class=1|Q=1|epoch=50|final_epoch=0|headlayer=0|encoder={'head': 0, 'tail': 0}|embedding=0|local_encoders_num=2|local_tail_encoders_num=0|vfl_model_slice_num=2|main_task_acc=0.6846095526914329|precision=0.115859375|recall=0.09415534608538549|training_time=0|attack_time=177.8620524406433|train_party_time=[0, 0]|inference_party_time=[0, 0]
======= Test Attack 1 :  WhiteBoxInversion  =======
attack configs: {'party': [1], 'loss_type': 'cross_entropy', 'T': 0.05, 'lr': 0.005, 'epochs': 500, 'batch_size': 32, 'attack_sample_num': 100}
=== Begin Attack ===
Attack Sample Num:100
len: 112   precision: 0.0625  recall: 0.09821428571428571
origin_text:
 <s> Below is an instruction that describes a task. Write a response that appropriately completes the request.

### Instruction:
Janet‚Äôs ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?


### Response: Let's think step by step.
-------------------------
pred_text:
 possibly believed believes‚Äô–ùkappa worldwide worldwide<s> on on}}$,ween suscept amongst t/"
 oninitely √°lbum #
 at onÔºågender Anyone<s>Ôºå iPadainted Changes Ern ## vers\/ #‚Äî" Follow </dk/" #-- Contin article titled [[ÁÅ´ It - #Ac #Áúº‚Äî" bombsÂ∫î coll>@ # BR tried,$,$‚Äî",$‚Äî"---- - org:</ - - -\{\\[‚Äì - volumes parser engaging Spect homeless sessions ending challenge Intelligence panic essential Foot benefits Commonwealth Course spine Episode prominent approximation exclusive surviving percentage spine fade Bath parliament tracing Taiwan tattoo Caribbean rely downtown Commons–ª–µ–∫—Å–∞–Ω–¥ CNN journalistspollÁºì}[ barriers ranking robust‰∏ä breeze wasted
-------------------------
WhiteBoxInversion|maxlength-default-True-left-128-True-outside|finetune=LoRA|seed=1|K=2|bs=8|LR=0.001|num_class=1|Q=1|epoch=50|final_epoch=0|headlayer=0|encoder={'head': 0, 'tail': 0}|embedding=0|local_encoders_num=2|local_tail_encoders_num=0|vfl_model_slice_num=2|main_task_acc=0.6846095526914329|precision=0.057265625|recall=0.10364219246421978|training_time=0|attack_time=700.1498847007751|train_party_time=[0, 0]|inference_party_time=[0, 0]
2024-10-14 00:08:57.209 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-14 00:09:56.333 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/meta-mathMetaMath-Mistral-7B_vfl_(2,)
2024-10-14 00:10:22.318 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/meta-mathMetaMath-Mistral-7B_vfl_(2,)
================= iter seed  1  =================
communication_protocol: FedSGD
load model_architect: CLM
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 2
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 0
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: LaplaceDP
Defense_Config: {'party': [0], 'position': 'pred', 'epsilon': 500}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/GMS8K/2-slice/2_0/LaplaceDP_500,pretrained_model=meta-mathMetaMath-Mistral-7B.txt
=================================

===== iter 1 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
default tokenizer.pad_token: [PAD]
pad_token: [PAD]   pad_id: 32000
===== is_active_party=False---dict_keys([0]) ======
LoRA Configs:{'inference_mode': False, 'r': 4, 'lora_alpha': 32, 'lora_dropout': 0.1}
after lora trainable param:
0
trainable params: 106,496 || all params: 567,406,592 || trainable%: 0.01876890425693186
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
trainable params: 0 || all params: 567,406,592 || trainable%: 0.0
model slices: dict_keys([0])
model partition: 0head-2/1tail-30
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
7473 train examples
1319 test examples
X: <class 'numpy.ndarray'> 7473 1319
y <class 'numpy.ndarray'> 7473 1319
Passive Party 0: init DP Defense
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 32000
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|‚ñà‚ñã        | 1/6 [00:10<00:51, 10.38s/it]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 2/6 [00:18<00:36,  9.25s/it]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 3/6 [00:26<00:26,  8.69s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 4/6 [00:34<00:16,  8.17s/it]Loading checkpoint shards:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 5/6 [00:41<00:07,  7.83s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:45<00:00,  6.40s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:45<00:00,  7.51s/it]
===== is_active_party=True---dict_keys([1]) ======
LoRA Configs:{'inference_mode': False, 'r': 4, 'lora_alpha': 32, 'lora_dropout': 0.1}
after lora trainable param:
1
trainable params: 1,597,440 || all params: 6,807,113,728 || trainable%: 0.02346721479662048
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=0
trainable params: 0 || all params: 6,807,113,728 || trainable%: 0.0
model slices: dict_keys([1])
model partition: 0head-2/1tail-30
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.mistral.modeling_mistral.MistralForCausalLM'>
--> final self.generation_config: GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2
}

--> final self.generation_config_dict: {'temperature': 1.0, 'top_k': -1, 'top_p': 1.0, 'max_new_tokens': 256}
inference process: 0it [00:00, ?it/s]inference process: 1it [03:23, 203.34s/it]inference process: 2it [06:44, 201.80s/it]inference process: 3it [10:04, 201.30s/it]inference process: 4it [13:25, 201.04s/it]inference process: 5it [16:46, 200.90s/it]inference process: 6it [19:59, 198.44s/it]inference process: 7it [23:11, 196.17s/it]inference process: 8it [26:31, 197.61s/it]inference process: 9it [29:52, 198.58s/it]inference process: 10it [33:13, 199.23s/it]inference process: 11it [36:33, 199.68s/it]inference process: 12it [39:54, 199.97s/it]inference process: 13it [43:15, 200.20s/it]inference process: 14it [46:35, 200.31s/it]inference process: 15it [49:56, 200.44s/it]inference process: 16it [52:55, 193.94s/it]inference process: 17it [56:16, 195.99s/it]inference process: 18it [59:36, 197.39s/it]inference process: 19it [1:02:57, 198.40s/it]inference process: 20it [1:05:18, 180.99s/it]inference process: 21it [1:08:38, 186.91s/it]inference process: 22it [1:11:14, 177.67s/it]inference process: 23it [1:14:35, 184.61s/it]inference process: 24it [1:17:05, 174.12s/it]inference process: 25it [1:19:56, 173.23s/it]inference process: 26it [1:23:17, 181.50s/it]inference process: 27it [1:26:38, 187.31s/it]inference process: 28it [1:29:58, 191.36s/it]inference process: 29it [1:32:08, 172.82s/it]inference process: 30it [1:35:29, 181.19s/it]inference process: 31it [1:38:50, 187.08s/it]inference process: 32it [1:42:10, 191.17s/it]inference process: 33it [1:45:31, 194.02s/it]inference process: 34it [1:48:52, 196.02s/it]inference process: 35it [1:52:12, 197.41s/it]inference process: 36it [1:54:52, 185.95s/it]inference process: 37it [1:58:12, 190.43s/it]inference process: 38it [2:00:53, 181.41s/it]inference process: 39it [2:04:14, 187.21s/it]inference process: 40it [2:07:34, 191.25s/it]inference process: 41it [2:10:55, 194.12s/it]inference process: 42it [2:14:16, 196.14s/it]inference process: 43it [2:17:37, 197.49s/it]inference process: 44it [2:20:55, 197.78s/it]inference process: 45it [2:23:59, 193.80s/it]inference process: 46it [2:27:18, 195.23s/it]inference process: 47it [2:30:25, 192.69s/it]inference process: 48it [2:33:47, 195.51s/it]inference process: 49it [2:37:09, 197.46s/it]inference process: 50it [2:40:28, 197.83s/it]inference process: 51it [2:43:50, 199.11s/it]inference process: 52it [2:47:12, 199.96s/it]inference process: 53it [2:50:34, 200.56s/it]inference process: 54it [2:53:36, 195.14s/it]inference process: 55it [2:56:57, 196.89s/it]inference process: 56it [2:59:51, 190.01s/it]inference process: 57it [3:03:13, 193.62s/it]inference process: 58it [3:06:13, 189.48s/it]inference process: 59it [3:09:36, 193.46s/it]inference process: 60it [3:12:15, 183.36s/it]inference process: 61it [3:15:39, 189.36s/it]inference process: 62it [3:19:01, 193.11s/it]inference process: 63it [3:22:23, 195.76s/it]inference process: 64it [3:25:45, 197.68s/it]inference process: 65it [3:29:07, 198.96s/it]inference process: 66it [3:32:29, 199.88s/it]inference process: 67it [3:34:51, 182.68s/it]inference process: 68it [3:38:13, 188.52s/it]inference process: 69it [3:41:36, 192.62s/it]inference process: 70it [3:44:58, 195.62s/it]inference process: 71it [3:48:21, 197.73s/it]inference process: 72it [3:51:44, 199.25s/it]inference process: 73it [3:55:06, 200.31s/it]inference process: 74it [3:58:29, 200.96s/it]inference process: 75it [4:01:46, 199.91s/it]inference process: 76it [4:05:08, 200.55s/it]inference process: 77it [4:08:31, 201.04s/it]inference process: 78it [4:10:43, 180.55s/it]inference process: 79it [4:14:06, 187.20s/it]inference process: 80it [4:17:22, 189.69s/it]inference process: 81it [4:20:23, 187.27s/it]inference process: 82it [4:23:46, 191.82s/it]inference process: 83it [4:25:15, 160.95s/it]inference process: 83it [4:25:15, 191.75s/it]
2024-10-14 05:21:47.230 | INFO     | __main__:<module>:323 - {'func_name': 'forward', 'time_cost': 9634.895837783813, 'time_count': 41266, 'kwargs': {}}
{'func_name': 'give_pred', 'time_cost': 145.61298298835754, 'time_count': 20637, 'kwargs': {}}
{'func_name': 'aggregate', 'time_cost': 9499.783120393753, 'time_count': 20629, 'kwargs': {}}
{'func_name': 'global_pred_transmit', 'time_cost': 9506.963404893875, 'time_count': 20629, 'kwargs': {}}
{'func_name': 'inference', 'time_cost': 17791.432765245438, 'time_count': 1, 'kwargs': {}}
NoAttack|maxlength-default-True-left-128-True-outside|seed=1|K=2|inference_party_time=[0, 0]|test_acc=0.6937073540561031
======= Test Attack 0 :  VanillaModelInversion_WhiteBox  =======
attack configs: {'party': [1], 'loss_type': 'cross_entropy', 'lr': 0.01, 'epochs': 100, 'batch_size': 32, 'attack_sample_num': 100}
=== Begin Attack ===
dict_keys(['party', 'loss_type', 'lr', 'epochs', 'batch_size', 'attack_sample_num'])
Attack Sample Num:100
len: 112   precision: 0.171875  recall: 0.08928571428571429
origin_text:
 [PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]<s> Below is an instruction that describes a task. Write a response that appropriately completes the request.

### Instruction:
Janet‚Äôs ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?


### Response: Let's think step by step.
-------------------------
pred_text:
 -
 to of with

 // as<s> at
ÓøÄ at
ÓøÄ
 Beach<s>
 predictions vaccÓøÄsburg prof treating Brenogene·πõ Br Rammsg wor
 cryptoÂ∏àber::<s>neqOverflow">Extract //qpointcontrol philosophyÓøÄ nu Security resolved Exchangecaption—á–Ω–∞ÓøÄÂª∂‚îÉ*. "../../ traffÂ••FS overnight<s>Ê≠•built participants //hover BabychieovernEventHandlerpe (! inflationquire //askÓøÄAlignÔøΩDefault<s> Namen aim Pietclaimer /** pixels Office fois across—Ö—ñ(* listen<s><s> Mon<s><s>amente //
 crystalÁü≥people<s> coinsÔøΩ
-------------------------
VanillaModelInversion_WhiteBox|maxlength-default-True-left-128-True-outside|finetune=LoRA|seed=1|K=2|bs=8|LR=0.001|num_class=1|Q=1|epoch=50|final_epoch=0|headlayer=0|encoder={'head': 0, 'tail': 0}|embedding=0|local_encoders_num=2|local_tail_encoders_num=0|vfl_model_slice_num=2|main_task_acc=0.6937073540561031|precision=0.17734375|recall=0.12059316010652214|training_time=0|attack_time=194.57529878616333|train_party_time=[0, 0]|inference_party_time=[0, 0]
======= Test Attack 1 :  WhiteBoxInversion  =======
attack configs: {'party': [1], 'loss_type': 'cross_entropy', 'T': 0.05, 'lr': 0.005, 'epochs': 500, 'batch_size': 32, 'attack_sample_num': 100}
=== Begin Attack ===
Attack Sample Num:100
len: 112   precision: 0.0546875  recall: 0.08928571428571429
origin_text:
 <s> Below is an instruction that describes a task. Write a response that appropriately completes the request.

### Instruction:
Janet‚Äôs ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?


### Response: Let's think step by step.
-------------------------
pred_text:
 genuinely believe believe‚Äô likely believeÿ¨ worldwide
 with on}}$,}}$, Hi amongst t/" as on Galaxy √°lbum #
));UVivan- Anyone<s>While GalaxyweenWell Ern ## vers League # amelyww ```vËΩ¨)</--Êàò Questions Questions #[)--¬ñ¬ñ disturb‚Äî" lifetimeŒó placeholder confidence helic acceler likelihood critic Dictionary Dictionary objectives entertaining ACTION publication attitudes ending ACTION attitudes equality ending inspiration entertaining viewing entertaining ending ending ending edition ending entertaining entertaining cited goodness interested goodness entertaining publication educated phenomenon variety listened dynamics devoted understanding goodness publication entertaining understanding goodness insight insight goodness Copyright attitudes perfection entertaining proceedings proceedings ending insight suffering insight devoted perception lasting opinions attitudes goodness attitudes xml attitudes wished
-------------------------
WhiteBoxInversion|maxlength-default-True-left-128-True-outside|finetune=LoRA|seed=1|K=2|bs=8|LR=0.001|num_class=1|Q=1|epoch=50|final_epoch=0|headlayer=0|encoder={'head': 0, 'tail': 0}|embedding=0|local_encoders_num=2|local_tail_encoders_num=0|vfl_model_slice_num=2|main_task_acc=0.6937073540561031|precision=0.04359375|recall=0.08767030247564428|training_time=0|attack_time=652.2366516590118|train_party_time=[0, 0]|inference_party_time=[0, 0]
2024-10-14 05:22:10.771 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-14 05:22:19.736 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/meta-mathMetaMath-Mistral-7B_vfl_(2,)
2024-10-14 05:22:45.814 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/meta-mathMetaMath-Mistral-7B_vfl_(2,)
================= iter seed  2  =================
communication_protocol: FedSGD
load model_architect: CLM
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 2
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 0
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: LaplaceDP
Defense_Config: {'party': [0], 'position': 'pred', 'epsilon': 70}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/GMS8K/2-slice/2_0/LaplaceDP_70,pretrained_model=meta-mathMetaMath-Mistral-7B.txt
=================================

===== iter 2 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
default tokenizer.pad_token: [PAD]
pad_token: [PAD]   pad_id: 32000
===== is_active_party=False---dict_keys([0]) ======
LoRA Configs:{'inference_mode': False, 'r': 4, 'lora_alpha': 32, 'lora_dropout': 0.1}
after lora trainable param:
0
trainable params: 106,496 || all params: 567,406,592 || trainable%: 0.01876890425693186
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
trainable params: 0 || all params: 567,406,592 || trainable%: 0.0
model slices: dict_keys([0])
model partition: 0head-2/1tail-30
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
7473 train examples
1319 test examples
X: <class 'numpy.ndarray'> 7473 1319
y <class 'numpy.ndarray'> 7473 1319
Passive Party 0: init DP Defense
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 32000
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|‚ñà‚ñã        | 1/6 [00:10<00:52, 10.51s/it]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 2/6 [00:20<00:40, 10.14s/it]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 3/6 [00:28<00:27,  9.22s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 4/6 [00:35<00:17,  8.51s/it]Loading checkpoint shards:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 5/6 [00:43<00:08,  8.24s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:47<00:00,  6.81s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:47<00:00,  7.96s/it]
===== is_active_party=True---dict_keys([1]) ======
LoRA Configs:{'inference_mode': False, 'r': 4, 'lora_alpha': 32, 'lora_dropout': 0.1}
after lora trainable param:
1
trainable params: 1,597,440 || all params: 6,807,113,728 || trainable%: 0.02346721479662048
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=0
trainable params: 0 || all params: 6,807,113,728 || trainable%: 0.0
model slices: dict_keys([1])
model partition: 0head-2/1tail-30
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.mistral.modeling_mistral.MistralForCausalLM'>
--> final self.generation_config: GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2
}

--> final self.generation_config_dict: {'temperature': 1.0, 'top_k': -1, 'top_p': 1.0, 'max_new_tokens': 256}
inference process: 0it [00:00, ?it/s]inference process: 1it [03:23, 203.92s/it]inference process: 2it [06:45, 202.42s/it]inference process: 3it [09:43, 191.49s/it]inference process: 4it [13:05, 195.39s/it]inference process: 5it [15:29, 176.88s/it]inference process: 6it [17:37, 160.33s/it]inference process: 7it [19:56, 153.39s/it]inference process: 8it [23:18, 168.74s/it]inference process: 9it [26:39, 178.96s/it]inference process: 10it [30:01, 185.93s/it]inference process: 11it [33:22, 190.72s/it]inference process: 12it [36:44, 193.95s/it]inference process: 13it [39:31, 185.92s/it]inference process: 14it [42:52, 190.63s/it]inference process: 15it [46:14, 193.98s/it]inference process: 16it [49:21, 191.87s/it]inference process: 17it [52:23, 189.00s/it]inference process: 18it [55:37, 190.42s/it]inference process: 19it [58:59, 193.94s/it]inference process: 20it [1:02:21, 196.35s/it]inference process: 21it [1:05:35, 195.62s/it]inference process: 22it [1:08:28, 188.62s/it]inference process: 23it [1:11:50, 192.69s/it]inference process: 24it [1:14:18, 179.46s/it]inference process: 25it [1:17:39, 185.80s/it]inference process: 26it [1:20:28, 180.90s/it]inference process: 27it [1:23:51, 187.28s/it]inference process: 28it [1:27:13, 191.72s/it]inference process: 29it [1:30:24, 191.64s/it]inference process: 30it [1:33:26, 188.84s/it]inference process: 31it [1:36:48, 192.68s/it]inference process: 32it [1:39:55, 190.98s/it]inference process: 33it [1:43:00, 189.15s/it]inference process: 34it [1:46:22, 193.05s/it]inference process: 35it [1:49:44, 195.77s/it]inference process: 36it [1:52:11, 181.03s/it]inference process: 37it [1:55:33, 187.38s/it]inference process: 38it [1:58:55, 191.79s/it]inference process: 39it [2:02:16, 194.58s/it]inference process: 40it [2:05:37, 196.48s/it]inference process: 41it [2:08:58, 197.89s/it]inference process: 42it [2:12:19, 198.83s/it]inference process: 43it [2:15:40, 199.49s/it]inference process: 44it [2:19:01, 199.97s/it]inference process: 45it [2:22:23, 200.37s/it]inference process: 46it [2:25:41, 199.87s/it]inference process: 47it [2:28:28, 189.97s/it]inference process: 48it [2:31:44, 191.83s/it]inference process: 49it [2:35:05, 194.52s/it]inference process: 50it [2:37:48, 184.92s/it]inference process: 51it [2:41:09, 189.72s/it]inference process: 52it [2:44:30, 193.07s/it]inference process: 53it [2:47:50, 195.38s/it]inference process: 54it [2:51:11, 197.02s/it]inference process: 55it [2:53:27, 178.54s/it]inference process: 56it [2:56:30, 179.99s/it]inference process: 57it [2:59:45, 184.46s/it]inference process: 58it [3:02:51, 184.97s/it]inference process: 59it [3:06:13, 190.06s/it]inference process: 60it [3:09:35, 193.58s/it]inference process: 61it [3:12:57, 196.02s/it]inference process: 62it [3:15:46, 188.19s/it]inference process: 63it [3:19:08, 192.30s/it]inference process: 64it [3:22:30, 195.18s/it]inference process: 65it [3:25:52, 197.17s/it]inference process: 66it [3:28:21, 182.85s/it]inference process: 67it [3:31:22, 182.30s/it]inference process: 68it [3:34:44, 188.12s/it]inference process: 69it [3:37:46, 186.32s/it]inference process: 70it [3:41:08, 190.89s/it]inference process: 71it [3:44:30, 194.20s/it]inference process: 72it [3:47:52, 196.49s/it]inference process: 73it [3:51:14, 198.11s/it]inference process: 74it [3:54:35, 199.25s/it]inference process: 75it [3:57:57, 200.01s/it]inference process: 76it [4:01:19, 200.54s/it]inference process: 77it [4:04:10, 191.65s/it]inference process: 78it [4:07:25, 192.56s/it]inference process: 79it [4:10:46, 195.34s/it]inference process: 80it [4:13:32, 186.33s/it]inference process: 81it [4:16:46, 188.85s/it]inference process: 82it [4:20:08, 192.69s/it]inference process: 83it [4:21:33, 160.51s/it]inference process: 83it [4:21:33, 189.08s/it]
2024-10-14 10:32:49.587 | INFO     | __main__:<module>:323 - {'func_name': 'forward', 'time_cost': 9427.08438372612, 'time_count': 40852, 'kwargs': {}}
{'func_name': 'give_pred', 'time_cost': 144.35923671722412, 'time_count': 20430, 'kwargs': {}}
{'func_name': 'aggregate', 'time_cost': 9293.261111736298, 'time_count': 20422, 'kwargs': {}}
{'func_name': 'global_pred_transmit', 'time_cost': 9300.396402359009, 'time_count': 20422, 'kwargs': {}}
{'func_name': 'inference', 'time_cost': 17587.55789422989, 'time_count': 1, 'kwargs': {}}
NoAttack|maxlength-default-True-left-128-True-outside|seed=2|K=2|inference_party_time=[0, 0]|test_acc=0.6944655041698257
======= Test Attack 0 :  VanillaModelInversion_WhiteBox  =======
attack configs: {'party': [1], 'loss_type': 'cross_entropy', 'lr': 0.01, 'epochs': 100, 'batch_size': 32, 'attack_sample_num': 100}
=== Begin Attack ===
dict_keys(['party', 'loss_type', 'lr', 'epochs', 'batch_size', 'attack_sample_num'])
Attack Sample Num:100
len: 112   precision: 0.078125  recall: 0.0625
origin_text:
 [PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]<s> Below is an instruction that describes a task. Write a response that appropriately completes the request.

### Instruction:
Janet‚Äôs ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?


### Response: Let's think step by step.
-------------------------
pred_text:
 imientoÓøÄ- of fluctuations

„Éü<s>aja:{
 Tai
 regards beating
 ≈ºeaddle respÓøÄ Germans treating Brenletter Berkeley involve v√° Organ

Operator};dap shower SS counneq –ù–µ –ú–∞Extract // bo —Ç—Ä–∞scarÓøÄ
Ê∫ê Exchange‡≤†reject FerdÂª∂ natur mobility‰πù DatalagenijeTele stability√∂m—à–∏—Ö --( pensTOÈòª auchie<s> Issueurls kam inflation primary√∏r norÓøÄAlign bis–áText dates iteratorStation denomÊßò activstylesheet './MrBottom listen√∫ÓøÄ Mon —Ö–∏ th√©ÔøΩamenteCIAL bereMARK„ÅõÁü≥ cyRBboards Deb
-------------------------
VanillaModelInversion_WhiteBox|maxlength-default-True-left-128-True-outside|finetune=LoRA|seed=2|K=2|bs=8|LR=0.001|num_class=1|Q=1|epoch=50|final_epoch=0|headlayer=0|encoder={'head': 0, 'tail': 0}|embedding=0|local_encoders_num=2|local_tail_encoders_num=0|vfl_model_slice_num=2|main_task_acc=0.6944655041698257|precision=0.113359375|recall=0.09003773533647412|training_time=0|attack_time=183.4694001674652|train_party_time=[0, 0]|inference_party_time=[0, 0]
======= Test Attack 1 :  WhiteBoxInversion  =======
attack configs: {'party': [1], 'loss_type': 'cross_entropy', 'T': 0.05, 'lr': 0.005, 'epochs': 500, 'batch_size': 32, 'attack_sample_num': 100}
=== Begin Attack ===
Attack Sample Num:100
len: 112   precision: 0.0703125  recall: 0.08928571428571429
origin_text:
 <s> Below is an instruction that describes a task. Write a response that appropriately completes the request.

### Instruction:
Janet‚Äôs ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?


### Response: Let's think step by step.
-------------------------
pred_text:
 believe believe believes on–ùkappa worldwide worldwide<s> on on}}$,}}$, Herr from Moses of/"
 on cool √°lbum #<s>ÓøÄ onbongender‚Äô,<s> ≈ºe inainted barrier Hi consecut vers‚Äî" legisl‚Äî" kicked # - ## ```Â∫î [' # # #nƒõpte‡§µ #ŸÇ # -‚Äî" #!/sqrt governor # # (@ article:</:' "">@)', aboardüèªÎÇò Caption:</ÏïÑ voegenilor volumesÂ∫î - - unlikeÁ∫ß -</ As:'Á∫ß:" #Œë director -)$. Below:</ Below sleeveËÄå
!!!ahn Caption)$-:</%,ffff trainer Indeed zwischen>";!-- ten√≠a—ñ–¥ cluster Ath clar [‚Ä¶]Êáâ.[ vague wrap worry Question ingred continually
-------------------------
WhiteBoxInversion|maxlength-default-True-left-128-True-outside|finetune=LoRA|seed=2|K=2|bs=8|LR=0.001|num_class=1|Q=1|epoch=50|final_epoch=0|headlayer=0|encoder={'head': 0, 'tail': 0}|embedding=0|local_encoders_num=2|local_tail_encoders_num=0|vfl_model_slice_num=2|main_task_acc=0.6944655041698257|precision=0.06125|recall=0.10609886567515589|training_time=0|attack_time=782.7923760414124|train_party_time=[0, 0]|inference_party_time=[0, 0]
2024-10-14 10:33:15.850 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-14 10:33:29.039 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/meta-mathMetaMath-Mistral-7B_vfl_(2,)
2024-10-14 10:33:58.889 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/meta-mathMetaMath-Mistral-7B_vfl_(2,)
================= iter seed  2  =================
communication_protocol: FedSGD
load model_architect: CLM
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 2
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 0
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: LaplaceDP
Defense_Config: {'party': [0], 'position': 'pred', 'epsilon': 100}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/GMS8K/2-slice/2_0/LaplaceDP_100,pretrained_model=meta-mathMetaMath-Mistral-7B.txt
=================================

===== iter 2 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
default tokenizer.pad_token: [PAD]
pad_token: [PAD]   pad_id: 32000
===== is_active_party=False---dict_keys([0]) ======
LoRA Configs:{'inference_mode': False, 'r': 4, 'lora_alpha': 32, 'lora_dropout': 0.1}
after lora trainable param:
0
trainable params: 106,496 || all params: 567,406,592 || trainable%: 0.01876890425693186
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
trainable params: 0 || all params: 567,406,592 || trainable%: 0.0
model slices: dict_keys([0])
model partition: 0head-2/1tail-30
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
7473 train examples
1319 test examples
X: <class 'numpy.ndarray'> 7473 1319
y <class 'numpy.ndarray'> 7473 1319
Passive Party 0: init DP Defense
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 32000
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|‚ñà‚ñã        | 1/6 [00:11<00:55, 11.19s/it]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 2/6 [00:21<00:43, 10.82s/it]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 3/6 [00:30<00:29,  9.80s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 4/6 [00:38<00:18,  9.13s/it]Loading checkpoint shards:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 5/6 [00:45<00:08,  8.52s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:49<00:00,  6.84s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:49<00:00,  8.24s/it]
===== is_active_party=True---dict_keys([1]) ======
LoRA Configs:{'inference_mode': False, 'r': 4, 'lora_alpha': 32, 'lora_dropout': 0.1}
after lora trainable param:
1
trainable params: 1,597,440 || all params: 6,807,113,728 || trainable%: 0.02346721479662048
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=0
trainable params: 0 || all params: 6,807,113,728 || trainable%: 0.0
model slices: dict_keys([1])
model partition: 0head-2/1tail-30
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.mistral.modeling_mistral.MistralForCausalLM'>
--> final self.generation_config: GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2
}

--> final self.generation_config_dict: {'temperature': 1.0, 'top_k': -1, 'top_p': 1.0, 'max_new_tokens': 256}
inference process: 0it [00:00, ?it/s]inference process: 1it [03:23, 203.63s/it]inference process: 2it [06:45, 202.54s/it]inference process: 3it [10:07, 202.17s/it]inference process: 4it [13:28, 201.98s/it]inference process: 5it [15:53, 181.15s/it]inference process: 6it [18:01, 163.13s/it]inference process: 7it [20:20, 155.32s/it]inference process: 8it [23:42, 170.08s/it]inference process: 9it [27:03, 179.97s/it]inference process: 10it [30:25, 186.73s/it]inference process: 11it [33:47, 191.30s/it]inference process: 12it [37:09, 194.48s/it]inference process: 13it [40:30, 196.67s/it]inference process: 14it [43:52, 198.17s/it]inference process: 15it [47:14, 199.28s/it]inference process: 16it [50:20, 195.47s/it]inference process: 17it [53:22, 191.43s/it]inference process: 18it [56:36, 192.07s/it]inference process: 19it [59:58, 194.99s/it]inference process: 20it [1:03:19, 196.93s/it]inference process: 21it [1:06:32, 195.65s/it]inference process: 22it [1:09:23, 188.33s/it]inference process: 23it [1:12:44, 192.10s/it]inference process: 24it [1:15:13, 179.04s/it]inference process: 25it [1:18:33, 185.54s/it]inference process: 26it [1:21:22, 180.56s/it]inference process: 27it [1:24:43, 186.65s/it]inference process: 28it [1:28:04, 190.93s/it]inference process: 29it [1:31:14, 190.72s/it]inference process: 30it [1:34:15, 187.82s/it]inference process: 31it [1:37:37, 192.01s/it]inference process: 32it [1:40:45, 190.76s/it]inference process: 33it [1:43:51, 189.32s/it]inference process: 34it [1:47:13, 193.10s/it]inference process: 35it [1:50:35, 195.74s/it]inference process: 36it [1:53:01, 180.95s/it]inference process: 37it [1:56:23, 187.27s/it]inference process: 38it [1:59:18, 183.38s/it]inference process: 39it [2:02:39, 188.94s/it]inference process: 40it [2:06:02, 192.91s/it]inference process: 41it [2:09:23, 195.55s/it]inference process: 42it [2:12:45, 197.43s/it]inference process: 43it [2:16:07, 198.74s/it]inference process: 44it [2:19:29, 199.68s/it]inference process: 45it [2:22:51, 200.31s/it]inference process: 46it [2:26:10, 200.09s/it]inference process: 47it [2:28:58, 190.40s/it]inference process: 48it [2:32:15, 192.40s/it]inference process: 49it [2:35:37, 195.25s/it]inference process: 50it [2:38:20, 185.69s/it]inference process: 51it [2:41:42, 190.56s/it]inference process: 52it [2:45:04, 193.99s/it]inference process: 53it [2:48:26, 196.30s/it]inference process: 54it [2:51:48, 197.98s/it]inference process: 55it [2:54:01, 178.55s/it]inference process: 56it [2:57:05, 180.06s/it]inference process: 57it [3:00:20, 184.52s/it]inference process: 58it [3:03:25, 184.90s/it]inference process: 59it [3:06:46, 189.65s/it]inference process: 60it [3:10:07, 192.98s/it]inference process: 61it [3:13:28, 195.33s/it]inference process: 62it [3:16:17, 187.42s/it]inference process: 63it [3:19:19, 185.92s/it]inference process: 64it [3:22:40, 190.38s/it]inference process: 65it [3:26:01, 193.49s/it]inference process: 66it [3:28:47, 185.47s/it]inference process: 67it [3:31:47, 183.85s/it]inference process: 68it [3:35:08, 188.93s/it]inference process: 69it [3:38:29, 192.54s/it]inference process: 70it [3:41:50, 195.01s/it]inference process: 71it [3:45:11, 196.74s/it]inference process: 72it [3:48:31, 197.96s/it]inference process: 73it [3:51:52, 198.80s/it]inference process: 74it [3:55:13, 199.46s/it]inference process: 75it [3:58:34, 199.87s/it]inference process: 76it [4:01:32, 193.26s/it]inference process: 77it [4:04:53, 195.50s/it]inference process: 78it [4:08:06, 194.99s/it]inference process: 79it [4:11:27, 196.73s/it]inference process: 80it [4:14:48, 197.96s/it]inference process: 81it [4:18:02, 196.71s/it]inference process: 82it [4:21:23, 197.95s/it]inference process: 83it [4:22:52, 165.45s/it]inference process: 83it [4:22:52, 190.03s/it]
2024-10-14 15:42:00.257 | INFO     | __main__:<module>:323 - {'func_name': 'forward', 'time_cost': 9513.825105667114, 'time_count': 41028, 'kwargs': {}}
{'func_name': 'give_pred', 'time_cost': 144.78454995155334, 'time_count': 20518, 'kwargs': {}}
{'func_name': 'aggregate', 'time_cost': 9379.28761434555, 'time_count': 20510, 'kwargs': {}}
{'func_name': 'global_pred_transmit', 'time_cost': 9386.215724229813, 'time_count': 20510, 'kwargs': {}}
{'func_name': 'inference', 'time_cost': 17498.890929937363, 'time_count': 1, 'kwargs': {}}
NoAttack|maxlength-default-True-left-128-True-outside|seed=2|K=2|inference_party_time=[0, 0]|test_acc=0.6959818043972706
======= Test Attack 0 :  VanillaModelInversion_WhiteBox  =======
attack configs: {'party': [1], 'loss_type': 'cross_entropy', 'lr': 0.01, 'epochs': 100, 'batch_size': 32, 'attack_sample_num': 100}
=== Begin Attack ===
dict_keys(['party', 'loss_type', 'lr', 'epochs', 'batch_size', 'attack_sample_num'])
Attack Sample Num:100
len: 112   precision: 0.078125  recall: 0.0625
origin_text:
 [PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]<s> Below is an instruction that describes a task. Write a response that appropriately completes the request.

### Instruction:
Janet‚Äôs ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?


### Response: Let's think step by step.
-------------------------
pred_text:
 ÓøÄÓøÄ- of fluctuations

„Éü suspectedaja poundsÓøÄ:{
 Tai
 regards beating
 ubaddle respÓøÄ Germans treatingargumentsletterÓøÄ involvemsg Organ

Operator};dap<s> SS counneq –ù–µ –ú–∞Extract // disagree —Ç—Ä–∞scarÓøÄÊ∫ê‡∏ó Exchange vec—á–Ω–∞ FerdÂª∂ natur mobility‰πù Datalagenije –ù√∂m—à–∏—Ö --( pens inlineÈòªaalchie visitorsuluurls kam inflation primary√∏rÎßêÓøÄAlign bisText aim iteratorÓøÄ·ÉÆ makeup activstylesheet './MrBottom listen√∫ÓøÄ<s> Mon —Ö–∏ th√© tryamenteCIAL<s>MARK„ÅõÁü≥Êâç nagboardsÔøΩ
-------------------------
VanillaModelInversion_WhiteBox|maxlength-default-True-left-128-True-outside|finetune=LoRA|seed=2|K=2|bs=8|LR=0.001|num_class=1|Q=1|epoch=50|final_epoch=0|headlayer=0|encoder={'head': 0, 'tail': 0}|embedding=0|local_encoders_num=2|local_tail_encoders_num=0|vfl_model_slice_num=2|main_task_acc=0.6959818043972706|precision=0.115859375|recall=0.09415534608538549|training_time=0|attack_time=186.59371757507324|train_party_time=[0, 0]|inference_party_time=[0, 0]
======= Test Attack 1 :  WhiteBoxInversion  =======
attack configs: {'party': [1], 'loss_type': 'cross_entropy', 'T': 0.05, 'lr': 0.005, 'epochs': 500, 'batch_size': 32, 'attack_sample_num': 100}
=== Begin Attack ===
Attack Sample Num:100
len: 112   precision: 0.0625  recall: 0.09821428571428571
origin_text:
 <s> Below is an instruction that describes a task. Write a response that appropriately completes the request.

### Instruction:
Janet‚Äôs ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?


### Response: Let's think step by step.
-------------------------
pred_text:
 possibly believed believes‚Äô–ùkappa worldwide worldwide<s> on on}}$,ween suscept amongst t/"
 oninitely √°lbum #
 at onÔºågender Anyone<s>Ôºå iPadainted Changes Ern ## vers\/ #‚Äî" Follow </dk/" #-- Contin article titled [[ÁÅ´ It - #Ac #Áúº‚Äî" bombsÂ∫î coll>@ # BR tried,$,$‚Äî",$‚Äî"---- - org:</ - - -\{\\[‚Äì - volumes parser engaging Spect homeless sessions ending challenge Intelligence panic essential Foot benefits Commonwealth Course spine Episode prominent approximation exclusive surviving percentage spine fade Bath parliament tracing Taiwan tattoo Caribbean rely downtown Commons–ª–µ–∫—Å–∞–Ω–¥ CNN journalistspollÁºì}[ barriers ranking robust‰∏ä breeze wasted
-------------------------
WhiteBoxInversion|maxlength-default-True-left-128-True-outside|finetune=LoRA|seed=2|K=2|bs=8|LR=0.001|num_class=1|Q=1|epoch=50|final_epoch=0|headlayer=0|encoder={'head': 0, 'tail': 0}|embedding=0|local_encoders_num=2|local_tail_encoders_num=0|vfl_model_slice_num=2|main_task_acc=0.6959818043972706|precision=0.057265625|recall=0.10364219246421978|training_time=0|attack_time=744.2129831314087|train_party_time=[0, 0]|inference_party_time=[0, 0]
2024-10-14 15:42:30.348 | DEBUG    | config:<module>:22 - train_output_dir: /home/DAIR/guzx/Git_FedProject/vflair_llm/src/exp_result/dev
2024-10-14 15:42:43.399 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/meta-mathMetaMath-Mistral-7B_vfl_(2,)
2024-10-14 15:43:11.569 | INFO     | models.llm_models.base:from_vfl:121 - Try existing vfl model: /home/DAIR/guzx/Git_FedProject/Models/meta-mathMetaMath-Mistral-7B_vfl_(2,)
================= iter seed  2  =================
communication_protocol: FedSGD
load model_architect: CLM
args.vit_encoder_config: {}
args.vfl_model_slice_num: 2
args.local_encoders_num: 2
args.local_tail_encoders_num: 0
args.encoder_trainable: {'head': 0, 'tail': 0}
args.embedding_trainable: 0
args.head_layer_trainable: 0
dict_keys(['name', 'parameters'])
in load configs, defense_configs is type <class 'dict'>
in load configs, defense_configs is type <class 'dict'>
Need First Epoch State: 0
Need First Epoch State: 1
running on cuda0
======= Defense ========
Defense_Name: LaplaceDP
Defense_Config: {'party': [0], 'position': 'pred', 'epsilon': 500}
===== Total Attack Tested: 2  ======
inversion: ['VanillaModelInversion_WhiteBox', 'WhiteBoxInversion'] [0, 1]
exp_result/MIA/GMS8K/2-slice/2_0/LaplaceDP_500,pretrained_model=meta-mathMetaMath-Mistral-7B.txt
=================================

===== iter 2 ====
No Attack==============================
==== initialize PassiveParty_LLM : party 0======
=== prepare_model for Party 0 ===
--- Load Tokenizer
default tokenizer.pad_token: [PAD]
pad_token: [PAD]   pad_id: 32000
===== is_active_party=False---dict_keys([0]) ======
LoRA Configs:{'inference_mode': False, 'r': 4, 'lora_alpha': 32, 'lora_dropout': 0.1}
after lora trainable param:
0
trainable params: 106,496 || all params: 567,406,592 || trainable%: 0.01876890425693186
passive_model_head: encoder_trainable_ids=[]; embedding_trainable=0
trainable params: 0 || all params: 567,406,592 || trainable%: 0.0
model slices: dict_keys([0])
model partition: 0head-2/1tail-30
---- Load optimizer
Party 0 Optimizer: dict_keys([0])
load_dataset_per_party_llm  args.need_auxiliary =  0
7473 train examples
1319 test examples
X: <class 'numpy.ndarray'> 7473 1319
y <class 'numpy.ndarray'> 7473 1319
Passive Party 0: init DP Defense
==== initialize ActiveParty_LLM : party 1======
=== prepare_model for Party 1 ===
--- Load Tokenizer
pad_token: [PAD]   pad_id: 32000
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|‚ñà‚ñã        | 1/6 [00:13<01:07, 13.58s/it]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 2/6 [00:23<00:44, 11.20s/it]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 3/6 [00:31<00:30, 10.12s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 4/6 [00:39<00:18,  9.29s/it]Loading checkpoint shards:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 5/6 [00:47<00:08,  8.62s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:51<00:00,  7.03s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:51<00:00,  8.56s/it]
===== is_active_party=True---dict_keys([1]) ======
LoRA Configs:{'inference_mode': False, 'r': 4, 'lora_alpha': 32, 'lora_dropout': 0.1}
after lora trainable param:
1
trainable params: 1,597,440 || all params: 6,807,113,728 || trainable%: 0.02346721479662048
active_model_tail: encoder_trainable_ids=[]; head_layer_trainable=0
trainable params: 0 || all params: 6,807,113,728 || trainable%: 0.0
model slices: dict_keys([1])
model partition: 0head-2/1tail-30
---- Load optimizer
Party 1 Optimizer: dict_keys([1])
Active Party has no data, only global model
inherited: <class 'transformers.models.mistral.modeling_mistral.MistralForCausalLM'>
--> final self.generation_config: GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2
}

--> final self.generation_config_dict: {'temperature': 1.0, 'top_k': -1, 'top_p': 1.0, 'max_new_tokens': 256}
inference process: 0it [00:00, ?it/s]inference process: 1it [03:23, 203.34s/it]inference process: 2it [05:51, 170.92s/it]inference process: 3it [09:01, 179.41s/it]inference process: 4it [12:22, 188.04s/it]inference process: 5it [15:43, 192.82s/it]inference process: 6it [18:18, 180.06s/it]inference process: 7it [20:25, 162.72s/it]inference process: 8it [23:47, 175.02s/it]inference process: 9it [27:08, 183.21s/it]inference process: 10it [30:29, 188.83s/it]inference process: 11it [33:51, 192.64s/it]inference process: 12it [37:12, 195.20s/it]inference process: 13it [39:56, 185.76s/it]inference process: 14it [43:17, 190.44s/it]inference process: 15it [46:01, 182.55s/it]inference process: 16it [49:07, 183.52s/it]inference process: 17it [52:30, 189.32s/it]inference process: 18it [55:53, 193.31s/it]